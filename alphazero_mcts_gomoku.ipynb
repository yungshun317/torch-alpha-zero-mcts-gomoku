{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9286a65-7939-4ae2-83dc-dfe41e5ecb33",
   "metadata": {},
   "source": [
    "# 1. Gomoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a7abc-73dc-445d-a062-c579be416971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Gomoku:\n",
    "    \"\"\"\n",
    "    N×N board, players in {+1 (black), -1 (white)}, empty=0.\n",
    "    Win condition: 5 in a row (any direction).\n",
    "    \"\"\"\n",
    "    def __init__(self, N=9, win_len=5):\n",
    "        self.N = int(N)\n",
    "        self.win_len = int(win_len)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.N, self.N), dtype=np.int8)\n",
    "        self.current_player = 1\n",
    "        self.done = False\n",
    "        self.last_move = None  # (r,c) or None\n",
    "        return self.get_state()  # canonical 1×(N*N) if you want; see below\n",
    "\n",
    "    # --- Canonical features for NN (3 planes): me, opp, to-move ---\n",
    "    def to_planes(self):\n",
    "        me  = (self.board == self.current_player).astype(np.float32)\n",
    "        opp = (self.board == -self.current_player).astype(np.float32)\n",
    "        turn = np.full_like(me, 1.0, dtype=np.float32)  # “to move” plane\n",
    "        # shape: [3, N, N]\n",
    "        return np.stack([me, opp, turn], axis=0)\n",
    "\n",
    "    # For drop-in compatibility with your older code:\n",
    "    def get_state(self):\n",
    "        # vectorized canonical state (board * current_player) like tic-tac-toe\n",
    "        return (self.board.flatten() * self.current_player).astype(np.float32)\n",
    "\n",
    "    def get_legal_actions(self):\n",
    "        # actions are 0..N*N-1\n",
    "        flat = self.board.reshape(-1)\n",
    "        return [i for i in range(self.N * self.N) if flat[i] == 0]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply action index (0..N^2-1). Returns (state, reward, done).\"\"\"\n",
    "        if self.done:\n",
    "            return self.get_state(), 0.0, True\n",
    "        r, c = divmod(int(action), self.N)\n",
    "        if self.board[r, c] != 0:\n",
    "            # illegal move penalty (AlphaZero typically forbids via masking;\n",
    "            # but keep this as a safeguard)\n",
    "            self.done = True\n",
    "            return self.get_state(), -1.0, True\n",
    "\n",
    "        self.board[r, c] = self.current_player\n",
    "        self.last_move = (r, c)\n",
    "\n",
    "        reward, done = self.check_winner_fast()\n",
    "        self.done = done\n",
    "        state = self.get_state()\n",
    "        self.current_player *= -1\n",
    "        return state, float(reward), bool(done)\n",
    "\n",
    "    # --------- winner checks ----------\n",
    "    def check_winner_fast(self):\n",
    "        \"\"\"\n",
    "        Check only the lines passing through self.last_move for speed.\n",
    "        Returns (reward_for_player_who_just_moved, done)\n",
    "        reward ∈ {+1 win, 0 draw/ongoing, -1 loss} relative to the mover.\n",
    "        \"\"\"\n",
    "        if self.last_move is None:\n",
    "            return 0.0, False\n",
    "        r, c = self.last_move\n",
    "        p = self.board[r, c]\n",
    "        if p == 0:\n",
    "            return 0.0, False\n",
    "\n",
    "        if ( self._count_dir(r, c, 1, 0, p) + self._count_dir(r, c, -1, 0, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 0, 1, p) + self._count_dir(r, c, 0, -1, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 1, 1, p) + self._count_dir(r, c, -1, -1, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 1, -1, p) + self._count_dir(r, c, -1, 1, p) - 1 >= self.win_len ):\n",
    "            # player p (who just moved) wins: reward +1 for mover\n",
    "            return 1.0, True\n",
    "\n",
    "        if (self.board != 0).all():\n",
    "            return 0.0, True  # draw\n",
    "\n",
    "        return 0.0, False\n",
    "\n",
    "    def _count_dir(self, r, c, dr, dc, p):\n",
    "        \"\"\"Count contiguous stones with color p starting at (r,c) inclusive along (dr,dc).\"\"\"\n",
    "        N = self.N\n",
    "        cnt = 0\n",
    "        rr, cc = r, c\n",
    "        while 0 <= rr < N and 0 <= cc < N and self.board[rr, cc] == p:\n",
    "            cnt += 1\n",
    "            rr += dr\n",
    "            cc += dc\n",
    "        return cnt\n",
    "\n",
    "    # --------- convenience ----------\n",
    "    def render_ascii(self):\n",
    "        sym = {1:'X', -1:'O', 0:'.'}\n",
    "        print(\"\\n\".join(\" \".join(sym[v] for v in row) for row in self.board))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7745f-094c-42a4-ac6a-404e421c0b71",
   "metadata": {},
   "source": [
    "# 2. ResNet Policy-Value Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d937944f-353a-4802-a13e-2fe97c41c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(x + y)\n",
    "\n",
    "class PVResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy–Value net for Gomoku.\n",
    "    Input: [B, 3, N, N]\n",
    "    Outputs:\n",
    "      - policy_logits: [B, N*N]\n",
    "      - value:         [B, 1]  (tanh)\n",
    "    \"\"\"\n",
    "    def __init__(self, board_size=9, channels=64, n_blocks=6):\n",
    "        super().__init__()\n",
    "        self.N = board_size\n",
    "        C = channels\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, C, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(C),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.trunk = nn.Sequential(*[ResidualBlock(C) for _ in range(n_blocks)])\n",
    "\n",
    "        # Policy head\n",
    "        self.p_head = nn.Sequential(\n",
    "            nn.Conv2d(C, 2, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.p_fc = nn.Linear(2 * self.N * self.N, self.N * self.N)\n",
    "\n",
    "        # Value head\n",
    "        self.v_head = nn.Sequential(\n",
    "            nn.Conv2d(C, 1, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.v_fc1 = nn.Linear(1 * self.N * self.N, C)\n",
    "        self.v_fc2 = nn.Linear(C, 1)\n",
    "\n",
    "        # (optional) init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):  # x: [B, 3, N, N]\n",
    "        z = self.stem(x)\n",
    "        z = self.trunk(z)\n",
    "\n",
    "        # policy\n",
    "        p = self.p_head(z)\n",
    "        p = p.view(p.size(0), -1)\n",
    "        policy_logits = self.p_fc(p)  # [B, N*N]\n",
    "\n",
    "        # value\n",
    "        v = self.v_head(z)\n",
    "        v = v.view(v.size(0), -1)\n",
    "        v = F.relu(self.v_fc1(v))\n",
    "        value = torch.tanh(self.v_fc2(v))  # [-1, 1]\n",
    "\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074746e0-9ece-44e3-bc79-40358fdb2873",
   "metadata": {},
   "source": [
    "# 3. Monte Carlo Tree Search (MCTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa8eaeb-17ca-4b4e-8727-7a773b80035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Node ----\n",
    "class Node:\n",
    "    def __init__(self, raw_board_tuple, player_to_move):\n",
    "        self.state = raw_board_tuple         # raw board (0/±1) flattened as tuple\n",
    "        self.player = player_to_move         # +1 or -1\n",
    "        self.children = {}                   # action -> Node\n",
    "        self.visit_count = 0\n",
    "        self.total_value = 0.0               # from perspective of self.player\n",
    "        self.prior = 0.0\n",
    "\n",
    "    def value(self):\n",
    "        return 0.0 if self.visit_count == 0 else self.total_value / self.visit_count\n",
    "\n",
    "# ---- MCTS ----\n",
    "class MCTS:\n",
    "    def __init__(self, model, simulations=50, c_puct=1.0):\n",
    "        self.model = model\n",
    "        self.simulations = simulations\n",
    "        self.c_puct = c_puct\n",
    "\n",
    "    def run(self, env):\n",
    "        # store RAW board in the root; player stored separately\n",
    "        root = Node(tuple(env.board.flatten()), env.current_player)\n",
    "        for _ in range(self.simulations):\n",
    "            self.simulate(root)\n",
    "        return root\n",
    "\n",
    "    def simulate(self, node):\n",
    "        # Terminal?\n",
    "        winner_val, done = self.check_terminal(node.state)\n",
    "        if done:\n",
    "            v = float(winner_val)            # +1 win for node.player, 0 draw, -1 loss\n",
    "            node.visit_count += 1\n",
    "            node.total_value += v\n",
    "            return v\n",
    "\n",
    "        # Leaf?\n",
    "        if node.visit_count == 0 and not node.children:\n",
    "            # NN eval in canonical perspective\n",
    "\n",
    "            # before (tic-tac-toe):\n",
    "            # x = np.array(node.state, dtype=np.float32) * node.player\n",
    "            # state_tensor = torch.from_numpy(x).unsqueeze(0)\n",
    "            # policy_logits, value = self.model(state_tensor)\n",
    "            # policy_logits = policy_logits.detach().numpy().flatten()\n",
    "            \n",
    "            # Gomoku:\n",
    "            planes = gomoku_planes_from_raw(node.state, node.player, N)  # [3, N, N] np.float32\n",
    "            state_tensor = torch.from_numpy(planes).unsqueeze(0)         # torch [1, 3, N, N]\n",
    "            logits, value = self.model(state_tensor)\n",
    "            policy = F.softmax(logits, dim=-1).detach().cpu().numpy().flatten()  # length N*N\n",
    "            \n",
    "            legal = self.legal_actions_from_raw(node.state)\n",
    "            # mask + renorm\n",
    "            mask = np.full(9, -1e9, dtype=np.float32)\n",
    "            mask[legal] = 0.0\n",
    "            masked = policy_logits + mask\n",
    "            priors = np.exp(masked - masked.max())\n",
    "            priors = priors / (priors[legal].sum() + 1e-8)\n",
    "\n",
    "            for a in legal:\n",
    "                child_board = self.apply_action_raw(node.state, a, node.player)\n",
    "                child = Node(child_board, -node.player)\n",
    "                child.prior = float(priors[a])\n",
    "                node.children[a] = child\n",
    "\n",
    "            v = float(value.item())          # value is from node.player perspective\n",
    "            node.visit_count += 1\n",
    "            node.total_value += v\n",
    "            return v\n",
    "\n",
    "        # Select child by PUCT\n",
    "        best_score, best_a = -1e9, None\n",
    "        sqrt_N = np.sqrt(node.visit_count + 1e-8)\n",
    "        for a, child in node.children.items():\n",
    "            ucb = child.value() + self.c_puct * child.prior * (sqrt_N / (1 + child.visit_count))\n",
    "            if ucb > best_score:\n",
    "                best_score, best_a = ucb, a\n",
    "\n",
    "        # Recurse\n",
    "        child = node.children[best_a]\n",
    "        v_child = self.simulate(child)\n",
    "        v = -v_child                           # switch perspective on backup\n",
    "        node.visit_count += 1\n",
    "        node.total_value += v\n",
    "        return v\n",
    "\n",
    "    # --- helpers on RAW board (0/±1) ---\n",
    "    def legal_actions_from_raw(self, raw_board_tuple):\n",
    "        raw = np.array(raw_board_tuple).reshape(3,3)\n",
    "        return [i for i in range(9) if raw.flat[i] == 0]\n",
    "\n",
    "    def apply_action_raw(self, raw_board_tuple, action, player):\n",
    "        raw = np.array(raw_board_tuple).reshape(3,3).copy()\n",
    "        r, c = divmod(action, 3)\n",
    "        raw[r, c] = player\n",
    "        return tuple(raw.flatten())\n",
    "\n",
    "    def check_terminal(self, raw_board_tuple):\n",
    "        b = np.array(raw_board_tuple).reshape(3,3)\n",
    "        lines = list(b.sum(axis=0)) + list(b.sum(axis=1)) + [b.trace(), np.fliplr(b).trace()]\n",
    "        if 3 in lines:   return (+1, True)  # current node.player wins\n",
    "        if -3 in lines:  return (-1, True)  # current node.player loses\n",
    "        if (b != 0).all(): return (0, True) # draw\n",
    "        return (0, False)\n",
    "\n",
    "    def select_action(self, root, temperature=1.0):\n",
    "        visits = np.array([root.children[a].visit_count if a in root.children else 0 for a in range(9)], dtype=np.float64)\n",
    "        legal = np.array([a in root.children for a in range(9)], dtype=bool)\n",
    "\n",
    "        if temperature == 0 or visits.sum() == 0:\n",
    "            # fallback: pick most visited among legal, otherwise random legal\n",
    "            legal_idxs = np.where(legal)[0]\n",
    "            if visits.sum() == 0:\n",
    "                return np.random.choice(legal_idxs)\n",
    "            return legal_idxs[np.argmax(visits[legal_idxs])]\n",
    "\n",
    "        x = visits.astype(np.float64) ** (1.0 / temperature)\n",
    "        x = x * legal\n",
    "        s = x.sum()\n",
    "        if s <= 0:\n",
    "            legal_idxs = np.where(legal)[0]\n",
    "            return np.random.choice(legal_idxs)\n",
    "        p = x / s\n",
    "        return np.random.choice(9, p=p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
