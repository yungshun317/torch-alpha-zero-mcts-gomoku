{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9286a65-7939-4ae2-83dc-dfe41e5ecb33",
   "metadata": {},
   "source": [
    "# 1. Gomoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207a7abc-73dc-445d-a062-c579be416971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Gomoku:\n",
    "    \"\"\"\n",
    "    N×N board, players in {+1 (black), -1 (white)}, empty=0.\n",
    "    Win condition: 5 in a row (any direction).\n",
    "    \"\"\"\n",
    "    def __init__(self, N=9, win_len=5):\n",
    "        self.N = int(N)\n",
    "        self.win_len = int(win_len)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.N, self.N), dtype=np.int8)\n",
    "        self.current_player = 1\n",
    "        self.done = False\n",
    "        self.last_move = None  # (r,c) or None\n",
    "        return self.get_state()  # canonical 1×(N*N) if you want; see below\n",
    "\n",
    "    # --- Canonical features for NN (3 planes): me, opp, to-move ---\n",
    "    def to_planes(self):\n",
    "        me  = (self.board == self.current_player).astype(np.float32)\n",
    "        opp = (self.board == -self.current_player).astype(np.float32)\n",
    "        turn = np.full_like(me, 1.0, dtype=np.float32)  # “to move” plane\n",
    "        # shape: [3, N, N]\n",
    "        return np.stack([me, opp, turn], axis=0)\n",
    "\n",
    "    # For drop-in compatibility with your older code:\n",
    "    def get_state(self):\n",
    "        # vectorized canonical state (board * current_player) like tic-tac-toe\n",
    "        return (self.board.flatten() * self.current_player).astype(np.float32)\n",
    "\n",
    "    def get_legal_actions(self):\n",
    "        # actions are 0..N*N-1\n",
    "        flat = self.board.reshape(-1)\n",
    "        return [i for i in range(self.N * self.N) if flat[i] == 0]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply action index (0..N^2-1). Returns (state, reward, done).\"\"\"\n",
    "        if self.done:\n",
    "            return self.get_state(), 0.0, True\n",
    "        r, c = divmod(int(action), self.N)\n",
    "        if self.board[r, c] != 0:\n",
    "            # illegal move penalty (AlphaZero typically forbids via masking;\n",
    "            # but keep this as a safeguard)\n",
    "            self.done = True\n",
    "            return self.get_state(), -1.0, True\n",
    "\n",
    "        self.board[r, c] = self.current_player\n",
    "        self.last_move = (r, c)\n",
    "\n",
    "        reward, done = self.check_winner_fast()\n",
    "        self.done = done\n",
    "        state = self.get_state()\n",
    "        self.current_player *= -1\n",
    "        return state, float(reward), bool(done)\n",
    "\n",
    "    # --------- winner checks ----------\n",
    "    def check_winner_fast(self):\n",
    "        \"\"\"\n",
    "        Check only the lines passing through self.last_move for speed.\n",
    "        Returns (reward_for_player_who_just_moved, done)\n",
    "        reward ∈ {+1 win, 0 draw/ongoing, -1 loss} relative to the mover.\n",
    "        \"\"\"\n",
    "        if self.last_move is None:\n",
    "            return 0.0, False\n",
    "        r, c = self.last_move\n",
    "        p = self.board[r, c]\n",
    "        if p == 0:\n",
    "            return 0.0, False\n",
    "\n",
    "        if ( self._count_dir(r, c, 1, 0, p) + self._count_dir(r, c, -1, 0, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 0, 1, p) + self._count_dir(r, c, 0, -1, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 1, 1, p) + self._count_dir(r, c, -1, -1, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 1, -1, p) + self._count_dir(r, c, -1, 1, p) - 1 >= self.win_len ):\n",
    "            # player p (who just moved) wins: reward +1 for mover\n",
    "            return 1.0, True\n",
    "\n",
    "        if (self.board != 0).all():\n",
    "            return 0.0, True  # draw\n",
    "\n",
    "        return 0.0, False\n",
    "\n",
    "    def _count_dir(self, r, c, dr, dc, p):\n",
    "        \"\"\"Count contiguous stones with color p starting at (r,c) inclusive along (dr,dc).\"\"\"\n",
    "        N = self.N\n",
    "        cnt = 0\n",
    "        rr, cc = r, c\n",
    "        while 0 <= rr < N and 0 <= cc < N and self.board[rr, cc] == p:\n",
    "            cnt += 1\n",
    "            rr += dr\n",
    "            cc += dc\n",
    "        return cnt\n",
    "\n",
    "    # --------- convenience ----------\n",
    "    def render_ascii(self):\n",
    "        sym = {1:'X', -1:'O', 0:'.'}\n",
    "        print(\"\\n\".join(\" \".join(sym[v] for v in row) for row in self.board))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7745f-094c-42a4-ac6a-404e421c0b71",
   "metadata": {},
   "source": [
    "# 2. ResNet Policy-Value Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d937944f-353a-4802-a13e-2fe97c41c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(x + y)\n",
    "\n",
    "class PVResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy–Value net for Gomoku.\n",
    "    Input: [B, 3, N, N]\n",
    "    Outputs:\n",
    "      - policy_logits: [B, N*N]\n",
    "      - value:         [B, 1]  (tanh)\n",
    "    \"\"\"\n",
    "    def __init__(self, board_size=9, channels=64, n_blocks=6):\n",
    "        super().__init__()\n",
    "        self.N = board_size\n",
    "        C = channels\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, C, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(C),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.trunk = nn.Sequential(*[ResidualBlock(C) for _ in range(n_blocks)])\n",
    "\n",
    "        # Policy head\n",
    "        self.p_head = nn.Sequential(\n",
    "            nn.Conv2d(C, 2, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.p_fc = nn.Linear(2 * self.N * self.N, self.N * self.N)\n",
    "\n",
    "        # Value head\n",
    "        self.v_head = nn.Sequential(\n",
    "            nn.Conv2d(C, 1, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.v_fc1 = nn.Linear(1 * self.N * self.N, C)\n",
    "        self.v_fc2 = nn.Linear(C, 1)\n",
    "\n",
    "        # (optional) init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):  # x: [B, 3, N, N]\n",
    "        z = self.stem(x)\n",
    "        z = self.trunk(z)\n",
    "\n",
    "        # policy\n",
    "        p = self.p_head(z)\n",
    "        p = p.view(p.size(0), -1)\n",
    "        policy_logits = self.p_fc(p)  # [B, N*N]\n",
    "\n",
    "        # value\n",
    "        v = self.v_head(z)\n",
    "        v = v.view(v.size(0), -1)\n",
    "        v = F.relu(self.v_fc1(v))\n",
    "        value = torch.tanh(self.v_fc2(v))  # [-1, 1]\n",
    "\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074746e0-9ece-44e3-bc79-40358fdb2873",
   "metadata": {},
   "source": [
    "# 3. Monte Carlo Tree Search (MCTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e6c3cd-3923-4635-9351-cee810961de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gomoku_planes_from_raw(raw_board_tuple, player, N):\n",
    "    \"\"\"Build [3, N, N] planes from (raw_board, player_to_move).\"\"\"\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N)\n",
    "    me  = (b == player).astype(np.float32)\n",
    "    opp = (b == -player).astype(np.float32)\n",
    "    turn = np.ones_like(me, dtype=np.float32)\n",
    "    return np.stack([me, opp, turn], axis=0)\n",
    "\n",
    "def legal_actions_from_raw(raw_board_tuple, N):\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N)\n",
    "    return [i for i in range(N*N) if b.flat[i] == 0]\n",
    "\n",
    "def apply_action_raw(raw_board_tuple, action, player, N):\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N).copy()\n",
    "    r, c = divmod(int(action), N)\n",
    "    b[r, c] = player\n",
    "    return tuple(b.reshape(-1))\n",
    "\n",
    "def check_terminal_gomoku(raw_board_tuple, N, win_len=5):\n",
    "    \"\"\"Return (value_for_current_node_player, done). Full scan (O(N^2)), fine for 9x9.\"\"\"\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N)\n",
    "\n",
    "    def has_k(p):\n",
    "        # rows\n",
    "        for r in range(N):\n",
    "            run = 0\n",
    "            for c in range(N):\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "        # cols\n",
    "        for c in range(N):\n",
    "            run = 0\n",
    "            for r in range(N):\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "        # diag \\\n",
    "        for r0 in range(N):\n",
    "            run = 0\n",
    "            r, c = r0, 0\n",
    "            while r < N and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r += 1; c += 1\n",
    "        for c0 in range(1, N):\n",
    "            run = 0\n",
    "            r, c = 0, c0\n",
    "            while r < N and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r += 1; c += 1\n",
    "        # diag /\n",
    "        for r0 in range(N):\n",
    "            run = 0\n",
    "            r, c = r0, 0\n",
    "            while r >= 0 and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r -= 1; c += 1\n",
    "        for c0 in range(1, N):\n",
    "            run = 0\n",
    "            r, c = N-1, c0\n",
    "            while r >= 0 and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r -= 1; c += 1\n",
    "        return False\n",
    "\n",
    "    if has_k(+1): return (+1.0, True)\n",
    "    if has_k(-1): return (-1.0, True)\n",
    "    if (b != 0).all(): return (0.0, True)\n",
    "    return (0.0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa8eaeb-17ca-4b4e-8727-7a773b80035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Node ----\n",
    "class Node:\n",
    "    def __init__(self, raw_board_tuple, player_to_move):\n",
    "        self.state = raw_board_tuple          # tuple of length N*N (0/±1)\n",
    "        self.player = int(player_to_move)     # +1 or -1\n",
    "        self.children = {}                    # action -> Node\n",
    "        self.visit_count = 0\n",
    "        self.total_value = 0.0                # mean value from this node's player perspective\n",
    "        self.prior = 0.0\n",
    "\n",
    "    def value(self):\n",
    "        return 0.0 if self.visit_count == 0 else self.total_value / self.visit_count\n",
    "\n",
    "# ---- MCTS ----\n",
    "class MCTS:\n",
    "    def __init__(self, model, simulations=800, c_puct=1.5, win_len=5):\n",
    "        self.model = model\n",
    "        self.simulations = simulations\n",
    "        self.c_puct = c_puct\n",
    "        self.win_len = win_len\n",
    "        self.ttable = {}  # (state_tuple, player, N) -> (priors_np[N*N], value_float)\n",
    "\n",
    "    # ----------------------------\n",
    "    def run(self, env, add_root_noise=True, dir_alpha=0.3, dir_eps=0.25):\n",
    "        N = env.N\n",
    "        root = Node(tuple(env.board.reshape(-1)), env.current_player)\n",
    "\n",
    "        # Ensure root is expanded (so we can add Dirichlet noise to its priors)\n",
    "        if not root.children:\n",
    "            _ = self._expand_and_evaluate(root, N)\n",
    "\n",
    "        if add_root_noise:\n",
    "            self._add_dirichlet_noise(root, N, alpha=dir_alpha, eps=dir_eps)\n",
    "\n",
    "        for _ in range(self.simulations):\n",
    "            self._simulate(root, N)\n",
    "\n",
    "        return root\n",
    "\n",
    "    # ----------------------------\n",
    "    def _simulate(self, node, N):\n",
    "        # Terminal test\n",
    "        wval, done = check_terminal_gomoku(node.state, N, self.win_len)\n",
    "        if done:\n",
    "            v = float(wval)                      # value from node.player perspective\n",
    "            node.visit_count += 1\n",
    "            node.total_value += v\n",
    "            return v\n",
    "\n",
    "        # Leaf: not visited yet and no children → expand\n",
    "        if node.visit_count == 0 and not node.children:\n",
    "            v = self._expand_and_evaluate(node, N)\n",
    "            node.visit_count += 1\n",
    "            node.total_value += v\n",
    "            return v\n",
    "\n",
    "        # Select via PUCT\n",
    "        best_a, best_score = None, -1e9\n",
    "        sqrt_N = np.sqrt(node.visit_count + 1e-8)\n",
    "        for a, child in node.children.items():\n",
    "            ucb = child.value() + self.c_puct * child.prior * (sqrt_N / (1 + child.visit_count))\n",
    "            if ucb > best_score:\n",
    "                best_score, best_a = ucb, a\n",
    "\n",
    "        child = node.children[best_a]\n",
    "        v_child = self._simulate(child, N)\n",
    "        v = -v_child                              # perspective switch\n",
    "        node.visit_count += 1\n",
    "        node.total_value += v\n",
    "        return v\n",
    "\n",
    "    # ----------------------------\n",
    "    def _expand_and_evaluate(self, node, N):\n",
    "        key = (node.state, node.player, N)\n",
    "        if key in self.ttable:\n",
    "            priors, v = self.ttable[key]\n",
    "        else:\n",
    "            planes = gomoku_planes_from_raw(node.state, node.player, N)\n",
    "            st = torch.from_numpy(planes).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                logits, value = self.model(st)\n",
    "                logits = logits.squeeze(0)        # [N*N]\n",
    "                v = float(value.item())\n",
    "            policy = F.softmax(logits, dim=-1).cpu().numpy()  # length N*N\n",
    "\n",
    "            legal = legal_actions_from_raw(node.state, N)\n",
    "            if len(legal) == 0:\n",
    "                priors = np.zeros(N*N, dtype=np.float32)\n",
    "            else:\n",
    "                mask = np.full(N*N, -1e9, dtype=np.float32)\n",
    "                mask[legal] = 0.0\n",
    "                masked = np.log(policy + 1e-12) + mask  # numeric stable masking\n",
    "                masked -= masked.max()\n",
    "                priors = np.exp(masked)\n",
    "                s = priors[legal].sum()\n",
    "                priors = priors / (s + 1e-8)\n",
    "\n",
    "            self.ttable[key] = (priors.astype(np.float32), v)\n",
    "\n",
    "        # Create children\n",
    "        for a in np.where(self.ttable[key][0] > 0)[0]:\n",
    "            child_board = apply_action_raw(node.state, int(a), node.player, N)\n",
    "            child = Node(child_board, -node.player)\n",
    "            child.prior = float(self.ttable[key][0][a])\n",
    "            node.children[int(a)] = child\n",
    "\n",
    "        return v\n",
    "\n",
    "    # ----------------------------\n",
    "    def _add_dirichlet_noise(self, node, N, alpha=0.3, eps=0.25):\n",
    "        if not node.children:\n",
    "            return\n",
    "        legal_actions = list(node.children.keys())\n",
    "        if len(legal_actions) == 0:\n",
    "            return\n",
    "        noise = np.random.dirichlet([alpha] * len(legal_actions)).astype(np.float32)\n",
    "        for a, n in zip(legal_actions, noise):\n",
    "            p = node.children[a].prior\n",
    "            node.children[a].prior = (1 - eps) * p + eps * float(n)\n",
    "\n",
    "    # ----------------------------\n",
    "    def select_action(self, root, temperature=1.0):\n",
    "        N2 = int(np.sqrt(len(self.ttable.get((root.state, root.player, int(np.sqrt(len(root.state)))), (np.zeros(len(root.state)), 0.0))[0])) or len(root.state))\n",
    "        # safer: compute over children\n",
    "        visits = np.zeros(N2*N2, dtype=np.float64)\n",
    "        legal  = np.zeros(N2*N2, dtype=bool)\n",
    "        for a, child in root.children.items():\n",
    "            visits[a] = child.visit_count\n",
    "            legal[a] = True\n",
    "\n",
    "        legal_idxs = np.where(legal)[0]\n",
    "        if legal_idxs.size == 0:\n",
    "            # no children? pick any legal by state\n",
    "            legal_idxs = np.array(legal_actions_from_raw(root.state, N2), dtype=int)\n",
    "            if legal_idxs.size == 0:\n",
    "                return 0\n",
    "            return int(np.random.choice(legal_idxs))\n",
    "\n",
    "        if temperature == 0.0:\n",
    "            vmax = visits[legal_idxs].max()\n",
    "            ties = legal_idxs[visits[legal_idxs] == vmax]\n",
    "            return int(np.random.choice(ties))\n",
    "\n",
    "        x = visits[legal_idxs] ** (1.0 / temperature)\n",
    "        p = x / (x.sum() + 1e-8)\n",
    "        return int(np.random.choice(legal_idxs, p=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da4567-9942-4349-8600-0f7306eab903",
   "metadata": {},
   "source": [
    "# 4. Self-Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8b0a0e-d470-4034-866b-c2157607d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build env/model\n",
    "env = Gomoku(N=9, win_len=5)\n",
    "model = PVResNet(board_size=env.N, channels=64, n_blocks=6)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(model, simulations=800, c_puct=1.5, win_len=env.win_len)\n",
    "\n",
    "# One move:\n",
    "root = mcts.run(env, add_root_noise=True)\n",
    "a = mcts.select_action(root, temperature=1.0)   # temperature=1 in opening, 0 later\n",
    "state, reward, done = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09fef56-6773-40c9-806e-2cbe04fb2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, math, collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 8 symmetry transforms for NxN boards\n",
    "def symmetries(planes):  # planes: [C,N,N] numpy\n",
    "    ps = []\n",
    "    for k in range(4):  # rotations\n",
    "        r = np.rot90(planes, k=k, axes=(1,2))\n",
    "        ps.append(r)\n",
    "        ps.append(np.flip(r, axis=1))  # mirror\n",
    "    return ps\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buf = collections.deque(maxlen=capacity)\n",
    "    def add(self, x): self.buf.append(x)\n",
    "    def sample(self, bs): return random.sample(self.buf, bs)\n",
    "    def __len__(self): return len(self.buf)\n",
    "\n",
    "def sims_schedule(iter_idx, move_no, base=200, max_sims=800, warmup_iters=20, ramp_moves=12):\n",
    "    \"\"\"\n",
    "    - ramp with training iteration (more search later)\n",
    "    - ramp with move number (opening cheaper, mid/late richer)\n",
    "    \"\"\"\n",
    "    # ramp by iteration\n",
    "    f_iter = min(1.0, iter_idx / float(warmup_iters))\n",
    "    sims_iter = int(base + (max_sims - base) * f_iter)\n",
    "    # ramp by move number\n",
    "    f_move = min(1.0, move_no / float(ramp_moves))\n",
    "    sims = int(base + (sims_iter - base) * f_move)\n",
    "    return max(base, min(max_sims, sims))\n",
    "\n",
    "def self_play_episode(env, model, mcts, temp_moves=8, iter_idx=1):\n",
    "    env.reset()\n",
    "    # list of (planes, pi, player)\n",
    "    traj = []\n",
    "    done = False\n",
    "    move_no = 0\n",
    "    while not done:\n",
    "        # adjust sims on the fly\n",
    "        mcts.simulations = sims_schedule(iter_idx, move_no, base=200, max_sims=mcts.simulations)\n",
    "        root = mcts.run(env, add_root_noise=True, dir_alpha=0.15, dir_eps=0.25)\n",
    "        T = 1.0 if move_no < temp_moves else 0.0\n",
    "        a = mcts.select_action(root, temperature=T)\n",
    "\n",
    "        # build visit-count policy target π\n",
    "        pi = np.zeros(env.N * env.N, dtype=np.float32)\n",
    "        for aa, child in root.children.items():\n",
    "            pi[aa] = child.visit_count\n",
    "        pi /= (pi.sum() + 1e-8)\n",
    "\n",
    "        planes = env.to_planes()\n",
    "        traj.append((planes, pi, env.current_player))\n",
    "\n",
    "        _, reward, done = env.step(a)\n",
    "        move_no += 1\n",
    "\n",
    "    # assign final outcome z to each step from that step's player perspective\n",
    "    data = []\n",
    "    for planes, pi, player in traj:\n",
    "        # +1 win for player's perspective, 0 draw, -1 loss\n",
    "        z = reward * player  \n",
    "        # 8 sym augmentations\n",
    "        for sp in symmetries(planes):\n",
    "            data.append((sp.astype(np.float32), pi.copy(), float(z)))\n",
    "        # list of (planes[3,N,N], pi[N*N], z)\n",
    "    return data\n",
    "\n",
    "def train_step(model, optimizer, batch, N, weight_decay=1e-4):\n",
    "    # batch: list of tuples\n",
    "    planes = torch.from_numpy(np.stack([b[0] for b in batch]))    # [B,3,N,N]\n",
    "    pi      = torch.from_numpy(np.stack([b[1] for b in batch]))   # [B,N*N]\n",
    "    z       = torch.tensor([[b[2]] for b in batch], dtype=torch.float32)\n",
    "\n",
    "    logits, value = model(planes)\n",
    "    policy_loss = -(pi * F.log_softmax(logits, dim=1)).sum(dim=1).mean()\n",
    "    value_loss  = F.mse_loss(value, z)\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # decoupled weight decay (AdamW style)\n",
    "    optimizer.step()\n",
    "    return float(loss.item()), float(policy_loss.item()), float(value_loss.item())\n",
    "\n",
    "def random_action(env):\n",
    "    return random.choice(env.get_legal_actions())\n",
    "\n",
    "def evaluate_vs_random(model, sims=800, games=50, N=9, win_len=5):\n",
    "    wins = draws = losses = 0\n",
    "    for g in range(games):\n",
    "        env = Gomoku(N=N, win_len=win_len)\n",
    "        mcts = MCTS(model, simulations=sims, c_puct=1.5, win_len=win_len)\n",
    "        # alternate who starts\n",
    "        human_like = (+1 if g % 2 == 0 else -1)  # model plays both colors\n",
    "        env.current_player = human_like\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.current_player == human_like:\n",
    "                root = mcts.run(env, add_root_noise=False)  # no root noise in eval\n",
    "                a = mcts.select_action(root, temperature=0.0)\n",
    "            else:\n",
    "                a = random_action(env)\n",
    "            _, reward, done = env.step(a)\n",
    "\n",
    "        # reward is from the perspective of the player who moved last\n",
    "        # winner = -env.current_player  (player who just moved)\n",
    "        if reward == 0:\n",
    "            draws += 1\n",
    "        else:\n",
    "            winner = -env.current_player\n",
    "            if winner == human_like:\n",
    "                wins += 1\n",
    "            else:\n",
    "                losses += 1\n",
    "    print(f\"[Eval vs random] W {wins}  D {draws}  L {losses}  (games={games})\")\n",
    "    return wins, draws, losses\n",
    "\n",
    "# ------------- main training loop (sketch) -------------\n",
    "def train_gomoku(num_iters=1, episodes_per_iter=2, sims=800, batch_size=256):\n",
    "    env = Gomoku(N=9, win_len=5)\n",
    "    model = PVResNet(board_size=env.N, channels=64, n_blocks=6)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_iters)\n",
    "    mcts = MCTS(model, simulations=sims, c_puct=1.5, win_len=env.win_len)\n",
    "    replay = Replay(100_000)\n",
    "    \n",
    "    for it in range(1, num_iters+1):\n",
    "        # Self-play\n",
    "        for _ in range(episodes_per_iter):\n",
    "            data = self_play_episode(env, model, mcts, temp_moves=8, iter_idx=it)\n",
    "            for item in data:\n",
    "                replay.add(item)\n",
    "\n",
    "        # Train\n",
    "        losses = []\n",
    "        for _ in range(min(400, len(replay)//batch_size) ):\n",
    "            batch = replay.sample(batch_size)\n",
    "            l, pl, vl = train_step(model, optimizer, batch, env.N)\n",
    "            losses.append(l)\n",
    "        sched.step()\n",
    "\n",
    "        print(f\"Iter {it:03d} | replay {len(replay)} | sims {sims} | lr {sched.get_last_lr()[0]:.4e} | loss {np.mean(losses) if losses else 0:.3f}\")\n",
    "\n",
    "        if it % 5 == 0:\n",
    "            evaluate_vs_random(model, sims=400, games=20, N=env.N, win_len=env.win_len)\n",
    "\n",
    "        if it % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"models/gomoku_alpha_zero_iter{it:03d}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"models/gomoku_alpha_zero.pth\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60033c42-23d3-48be-af4a-30de9d67be59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 001 | replay 640 | sims 800 | lr 0.0000e+00 | loss 5.582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PVResNet(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (trunk): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (p_head): Sequential(\n",
       "    (0): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (p_fc): Linear(in_features=162, out_features=81, bias=True)\n",
       "  (v_head): Sequential(\n",
       "    (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (v_fc1): Linear(in_features=81, out_features=64, bias=True)\n",
       "  (v_fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gomoku()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff38c77-d0f3-4738-9ad0-68cc54035844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipywidgets\n",
    "import ipywidgets as W\n",
    "from IPython.display import display\n",
    "\n",
    "class GomokuUI:\n",
    "    def __init__(self, model, N=9, win_len=5, sims=800, human_player=-1):\n",
    "        self.model = model\n",
    "        self.N = N\n",
    "        self.win_len = win_len\n",
    "        self.human_player = int(human_player)   # +1 black (X), -1 white (O)\n",
    "        self.sims = sims\n",
    "\n",
    "        self.env = Gomoku(N=N, win_len=win_len)\n",
    "        self.buttons = [[W.Button(description=\" \", layout=W.Layout(width=\"40px\", height=\"40px\")) \n",
    "                         for _ in range(N)] for _ in range(N)]\n",
    "        for r in range(N):\n",
    "            for c in range(N):\n",
    "                self.buttons[r][c].on_click(self._make_cb(r, c))\n",
    "\n",
    "        self.msg = W.HTML(value=\"\")\n",
    "        self.reset_btn = W.Button(description=\"Reset\")\n",
    "        self.reset_btn.on_click(self.reset)\n",
    "\n",
    "        grid = W.GridBox(\n",
    "            children=[self.buttons[r][c] for r in range(N) for c in range(N)],\n",
    "            layout=W.Layout(grid_template_columns=f\"repeat({N}, 42px)\")\n",
    "        )\n",
    "        display(W.VBox([grid, W.HBox([self.reset_btn]), self.msg]))\n",
    "\n",
    "        # AI may start\n",
    "        if self.human_player == -1 and self.env.current_player == +1:\n",
    "            self.ai_move()\n",
    "        self.refresh()\n",
    "\n",
    "    def _make_cb(self, r, c):\n",
    "        def _cb(btn):\n",
    "            if self.env.current_player != self.human_player:\n",
    "                return\n",
    "            a = r * self.N + c\n",
    "            if a not in self.env.get_legal_actions():\n",
    "                return\n",
    "            _, _, done = self.env.step(a)\n",
    "            if not done:\n",
    "                self.ai_move()\n",
    "            self.refresh()\n",
    "        return _cb\n",
    "\n",
    "    def ai_move(self):\n",
    "        mcts = MCTS(self.model, simulations=self.sims, c_puct=1.5, win_len=self.win_len)\n",
    "        root = mcts.run(self.env, add_root_noise=False)  # no noise for a strong play\n",
    "        a = mcts.select_action(root, temperature=0.0)\n",
    "        self.env.step(a)\n",
    "\n",
    "    def refresh(self):\n",
    "        sym = {1:\"X\", -1:\"O\", 0:\" \"}\n",
    "        for r in range(self.N):\n",
    "            for c in range(self.N):\n",
    "                v = int(self.env.board[r, c])\n",
    "                b = self.buttons[r][c]\n",
    "                b.description = sym[v]\n",
    "                b.disabled = bool(v != 0)\n",
    "\n",
    "        reward, done = self.env.check_winner_fast()\n",
    "        if done:\n",
    "            if reward == 0:\n",
    "                self.msg.value = \"<b>Draw.</b> Click Reset.\"\n",
    "            else:\n",
    "                winner = 'X' if -self.env.current_player == +1 else 'O'\n",
    "                self.msg.value = f\"<b>{winner} wins!</b> Click Reset.\"\n",
    "            for r in range(self.N):\n",
    "                for c in range(self.N):\n",
    "                    self.buttons[r][c].disabled = True\n",
    "        else:\n",
    "            turn = 'X' if self.env.current_player==+1 else 'O'\n",
    "            self.msg.value = f\"Player {turn} to move.\"\n",
    "\n",
    "    def reset(self, _=None):\n",
    "        self.env.reset()\n",
    "        if self.human_player == -1 and self.env.current_player == +1:\n",
    "            self.ai_move()\n",
    "        self.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055083b-7742-42c6-affc-441cf34cbaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "model.eval()\n",
    "ui = GomokuUI(model, N=9, win_len=5, sims=600, human_player=-1)  # human plays O"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
