{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9286a65-7939-4ae2-83dc-dfe41e5ecb33",
   "metadata": {},
   "source": [
    "# 1. Gomoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207a7abc-73dc-445d-a062-c579be416971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Gomoku:\n",
    "    \"\"\"\n",
    "    N×N board, players in {+1 (black), -1 (white)}, empty=0.\n",
    "    Win condition: 5 in a row (any direction).\n",
    "    \"\"\"\n",
    "    def __init__(self, N=9, win_len=5):\n",
    "        self.N = int(N)\n",
    "        self.win_len = int(win_len)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.N, self.N), dtype=np.int8)\n",
    "        self.current_player = 1\n",
    "        self.done = False\n",
    "        self.last_move = None  # (r,c) or None\n",
    "        return self.get_state()  # canonical 1×(N*N) if you want; see below\n",
    "\n",
    "    # --- Canonical features for NN (3 planes): me, opp, to-move ---\n",
    "    def to_planes(self):\n",
    "        me  = (self.board == self.current_player).astype(np.float32)\n",
    "        opp = (self.board == -self.current_player).astype(np.float32)\n",
    "        turn = np.full_like(me, 1.0, dtype=np.float32)  # “to move” plane\n",
    "        # shape: [3, N, N]\n",
    "        return np.stack([me, opp, turn], axis=0)\n",
    "\n",
    "    # For drop-in compatibility with your older code:\n",
    "    def get_state(self):\n",
    "        # vectorized canonical state (board * current_player) like tic-tac-toe\n",
    "        return (self.board.flatten() * self.current_player).astype(np.float32)\n",
    "\n",
    "    def get_legal_actions(self):\n",
    "        # actions are 0..N*N-1\n",
    "        flat = self.board.reshape(-1)\n",
    "        return [i for i in range(self.N * self.N) if flat[i] == 0]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply action index (0..N^2-1). Returns (state, reward, done).\"\"\"\n",
    "        if self.done:\n",
    "            return self.get_state(), 0.0, True\n",
    "        r, c = divmod(int(action), self.N)\n",
    "        if self.board[r, c] != 0:\n",
    "            # illegal move penalty (AlphaZero typically forbids via masking;\n",
    "            # but keep this as a safeguard)\n",
    "            self.done = True\n",
    "            return self.get_state(), -1.0, True\n",
    "\n",
    "        self.board[r, c] = self.current_player\n",
    "        self.last_move = (r, c)\n",
    "\n",
    "        reward, done = self.check_winner_fast()\n",
    "        self.done = done\n",
    "        state = self.get_state()\n",
    "        self.current_player *= -1\n",
    "        return state, float(reward), bool(done)\n",
    "\n",
    "    # --------- winner checks ----------\n",
    "    def check_winner_fast(self):\n",
    "        \"\"\"\n",
    "        Check only the lines passing through self.last_move for speed.\n",
    "        Returns (reward_for_player_who_just_moved, done)\n",
    "        reward ∈ {+1 win, 0 draw/ongoing, -1 loss} relative to the mover.\n",
    "        \"\"\"\n",
    "        if self.last_move is None:\n",
    "            return 0.0, False\n",
    "        r, c = self.last_move\n",
    "        p = self.board[r, c]\n",
    "        if p == 0:\n",
    "            return 0.0, False\n",
    "\n",
    "        if ( self._count_dir(r, c, 1, 0, p) + self._count_dir(r, c, -1, 0, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 0, 1, p) + self._count_dir(r, c, 0, -1, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 1, 1, p) + self._count_dir(r, c, -1, -1, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 1, -1, p) + self._count_dir(r, c, -1, 1, p) - 1 >= self.win_len ):\n",
    "            # player p (who just moved) wins: reward +1 for mover\n",
    "            return 1.0, True\n",
    "\n",
    "        if (self.board != 0).all():\n",
    "            return 0.0, True  # draw\n",
    "\n",
    "        return 0.0, False\n",
    "\n",
    "    def _count_dir(self, r, c, dr, dc, p):\n",
    "        \"\"\"Count contiguous stones with color p starting at (r,c) inclusive along (dr,dc).\"\"\"\n",
    "        N = self.N\n",
    "        cnt = 0\n",
    "        rr, cc = r, c\n",
    "        while 0 <= rr < N and 0 <= cc < N and self.board[rr, cc] == p:\n",
    "            cnt += 1\n",
    "            rr += dr\n",
    "            cc += dc\n",
    "        return cnt\n",
    "\n",
    "    # --------- convenience ----------\n",
    "    def render_ascii(self):\n",
    "        sym = {1:'X', -1:'O', 0:'.'}\n",
    "        print(\"\\n\".join(\" \".join(sym[v] for v in row) for row in self.board))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7745f-094c-42a4-ac6a-404e421c0b71",
   "metadata": {},
   "source": [
    "# 2. ResNet Policy-Value Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d937944f-353a-4802-a13e-2fe97c41c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(x + y)\n",
    "\n",
    "class PVResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy–Value net for Gomoku.\n",
    "    Input: [B, 3, N, N]\n",
    "    Outputs:\n",
    "      - policy_logits: [B, N*N]\n",
    "      - value:         [B, 1]  (tanh)\n",
    "    \"\"\"\n",
    "    def __init__(self, board_size=9, channels=64, n_blocks=6):\n",
    "        super().__init__()\n",
    "        self.N = board_size\n",
    "        C = channels\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, C, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(C),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.trunk = nn.Sequential(*[ResidualBlock(C) for _ in range(n_blocks)])\n",
    "\n",
    "        # Policy head\n",
    "        self.p_head = nn.Sequential(\n",
    "            nn.Conv2d(C, 2, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.p_fc = nn.Linear(2 * self.N * self.N, self.N * self.N)\n",
    "\n",
    "        # Value head\n",
    "        self.v_head = nn.Sequential(\n",
    "            nn.Conv2d(C, 1, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.v_fc1 = nn.Linear(1 * self.N * self.N, C)\n",
    "        self.v_fc2 = nn.Linear(C, 1)\n",
    "\n",
    "        # (optional) init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):  # x: [B, 3, N, N]\n",
    "        z = self.stem(x)\n",
    "        z = self.trunk(z)\n",
    "\n",
    "        # policy\n",
    "        p = self.p_head(z)\n",
    "        p = p.view(p.size(0), -1)\n",
    "        policy_logits = self.p_fc(p)  # [B, N*N]\n",
    "\n",
    "        # value\n",
    "        v = self.v_head(z)\n",
    "        v = v.view(v.size(0), -1)\n",
    "        v = F.relu(self.v_fc1(v))\n",
    "        value = torch.tanh(self.v_fc2(v))  # [-1, 1]\n",
    "\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074746e0-9ece-44e3-bc79-40358fdb2873",
   "metadata": {},
   "source": [
    "# 3. Monte Carlo Tree Search (MCTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e6c3cd-3923-4635-9351-cee810961de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gomoku_planes_from_raw(raw_board_tuple, player, N):\n",
    "    \"\"\"Build [3, N, N] planes from (raw_board, player_to_move).\"\"\"\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N)\n",
    "    me  = (b == player).astype(np.float32)\n",
    "    opp = (b == -player).astype(np.float32)\n",
    "    turn = np.ones_like(me, dtype=np.float32)\n",
    "    return np.stack([me, opp, turn], axis=0)\n",
    "\n",
    "def legal_actions_from_raw(raw_board_tuple, N):\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N)\n",
    "    return [i for i in range(N*N) if b.flat[i] == 0]\n",
    "\n",
    "def apply_action_raw(raw_board_tuple, action, player, N):\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N).copy()\n",
    "    r, c = divmod(int(action), N)\n",
    "    b[r, c] = player\n",
    "    return tuple(b.reshape(-1))\n",
    "\n",
    "def check_terminal_gomoku(raw_board_tuple, N, win_len=5):\n",
    "    \"\"\"Return (value_for_current_node_player, done). Full scan (O(N^2)), fine for 9x9.\"\"\"\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N)\n",
    "\n",
    "    def has_k(p):\n",
    "        # rows\n",
    "        for r in range(N):\n",
    "            run = 0\n",
    "            for c in range(N):\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "        # cols\n",
    "        for c in range(N):\n",
    "            run = 0\n",
    "            for r in range(N):\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "        # diag \\\n",
    "        for r0 in range(N):\n",
    "            run = 0\n",
    "            r, c = r0, 0\n",
    "            while r < N and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r += 1; c += 1\n",
    "        for c0 in range(1, N):\n",
    "            run = 0\n",
    "            r, c = 0, c0\n",
    "            while r < N and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r += 1; c += 1\n",
    "        # diag /\n",
    "        for r0 in range(N):\n",
    "            run = 0\n",
    "            r, c = r0, 0\n",
    "            while r >= 0 and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r -= 1; c += 1\n",
    "        for c0 in range(1, N):\n",
    "            run = 0\n",
    "            r, c = N-1, c0\n",
    "            while r >= 0 and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r -= 1; c += 1\n",
    "        return False\n",
    "\n",
    "    if has_k(+1): return (+1.0, True)\n",
    "    if has_k(-1): return (-1.0, True)\n",
    "    if (b != 0).all(): return (0.0, True)\n",
    "    return (0.0, False)\n",
    "\n",
    "# Opening bias\n",
    "def center_bias(N, sigma=0.35):\n",
    "    \"\"\"Return normalized N*N center-biased prior (Gaussian on [0,1]^2 grid).\"\"\"\n",
    "    xs = np.linspace(0, 1, N)\n",
    "    X, Y = np.meshgrid(xs, xs, indexing='ij')\n",
    "    cx = cy = 0.5\n",
    "    g = np.exp(-((X-cx)**2 + (Y-cy)**2) / (2*(sigma**2)))\n",
    "    v = g.reshape(-1).astype(np.float32)\n",
    "    v /= v.sum() + 1e-8\n",
    "    return v\n",
    "\n",
    "def mix_opening_bias(node, N, bias_vec, eps=0.25):\n",
    "    \"\"\"Blend bias_vec into child priors at the root (legal only).\"\"\"\n",
    "    if eps is None or eps <= 0.0:\n",
    "        return\n",
    "    legal = list(node.children.keys())\n",
    "    if not legal: return\n",
    "    b = bias_vec.copy()\n",
    "    # renorm on legal only\n",
    "    s = b[legal].sum()\n",
    "    b = b / (s + 1e-8)\n",
    "    for a in legal:\n",
    "        p = node.children[a].prior\n",
    "        node.children[a].prior = (1 - eps) * p + eps * float(b[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa8eaeb-17ca-4b4e-8727-7a773b80035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Node ----\n",
    "class Node:\n",
    "    def __init__(self, raw_board_tuple, player_to_move):\n",
    "        self.state = raw_board_tuple          # tuple of length N*N (0/±1)\n",
    "        self.player = int(player_to_move)     # +1 or -1\n",
    "        self.children = {}                    # action -> Node\n",
    "        self.visit_count = 0\n",
    "        self.total_value = 0.0                # mean value from this node's player perspective\n",
    "        self.prior = 0.0\n",
    "\n",
    "    def value(self):\n",
    "        return 0.0 if self.visit_count == 0 else self.total_value / self.visit_count\n",
    "\n",
    "# ---- MCTS ----\n",
    "class MCTS:\n",
    "    def __init__(self, model, simulations=800, c_puct=1.5, win_len=5):\n",
    "        self.model = model\n",
    "        self.simulations = simulations\n",
    "        self.c_puct = c_puct\n",
    "        self.win_len = win_len\n",
    "        self.ttable = {}  # (state_tuple, player, N) -> (priors_np[N*N], value_float)\n",
    "\n",
    "    # ----------------------------\n",
    "    def run(self, env, add_root_noise=True, dir_alpha=0.3, dir_eps=0.25,\n",
    "            opening_bias=None, opening_eps=0.25):\n",
    "        N = env.N\n",
    "        root = Node(tuple(env.board.reshape(-1)), env.current_player)\n",
    "        if not root.children:\n",
    "            _ = self._expand_and_evaluate(root, N)\n",
    "    \n",
    "        if add_root_noise:\n",
    "            self._add_dirichlet_noise(root, N, alpha=dir_alpha, eps=dir_eps)\n",
    "    \n",
    "        if opening_bias is not None:\n",
    "            mix_opening_bias(root, N, opening_bias, eps=opening_eps)\n",
    "    \n",
    "        for _ in range(self.simulations):\n",
    "            self._simulate(root, N)\n",
    "        return root\n",
    "\n",
    "    # ----------------------------\n",
    "    def _simulate(self, node, N):\n",
    "        # Terminal test\n",
    "        wval, done = check_terminal_gomoku(node.state, N, self.win_len)\n",
    "        if done:\n",
    "            v = float(wval)                      # value from node.player perspective\n",
    "            node.visit_count += 1\n",
    "            node.total_value += v\n",
    "            return v\n",
    "\n",
    "        # Leaf: not visited yet and no children → expand\n",
    "        if node.visit_count == 0 and not node.children:\n",
    "            v = self._expand_and_evaluate(node, N)\n",
    "            node.visit_count += 1\n",
    "            node.total_value += v\n",
    "            return v\n",
    "\n",
    "        # Select via PUCT\n",
    "        best_a, best_score = None, -1e9\n",
    "        sqrt_N = np.sqrt(node.visit_count + 1e-8)\n",
    "        for a, child in node.children.items():\n",
    "            ucb = child.value() + self.c_puct * child.prior * (sqrt_N / (1 + child.visit_count))\n",
    "            if ucb > best_score:\n",
    "                best_score, best_a = ucb, a\n",
    "\n",
    "        child = node.children[best_a]\n",
    "        v_child = self._simulate(child, N)\n",
    "        v = -v_child                              # perspective switch\n",
    "        node.visit_count += 1\n",
    "        node.total_value += v\n",
    "        return v\n",
    "\n",
    "    # ----------------------------\n",
    "    def _expand_and_evaluate(self, node, N):\n",
    "        key = (node.state, node.player, N)\n",
    "        if key in self.ttable:\n",
    "            priors, v = self.ttable[key]\n",
    "        else:\n",
    "            planes = gomoku_planes_from_raw(node.state, node.player, N)\n",
    "            st = torch.from_numpy(planes).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                logits, value = self.model(st)\n",
    "                logits = logits.squeeze(0)        # [N*N]\n",
    "                v = float(value.item())\n",
    "            policy = F.softmax(logits, dim=-1).cpu().numpy()  # length N*N\n",
    "\n",
    "            legal = legal_actions_from_raw(node.state, N)\n",
    "            if len(legal) == 0:\n",
    "                priors = np.zeros(N*N, dtype=np.float32)\n",
    "            else:\n",
    "                mask = np.full(N*N, -1e9, dtype=np.float32)\n",
    "                mask[legal] = 0.0\n",
    "                masked = np.log(policy + 1e-12) + mask  # numeric stable masking\n",
    "                masked -= masked.max()\n",
    "                priors = np.exp(masked)\n",
    "                s = priors[legal].sum()\n",
    "                priors = priors / (s + 1e-8)\n",
    "\n",
    "            self.ttable[key] = (priors.astype(np.float32), v)\n",
    "\n",
    "        # Create children\n",
    "        for a in np.where(self.ttable[key][0] > 0)[0]:\n",
    "            child_board = apply_action_raw(node.state, int(a), node.player, N)\n",
    "            child = Node(child_board, -node.player)\n",
    "            child.prior = float(self.ttable[key][0][a])\n",
    "            node.children[int(a)] = child\n",
    "\n",
    "        return v\n",
    "\n",
    "    # ----------------------------\n",
    "    def _add_dirichlet_noise(self, node, N, alpha=0.3, eps=0.25):\n",
    "        if not node.children:\n",
    "            return\n",
    "        legal_actions = list(node.children.keys())\n",
    "        if len(legal_actions) == 0:\n",
    "            return\n",
    "        noise = np.random.dirichlet([alpha] * len(legal_actions)).astype(np.float32)\n",
    "        for a, n in zip(legal_actions, noise):\n",
    "            p = node.children[a].prior\n",
    "            node.children[a].prior = (1 - eps) * p + eps * float(n)\n",
    "\n",
    "    # ----------------------------\n",
    "    def select_action(self, root, temperature=1.0):\n",
    "        N2 = int(np.sqrt(len(self.ttable.get((root.state, root.player, int(np.sqrt(len(root.state)))), (np.zeros(len(root.state)), 0.0))[0])) or len(root.state))\n",
    "        # safer: compute over children\n",
    "        visits = np.zeros(N2*N2, dtype=np.float64)\n",
    "        legal  = np.zeros(N2*N2, dtype=bool)\n",
    "        for a, child in root.children.items():\n",
    "            visits[a] = child.visit_count\n",
    "            legal[a] = True\n",
    "\n",
    "        legal_idxs = np.where(legal)[0]\n",
    "        if legal_idxs.size == 0:\n",
    "            # no children? pick any legal by state\n",
    "            legal_idxs = np.array(legal_actions_from_raw(root.state, N2), dtype=int)\n",
    "            if legal_idxs.size == 0:\n",
    "                return 0\n",
    "            return int(np.random.choice(legal_idxs))\n",
    "\n",
    "        if temperature == 0.0:\n",
    "            vmax = visits[legal_idxs].max()\n",
    "            ties = legal_idxs[visits[legal_idxs] == vmax]\n",
    "            return int(np.random.choice(ties))\n",
    "\n",
    "        x = visits[legal_idxs] ** (1.0 / temperature)\n",
    "        p = x / (x.sum() + 1e-8)\n",
    "        return int(np.random.choice(legal_idxs, p=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da4567-9942-4349-8600-0f7306eab903",
   "metadata": {},
   "source": [
    "# 4. Self-Play with Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8b0a0e-d470-4034-866b-c2157607d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np, torch\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(0); np.random.seed(0); torch.manual_seed(0)\n",
    "\n",
    "# Build env/model\n",
    "env = Gomoku(N=9, win_len=5)\n",
    "model = PVResNet(board_size=env.N, channels=64, n_blocks=6)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(model, simulations=800, c_puct=1.5, win_len=env.win_len)\n",
    "\n",
    "# One move:\n",
    "root = mcts.run(env, add_root_noise=True)\n",
    "a = mcts.select_action(root, temperature=1.0)   # temperature=1 in opening, 0 later\n",
    "state, reward, done = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a09fef56-6773-40c9-806e-2cbe04fb2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, math, collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 8 symmetry transforms for NxN boards\n",
    "def symmetries(planes):  # planes: [C,N,N] numpy\n",
    "    ps = []\n",
    "    for k in range(4):  # rotations\n",
    "        r = np.rot90(planes, k=k, axes=(1,2))\n",
    "        ps.append(r)\n",
    "        ps.append(np.flip(r, axis=1))  # mirror\n",
    "    return ps\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buf = collections.deque(maxlen=capacity)\n",
    "    def add(self, x): self.buf.append(x)\n",
    "    def sample(self, bs): return random.sample(self.buf, bs)\n",
    "    def __len__(self): return len(self.buf)\n",
    "\n",
    "# precompute once\n",
    "OPENING_BIAS = center_bias(N=env.N, sigma=0.35)\n",
    "\n",
    "\"\"\"\n",
    "# fixed ceiling for clarity\n",
    "MAX_SIMS = 800\n",
    "\n",
    "def sims_schedule(iter_idx, move_no, base=200, max_sims=MAX_SIMS, warmup_iters=20, ramp_moves=12):\n",
    "    # ramp with training iteration (more search later)\n",
    "    # ramp with move number (opening cheaper, mid/late richer)\n",
    "    \n",
    "    # ramp by iteration\n",
    "    f_iter = min(1.0, iter_idx / float(warmup_iters))\n",
    "    sims_iter = int(base + (max_sims - base) * f_iter)\n",
    "    # ramp by move number\n",
    "    f_move = min(1.0, move_no / float(ramp_moves))\n",
    "    sims = int(base + (sims_iter - base) * f_move)\n",
    "    return max(base, min(max_sims, sims))\n",
    "\n",
    "def opening_eps_schedule(iter_idx, move_no, max_eps=0.40, stop_iter=30, stop_move=10):\n",
    "    f_iter = max(0.0, 1.0 - iter_idx/float(stop_iter))\n",
    "    f_move = max(0.0, 1.0 - move_no/float(stop_move))\n",
    "    return max_eps * f_iter * f_move\n",
    "\n",
    "def self_play_episode(env, model, mcts, temp_moves=8, iter_idx=1):\n",
    "    env.reset()\n",
    "    # list of (planes, pi, player)\n",
    "    traj = []\n",
    "    done = False\n",
    "    move_no = 0\n",
    "    while not done:\n",
    "        # adjust sims on the fly\n",
    "        # mcts.simulations = sims_schedule(iter_idx, move_no, base=200, max_sims=mcts.simulations)\n",
    "        mcts.simulations = sims_schedule(iter_idx, move_no, base=200, max_sims=MAX_SIMS)\n",
    "        eps_open = opening_eps_schedule(iter_idx, move_no)\n",
    "        root = mcts.run(\n",
    "            env, \n",
    "            add_root_noise=True, \n",
    "            dir_alpha=0.15, \n",
    "            dir_eps=0.25,\n",
    "            opening_bias=OPENING_BIAS,\n",
    "            opening_eps=eps_open\n",
    "        )\n",
    "        T = 1.0 if move_no < temp_moves else 0.0\n",
    "        a = mcts.select_action(root, temperature=T)\n",
    "\n",
    "        # build visit-count policy target π\n",
    "        pi = np.zeros(env.N * env.N, dtype=np.float32)\n",
    "        for aa, child in root.children.items():\n",
    "            pi[aa] = child.visit_count\n",
    "        pi /= (pi.sum() + 1e-8)\n",
    "\n",
    "        planes = env.to_planes()\n",
    "        traj.append((planes, pi, env.current_player))\n",
    "\n",
    "        _, reward, done = env.step(a)\n",
    "        move_no += 1\n",
    "\n",
    "    # assign final outcome z to each step from that step's player perspective\n",
    "    data = []\n",
    "    for planes, pi, player in traj:\n",
    "        # +1 win for player's perspective, 0 draw, -1 loss\n",
    "        z = reward * player  \n",
    "        # 8 sym augmentations\n",
    "        for sp in symmetries(planes):\n",
    "            data.append((sp.astype(np.float32), pi.copy(), float(z)))\n",
    "        # list of (planes[3,N,N], pi[N*N], z)\n",
    "    return data\n",
    "\"\"\"\n",
    "\n",
    "# ----- curriculum knobs -----\n",
    "CURRICULUM = dict(\n",
    "    max_sims=800,          # stronger ceiling later if you like (e.g., 1600)\n",
    "    base_sims=200,         # very cheap opening search\n",
    "    sims_warmup_iters=40,  # how many iterations to reach near max_sims\n",
    "    sims_ramp_moves=15,    # ramp sims across the opening moves\n",
    "    opening_max_eps=0.40,  # strength of center prior mix at start\n",
    "    opening_stop_iter=30,  # how fast to fade opening bias across iters\n",
    "    opening_stop_move=10,  # ... and across moves\n",
    "    temp_opening_moves=8   # use T=1 for first K moves, then T=0\n",
    ")\n",
    "\n",
    "def sims_schedule(iter_idx, move_no, cfg=CURRICULUM):\n",
    "    base, maxs = cfg['base_sims'], cfg['max_sims']\n",
    "    f_iter = min(1.0, iter_idx / float(cfg['sims_warmup_iters']))\n",
    "    sims_iter = int(base + (maxs - base) * f_iter)\n",
    "    f_move = min(1.0, move_no / float(cfg['sims_ramp_moves']))\n",
    "    sims = int(base + (sims_iter - base) * f_move)\n",
    "    return max(base, min(maxs, sims))\n",
    "\n",
    "def opening_eps_schedule(iter_idx, move_no, cfg=CURRICULUM):\n",
    "    max_eps = cfg['opening_max_eps']\n",
    "    f_iter = max(0.0, 1.0 - iter_idx/float(cfg['opening_stop_iter']))\n",
    "    f_move = max(0.0, 1.0 - move_no/float(cfg['opening_stop_move']))\n",
    "    return max_eps * f_iter * f_move\n",
    "\n",
    "def temperature_for_move(move_no, cfg=CURRICULUM):\n",
    "    return 1.0 if move_no < cfg['temp_opening_moves'] else 0.0\n",
    "\n",
    "def self_play_episode(env, model, mcts, iter_idx=1, cfg=CURRICULUM):\n",
    "    env.reset()\n",
    "    traj, done, move_no = [], False, 0\n",
    "    while not done:\n",
    "        mcts.simulations = sims_schedule(iter_idx, move_no, cfg)\n",
    "        eps_open = opening_eps_schedule(iter_idx, move_no, cfg)\n",
    "        root = mcts.run(\n",
    "            env,\n",
    "            add_root_noise=True,\n",
    "            dir_alpha=0.15, dir_eps=0.25,\n",
    "            opening_bias=center_bias(env.N, sigma=0.35),\n",
    "            opening_eps=eps_open\n",
    "        )\n",
    "        T = temperature_for_move(move_no, cfg)\n",
    "        a = mcts.select_action(root, temperature=T)\n",
    "\n",
    "        pi = np.zeros(env.N * env.N, dtype=np.float32)\n",
    "        for aa, child in root.children.items(): pi[aa] = child.visit_count\n",
    "        pi /= (pi.sum() + 1e-8)\n",
    "\n",
    "        traj.append((env.to_planes(), pi, env.current_player))\n",
    "        _, reward, done = env.step(a)\n",
    "        move_no += 1\n",
    "\n",
    "    data = []\n",
    "    for planes, pi, player in traj:\n",
    "        z = reward * player\n",
    "        for sp in symmetries(planes):\n",
    "            data.append((sp.astype(np.float32), pi.copy(), float(z)))\n",
    "    return data\n",
    "\n",
    "def train_step(model, optimizer, batch, N, weight_decay=1e-4):\n",
    "    # batch: list of tuples\n",
    "    planes = torch.from_numpy(np.stack([b[0] for b in batch]))    # [B,3,N,N]\n",
    "    pi      = torch.from_numpy(np.stack([b[1] for b in batch]))   # [B,N*N]\n",
    "    z       = torch.tensor([[b[2]] for b in batch], dtype=torch.float32)\n",
    "\n",
    "    logits, value = model(planes)\n",
    "    policy_loss = -(pi * F.log_softmax(logits, dim=1)).sum(dim=1).mean()\n",
    "    value_loss  = F.mse_loss(value, z)\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # decoupled weight decay (AdamW style)\n",
    "    optimizer.step()\n",
    "    return float(loss.item()), float(policy_loss.item()), float(value_loss.item())\n",
    "\n",
    "def random_action(env):\n",
    "    return random.choice(env.get_legal_actions())\n",
    "\n",
    "def evaluate_vs_random(model, sims=800, games=50, N=9, win_len=5):\n",
    "    wins = draws = losses = 0\n",
    "    for g in range(games):\n",
    "        env = Gomoku(N=N, win_len=win_len)\n",
    "        mcts = MCTS(model, simulations=sims, c_puct=1.5, win_len=win_len)\n",
    "        # alternate who starts\n",
    "        human_like = (+1 if g % 2 == 0 else -1)  # model plays both colors\n",
    "        env.current_player = human_like\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.current_player == human_like:\n",
    "                root = mcts.run(env, add_root_noise=False)  # no root noise in eval\n",
    "                a = mcts.select_action(root, temperature=0.0)\n",
    "            else:\n",
    "                a = random_action(env)\n",
    "            _, reward, done = env.step(a)\n",
    "\n",
    "        # reward is from the perspective of the player who moved last\n",
    "        # winner = -env.current_player  (player who just moved)\n",
    "        if reward == 0:\n",
    "            draws += 1\n",
    "        else:\n",
    "            winner = -env.current_player\n",
    "            if winner == human_like:\n",
    "                wins += 1\n",
    "            else:\n",
    "                losses += 1\n",
    "    print(f\"[Eval vs random] W {wins}  D {draws}  L {losses}  (games={games})\")\n",
    "    return wins, draws, losses\n",
    "\n",
    "# ------------- main training loop (sketch) -------------\n",
    "def train_gomoku(num_iters=200, episodes_per_iter=20, sims=800, batch_size=256):\n",
    "    env = Gomoku(N=9, win_len=5)\n",
    "    model = PVResNet(board_size=env.N, channels=64, n_blocks=6)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_iters)\n",
    "    mcts = MCTS(model, simulations=sims, c_puct=1.5, win_len=env.win_len)\n",
    "    replay = Replay(100_000)\n",
    "    \n",
    "    for it in range(1, num_iters+1):\n",
    "        # Self-play\n",
    "        for _ in range(episodes_per_iter):\n",
    "            # data = self_play_episode(env, model, mcts, temp_moves=8, iter_idx=it)\n",
    "            data = self_play_episode(env, model, mcts, iter_idx=it, cfg=CURRICULUM)\n",
    "            for item in data:\n",
    "                replay.add(item)\n",
    "\n",
    "        # Train\n",
    "        losses = []\n",
    "        for _ in range(min(400, len(replay)//batch_size) ):\n",
    "            batch = replay.sample(batch_size)\n",
    "            l, pl, vl = train_step(model, optimizer, batch, env.N)\n",
    "            losses.append(l)\n",
    "        sched.step()\n",
    "\n",
    "        print(f\"Iter {it:03d} | replay {len(replay)} | sims {sims} | lr {sched.get_last_lr()[0]:.4e} | loss {np.mean(losses) if losses else 0:.3f}\")\n",
    "\n",
    "        if it % 5 == 0:\n",
    "            evaluate_vs_random(model, sims=400, games=20, N=env.N, win_len=env.win_len)\n",
    "\n",
    "        if it % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"models/gomoku_alpha_zero_iter{it:03d}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"models/gomoku_alpha_zero.pth\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60033c42-23d3-48be-af4a-30de9d67be59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 001 | replay 6520 | sims 800 | lr 9.9994e-04 | loss 5.065\n",
      "Iter 002 | replay 12168 | sims 800 | lr 9.9975e-04 | loss 4.438\n",
      "Iter 003 | replay 17304 | sims 800 | lr 9.9944e-04 | loss 4.223\n",
      "Iter 004 | replay 22392 | sims 800 | lr 9.9901e-04 | loss 4.184\n",
      "Iter 005 | replay 27288 | sims 800 | lr 9.9846e-04 | loss 4.172\n",
      "[Eval vs random] W 16  D 0  L 4  (games=20)\n",
      "Iter 006 | replay 33528 | sims 800 | lr 9.9778e-04 | loss 4.120\n",
      "Iter 007 | replay 38912 | sims 800 | lr 9.9698e-04 | loss 4.107\n",
      "Iter 008 | replay 44320 | sims 800 | lr 9.9606e-04 | loss 4.037\n",
      "Iter 009 | replay 50096 | sims 800 | lr 9.9501e-04 | loss 4.033\n",
      "Iter 010 | replay 55056 | sims 800 | lr 9.9384e-04 | loss 3.980\n",
      "[Eval vs random] W 14  D 1  L 5  (games=20)\n",
      "Iter 011 | replay 59808 | sims 800 | lr 9.9255e-04 | loss 3.940\n",
      "Iter 012 | replay 67056 | sims 800 | lr 9.9114e-04 | loss 3.957\n",
      "Iter 013 | replay 72016 | sims 800 | lr 9.8961e-04 | loss 3.911\n",
      "Iter 014 | replay 76096 | sims 800 | lr 9.8796e-04 | loss 3.872\n",
      "Iter 015 | replay 81184 | sims 800 | lr 9.8618e-04 | loss 3.843\n",
      "[Eval vs random] W 12  D 1  L 7  (games=20)\n",
      "Iter 016 | replay 85944 | sims 800 | lr 9.8429e-04 | loss 3.813\n",
      "Iter 017 | replay 90616 | sims 800 | lr 9.8228e-04 | loss 3.784\n",
      "Iter 018 | replay 95464 | sims 800 | lr 9.8015e-04 | loss 3.771\n",
      "Iter 019 | replay 100000 | sims 800 | lr 9.7790e-04 | loss 3.754\n",
      "Iter 020 | replay 100000 | sims 800 | lr 9.7553e-04 | loss 3.727\n",
      "[Eval vs random] W 17  D 0  L 3  (games=20)\n",
      "Iter 021 | replay 100000 | sims 800 | lr 9.7304e-04 | loss 3.701\n",
      "Iter 022 | replay 100000 | sims 800 | lr 9.7044e-04 | loss 3.686\n",
      "Iter 023 | replay 100000 | sims 800 | lr 9.6772e-04 | loss 3.652\n",
      "Iter 024 | replay 100000 | sims 800 | lr 9.6489e-04 | loss 3.633\n",
      "Iter 025 | replay 100000 | sims 800 | lr 9.6194e-04 | loss 3.616\n",
      "[Eval vs random] W 13  D 0  L 7  (games=20)\n",
      "Iter 026 | replay 100000 | sims 800 | lr 9.5888e-04 | loss 3.594\n",
      "Iter 027 | replay 100000 | sims 800 | lr 9.5570e-04 | loss 3.584\n",
      "Iter 028 | replay 100000 | sims 800 | lr 9.5241e-04 | loss 3.556\n",
      "Iter 029 | replay 100000 | sims 800 | lr 9.4901e-04 | loss 3.541\n",
      "Iter 030 | replay 100000 | sims 800 | lr 9.4550e-04 | loss 3.524\n",
      "[Eval vs random] W 13  D 0  L 7  (games=20)\n",
      "Iter 031 | replay 100000 | sims 800 | lr 9.4188e-04 | loss 3.512\n",
      "Iter 032 | replay 100000 | sims 800 | lr 9.3815e-04 | loss 3.493\n",
      "Iter 033 | replay 100000 | sims 800 | lr 9.3432e-04 | loss 3.464\n",
      "Iter 034 | replay 100000 | sims 800 | lr 9.3037e-04 | loss 3.457\n",
      "Iter 035 | replay 100000 | sims 800 | lr 9.2632e-04 | loss 3.433\n",
      "[Eval vs random] W 14  D 0  L 6  (games=20)\n",
      "Iter 036 | replay 100000 | sims 800 | lr 9.2216e-04 | loss 3.430\n",
      "Iter 037 | replay 100000 | sims 800 | lr 9.1790e-04 | loss 3.412\n",
      "Iter 038 | replay 100000 | sims 800 | lr 9.1354e-04 | loss 3.420\n",
      "Iter 039 | replay 100000 | sims 800 | lr 9.0907e-04 | loss 3.420\n",
      "Iter 040 | replay 100000 | sims 800 | lr 9.0451e-04 | loss 3.405\n",
      "[Eval vs random] W 17  D 0  L 3  (games=20)\n",
      "Iter 041 | replay 100000 | sims 800 | lr 8.9984e-04 | loss 3.379\n",
      "Iter 042 | replay 100000 | sims 800 | lr 8.9508e-04 | loss 3.369\n",
      "Iter 043 | replay 100000 | sims 800 | lr 8.9022e-04 | loss 3.354\n",
      "Iter 044 | replay 100000 | sims 800 | lr 8.8526e-04 | loss 3.335\n",
      "Iter 045 | replay 100000 | sims 800 | lr 8.8020e-04 | loss 3.323\n",
      "[Eval vs random] W 14  D 0  L 6  (games=20)\n",
      "Iter 046 | replay 100000 | sims 800 | lr 8.7506e-04 | loss 3.301\n",
      "Iter 047 | replay 100000 | sims 800 | lr 8.6982e-04 | loss 3.299\n",
      "Iter 048 | replay 100000 | sims 800 | lr 8.6448e-04 | loss 3.295\n",
      "Iter 049 | replay 100000 | sims 800 | lr 8.5906e-04 | loss 3.297\n",
      "Iter 050 | replay 100000 | sims 800 | lr 8.5355e-04 | loss 3.289\n",
      "[Eval vs random] W 15  D 1  L 4  (games=20)\n",
      "Iter 051 | replay 100000 | sims 800 | lr 8.4796e-04 | loss 3.268\n",
      "Iter 052 | replay 100000 | sims 800 | lr 8.4227e-04 | loss 3.259\n",
      "Iter 053 | replay 100000 | sims 800 | lr 8.3651e-04 | loss 3.265\n",
      "Iter 054 | replay 100000 | sims 800 | lr 8.3066e-04 | loss 3.260\n",
      "Iter 055 | replay 100000 | sims 800 | lr 8.2472e-04 | loss 3.254\n",
      "[Eval vs random] W 13  D 0  L 7  (games=20)\n",
      "Iter 056 | replay 100000 | sims 800 | lr 8.1871e-04 | loss 3.251\n",
      "Iter 057 | replay 100000 | sims 800 | lr 8.1262e-04 | loss 3.238\n",
      "Iter 058 | replay 100000 | sims 800 | lr 8.0645e-04 | loss 3.223\n",
      "Iter 059 | replay 100000 | sims 800 | lr 8.0021e-04 | loss 3.207\n",
      "Iter 060 | replay 100000 | sims 800 | lr 7.9389e-04 | loss 3.213\n",
      "[Eval vs random] W 14  D 0  L 6  (games=20)\n",
      "Iter 061 | replay 100000 | sims 800 | lr 7.8750e-04 | loss 3.235\n",
      "Iter 062 | replay 100000 | sims 800 | lr 7.8104e-04 | loss 3.216\n",
      "Iter 063 | replay 100000 | sims 800 | lr 7.7451e-04 | loss 3.208\n",
      "Iter 064 | replay 100000 | sims 800 | lr 7.6791e-04 | loss 3.192\n",
      "Iter 065 | replay 100000 | sims 800 | lr 7.6125e-04 | loss 3.184\n",
      "[Eval vs random] W 14  D 0  L 6  (games=20)\n",
      "Iter 066 | replay 100000 | sims 800 | lr 7.5452e-04 | loss 3.192\n",
      "Iter 067 | replay 100000 | sims 800 | lr 7.4773e-04 | loss 3.188\n",
      "Iter 068 | replay 100000 | sims 800 | lr 7.4088e-04 | loss 3.180\n",
      "Iter 069 | replay 100000 | sims 800 | lr 7.3396e-04 | loss 3.170\n",
      "Iter 070 | replay 100000 | sims 800 | lr 7.2700e-04 | loss 3.166\n",
      "[Eval vs random] W 13  D 0  L 7  (games=20)\n",
      "Iter 071 | replay 100000 | sims 800 | lr 7.1997e-04 | loss 3.153\n",
      "Iter 072 | replay 100000 | sims 800 | lr 7.1289e-04 | loss 3.157\n",
      "Iter 073 | replay 100000 | sims 800 | lr 7.0576e-04 | loss 3.169\n",
      "Iter 074 | replay 100000 | sims 800 | lr 6.9857e-04 | loss 3.168\n",
      "Iter 075 | replay 100000 | sims 800 | lr 6.9134e-04 | loss 3.164\n",
      "[Eval vs random] W 15  D 0  L 5  (games=20)\n",
      "Iter 076 | replay 100000 | sims 800 | lr 6.8406e-04 | loss 3.170\n",
      "Iter 077 | replay 100000 | sims 800 | lr 6.7674e-04 | loss 3.177\n",
      "Iter 078 | replay 100000 | sims 800 | lr 6.6937e-04 | loss 3.180\n",
      "Iter 079 | replay 100000 | sims 800 | lr 6.6196e-04 | loss 3.183\n",
      "Iter 080 | replay 100000 | sims 800 | lr 6.5451e-04 | loss 3.191\n",
      "[Eval vs random] W 14  D 1  L 5  (games=20)\n",
      "Iter 081 | replay 100000 | sims 800 | lr 6.4702e-04 | loss 3.185\n",
      "Iter 082 | replay 100000 | sims 800 | lr 6.3950e-04 | loss 3.190\n",
      "Iter 083 | replay 100000 | sims 800 | lr 6.3194e-04 | loss 3.192\n",
      "Iter 084 | replay 100000 | sims 800 | lr 6.2434e-04 | loss 3.195\n",
      "Iter 085 | replay 100000 | sims 800 | lr 6.1672e-04 | loss 3.176\n",
      "[Eval vs random] W 15  D 1  L 4  (games=20)\n",
      "Iter 086 | replay 100000 | sims 800 | lr 6.0907e-04 | loss 3.183\n",
      "Iter 087 | replay 100000 | sims 800 | lr 6.0139e-04 | loss 3.189\n",
      "Iter 088 | replay 100000 | sims 800 | lr 5.9369e-04 | loss 3.215\n",
      "Iter 089 | replay 100000 | sims 800 | lr 5.8596e-04 | loss 3.216\n",
      "Iter 090 | replay 100000 | sims 800 | lr 5.7822e-04 | loss 3.223\n",
      "[Eval vs random] W 11  D 0  L 9  (games=20)\n",
      "Iter 091 | replay 100000 | sims 800 | lr 5.7045e-04 | loss 3.241\n",
      "Iter 092 | replay 100000 | sims 800 | lr 5.6267e-04 | loss 3.250\n",
      "Iter 093 | replay 100000 | sims 800 | lr 5.5487e-04 | loss 3.228\n",
      "Iter 094 | replay 100000 | sims 800 | lr 5.4705e-04 | loss 3.234\n",
      "Iter 095 | replay 100000 | sims 800 | lr 5.3923e-04 | loss 3.239\n",
      "[Eval vs random] W 15  D 1  L 4  (games=20)\n",
      "Iter 096 | replay 100000 | sims 800 | lr 5.3140e-04 | loss 3.233\n",
      "Iter 097 | replay 100000 | sims 800 | lr 5.2355e-04 | loss 3.248\n",
      "Iter 098 | replay 100000 | sims 800 | lr 5.1571e-04 | loss 3.254\n",
      "Iter 099 | replay 100000 | sims 800 | lr 5.0785e-04 | loss 3.242\n",
      "Iter 100 | replay 100000 | sims 800 | lr 5.0000e-04 | loss 3.245\n",
      "[Eval vs random] W 16  D 1  L 3  (games=20)\n",
      "Iter 101 | replay 100000 | sims 800 | lr 4.9215e-04 | loss 3.238\n",
      "Iter 102 | replay 100000 | sims 800 | lr 4.8429e-04 | loss 3.248\n",
      "Iter 103 | replay 100000 | sims 800 | lr 4.7645e-04 | loss 3.246\n",
      "Iter 104 | replay 100000 | sims 800 | lr 4.6860e-04 | loss 3.245\n",
      "Iter 105 | replay 100000 | sims 800 | lr 4.6077e-04 | loss 3.258\n",
      "[Eval vs random] W 14  D 0  L 6  (games=20)\n",
      "Iter 106 | replay 100000 | sims 800 | lr 4.5295e-04 | loss 3.259\n",
      "Iter 107 | replay 100000 | sims 800 | lr 4.4513e-04 | loss 3.288\n",
      "Iter 108 | replay 100000 | sims 800 | lr 4.3733e-04 | loss 3.281\n",
      "Iter 109 | replay 100000 | sims 800 | lr 4.2955e-04 | loss 3.300\n",
      "Iter 110 | replay 100000 | sims 800 | lr 4.2178e-04 | loss 3.292\n",
      "[Eval vs random] W 14  D 1  L 5  (games=20)\n",
      "Iter 111 | replay 100000 | sims 800 | lr 4.1404e-04 | loss 3.298\n",
      "Iter 112 | replay 100000 | sims 800 | lr 4.0631e-04 | loss 3.306\n",
      "Iter 113 | replay 100000 | sims 800 | lr 3.9861e-04 | loss 3.303\n",
      "Iter 114 | replay 100000 | sims 800 | lr 3.9093e-04 | loss 3.296\n",
      "Iter 115 | replay 100000 | sims 800 | lr 3.8328e-04 | loss 3.296\n",
      "[Eval vs random] W 16  D 0  L 4  (games=20)\n",
      "Iter 116 | replay 100000 | sims 800 | lr 3.7566e-04 | loss 3.320\n",
      "Iter 117 | replay 100000 | sims 800 | lr 3.6806e-04 | loss 3.339\n",
      "Iter 118 | replay 100000 | sims 800 | lr 3.6050e-04 | loss 3.359\n",
      "Iter 119 | replay 100000 | sims 800 | lr 3.5298e-04 | loss 3.363\n",
      "Iter 120 | replay 100000 | sims 800 | lr 3.4549e-04 | loss 3.377\n",
      "[Eval vs random] W 15  D 1  L 4  (games=20)\n",
      "Iter 121 | replay 100000 | sims 800 | lr 3.3804e-04 | loss 3.368\n",
      "Iter 122 | replay 100000 | sims 800 | lr 3.3063e-04 | loss 3.368\n",
      "Iter 123 | replay 100000 | sims 800 | lr 3.2326e-04 | loss 3.364\n",
      "Iter 124 | replay 100000 | sims 800 | lr 3.1594e-04 | loss 3.364\n",
      "Iter 125 | replay 100000 | sims 800 | lr 3.0866e-04 | loss 3.352\n",
      "[Eval vs random] W 13  D 0  L 7  (games=20)\n",
      "Iter 126 | replay 100000 | sims 800 | lr 3.0143e-04 | loss 3.361\n",
      "Iter 127 | replay 100000 | sims 800 | lr 2.9424e-04 | loss 3.397\n",
      "Iter 128 | replay 100000 | sims 800 | lr 2.8711e-04 | loss 3.391\n",
      "Iter 129 | replay 100000 | sims 800 | lr 2.8003e-04 | loss 3.400\n",
      "Iter 130 | replay 100000 | sims 800 | lr 2.7300e-04 | loss 3.404\n",
      "[Eval vs random] W 15  D 0  L 5  (games=20)\n",
      "Iter 131 | replay 100000 | sims 800 | lr 2.6604e-04 | loss 3.411\n",
      "Iter 132 | replay 100000 | sims 800 | lr 2.5912e-04 | loss 3.422\n",
      "Iter 133 | replay 100000 | sims 800 | lr 2.5227e-04 | loss 3.412\n",
      "Iter 134 | replay 100000 | sims 800 | lr 2.4548e-04 | loss 3.410\n",
      "Iter 135 | replay 100000 | sims 800 | lr 2.3875e-04 | loss 3.427\n",
      "[Eval vs random] W 18  D 0  L 2  (games=20)\n",
      "Iter 136 | replay 100000 | sims 800 | lr 2.3209e-04 | loss 3.435\n",
      "Iter 137 | replay 100000 | sims 800 | lr 2.2549e-04 | loss 3.454\n",
      "Iter 138 | replay 100000 | sims 800 | lr 2.1896e-04 | loss 3.438\n",
      "Iter 139 | replay 100000 | sims 800 | lr 2.1250e-04 | loss 3.440\n",
      "Iter 140 | replay 100000 | sims 800 | lr 2.0611e-04 | loss 3.446\n",
      "[Eval vs random] W 16  D 0  L 4  (games=20)\n",
      "Iter 141 | replay 100000 | sims 800 | lr 1.9979e-04 | loss 3.450\n",
      "Iter 142 | replay 100000 | sims 800 | lr 1.9355e-04 | loss 3.469\n",
      "Iter 143 | replay 100000 | sims 800 | lr 1.8738e-04 | loss 3.460\n",
      "Iter 144 | replay 100000 | sims 800 | lr 1.8129e-04 | loss 3.476\n",
      "Iter 145 | replay 100000 | sims 800 | lr 1.7528e-04 | loss 3.467\n",
      "[Eval vs random] W 14  D 0  L 6  (games=20)\n",
      "Iter 146 | replay 100000 | sims 800 | lr 1.6934e-04 | loss 3.467\n",
      "Iter 147 | replay 100000 | sims 800 | lr 1.6349e-04 | loss 3.480\n",
      "Iter 148 | replay 100000 | sims 800 | lr 1.5773e-04 | loss 3.480\n",
      "Iter 149 | replay 100000 | sims 800 | lr 1.5204e-04 | loss 3.498\n",
      "Iter 150 | replay 100000 | sims 800 | lr 1.4645e-04 | loss 3.495\n",
      "[Eval vs random] W 16  D 0  L 4  (games=20)\n",
      "Iter 151 | replay 100000 | sims 800 | lr 1.4094e-04 | loss 3.498\n",
      "Iter 152 | replay 100000 | sims 800 | lr 1.3552e-04 | loss 3.497\n",
      "Iter 153 | replay 100000 | sims 800 | lr 1.3018e-04 | loss 3.512\n",
      "Iter 154 | replay 100000 | sims 800 | lr 1.2494e-04 | loss 3.517\n",
      "Iter 155 | replay 100000 | sims 800 | lr 1.1980e-04 | loss 3.534\n",
      "[Eval vs random] W 18  D 0  L 2  (games=20)\n",
      "Iter 156 | replay 100000 | sims 800 | lr 1.1474e-04 | loss 3.542\n",
      "Iter 157 | replay 100000 | sims 800 | lr 1.0978e-04 | loss 3.556\n",
      "Iter 158 | replay 100000 | sims 800 | lr 1.0492e-04 | loss 3.547\n",
      "Iter 159 | replay 100000 | sims 800 | lr 1.0016e-04 | loss 3.561\n",
      "Iter 160 | replay 100000 | sims 800 | lr 9.5492e-05 | loss 3.586\n",
      "[Eval vs random] W 16  D 0  L 4  (games=20)\n",
      "Iter 161 | replay 100000 | sims 800 | lr 9.0925e-05 | loss 3.589\n",
      "Iter 162 | replay 100000 | sims 800 | lr 8.6460e-05 | loss 3.593\n",
      "Iter 163 | replay 100000 | sims 800 | lr 8.2096e-05 | loss 3.612\n",
      "Iter 164 | replay 100000 | sims 800 | lr 7.7836e-05 | loss 3.621\n",
      "Iter 165 | replay 100000 | sims 800 | lr 7.3680e-05 | loss 3.629\n",
      "[Eval vs random] W 13  D 1  L 6  (games=20)\n",
      "Iter 166 | replay 100000 | sims 800 | lr 6.9629e-05 | loss 3.633\n",
      "Iter 167 | replay 100000 | sims 800 | lr 6.5684e-05 | loss 3.665\n",
      "Iter 168 | replay 100000 | sims 800 | lr 6.1847e-05 | loss 3.657\n",
      "Iter 169 | replay 100000 | sims 800 | lr 5.8117e-05 | loss 3.667\n",
      "Iter 170 | replay 100000 | sims 800 | lr 5.4497e-05 | loss 3.669\n",
      "[Eval vs random] W 14  D 0  L 6  (games=20)\n",
      "Iter 171 | replay 100000 | sims 800 | lr 5.0986e-05 | loss 3.679\n",
      "Iter 172 | replay 100000 | sims 800 | lr 4.7586e-05 | loss 3.674\n",
      "Iter 173 | replay 100000 | sims 800 | lr 4.4298e-05 | loss 3.689\n",
      "Iter 174 | replay 100000 | sims 800 | lr 4.1123e-05 | loss 3.701\n",
      "Iter 175 | replay 100000 | sims 800 | lr 3.8060e-05 | loss 3.713\n",
      "[Eval vs random] W 14  D 0  L 6  (games=20)\n",
      "Iter 176 | replay 100000 | sims 800 | lr 3.5112e-05 | loss 3.717\n",
      "Iter 177 | replay 100000 | sims 800 | lr 3.2278e-05 | loss 3.718\n",
      "Iter 178 | replay 100000 | sims 800 | lr 2.9560e-05 | loss 3.724\n",
      "Iter 179 | replay 100000 | sims 800 | lr 2.6957e-05 | loss 3.725\n",
      "Iter 180 | replay 100000 | sims 800 | lr 2.4472e-05 | loss 3.744\n",
      "[Eval vs random] W 13  D 0  L 7  (games=20)\n",
      "Iter 181 | replay 100000 | sims 800 | lr 2.2103e-05 | loss 3.752\n",
      "Iter 182 | replay 100000 | sims 800 | lr 1.9853e-05 | loss 3.759\n",
      "Iter 183 | replay 100000 | sims 800 | lr 1.7721e-05 | loss 3.767\n",
      "Iter 184 | replay 100000 | sims 800 | lr 1.5708e-05 | loss 3.782\n",
      "Iter 185 | replay 100000 | sims 800 | lr 1.3815e-05 | loss 3.792\n",
      "[Eval vs random] W 14  D 1  L 5  (games=20)\n",
      "Iter 186 | replay 100000 | sims 800 | lr 1.2042e-05 | loss 3.793\n",
      "Iter 187 | replay 100000 | sims 800 | lr 1.0389e-05 | loss 3.807\n",
      "Iter 188 | replay 100000 | sims 800 | lr 8.8564e-06 | loss 3.813\n",
      "Iter 189 | replay 100000 | sims 800 | lr 7.4453e-06 | loss 3.815\n",
      "Iter 190 | replay 100000 | sims 800 | lr 6.1558e-06 | loss 3.825\n",
      "[Eval vs random] W 16  D 0  L 4  (games=20)\n",
      "Iter 191 | replay 100000 | sims 800 | lr 4.9882e-06 | loss 3.837\n",
      "Iter 192 | replay 100000 | sims 800 | lr 3.9426e-06 | loss 3.848\n",
      "Iter 193 | replay 100000 | sims 800 | lr 3.0195e-06 | loss 3.857\n",
      "Iter 194 | replay 100000 | sims 800 | lr 2.2190e-06 | loss 3.867\n",
      "Iter 195 | replay 100000 | sims 800 | lr 1.5413e-06 | loss 3.872\n",
      "[Eval vs random] W 16  D 1  L 3  (games=20)\n",
      "Iter 196 | replay 100000 | sims 800 | lr 9.8664e-07 | loss 3.884\n",
      "Iter 197 | replay 100000 | sims 800 | lr 5.5506e-07 | loss 3.890\n",
      "Iter 198 | replay 100000 | sims 800 | lr 2.4672e-07 | loss 3.900\n",
      "Iter 199 | replay 100000 | sims 800 | lr 6.1684e-08 | loss 3.904\n",
      "Iter 200 | replay 100000 | sims 800 | lr 0.0000e+00 | loss 3.915\n",
      "[Eval vs random] W 16  D 0  L 4  (games=20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PVResNet(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (trunk): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (p_head): Sequential(\n",
       "    (0): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (p_fc): Linear(in_features=162, out_features=81, bias=True)\n",
       "  (v_head): Sequential(\n",
       "    (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (v_fc1): Linear(in_features=81, out_features=64, bias=True)\n",
       "  (v_fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gomoku()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500607d0-5ccc-4750-8364-a61f22098fde",
   "metadata": {},
   "source": [
    "# 5. Strength Ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f01ad424-5288-4808-8f83-bd3a2df30cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strength ladder\n",
    "def eval_vs_mcts(model, opp_sims, games=40, N=9, win_len=5):\n",
    "    wins = draws = losses = 0\n",
    "    for g in range(games):\n",
    "        env = Gomoku(N=N, win_len=win_len)\n",
    "        # alternate who starts\n",
    "        env.current_player = +1 if (g % 2 == 0) else -1\n",
    "        my_sims   = max(opp_sims, 800)  # or fix both; we’ll compare across opp_sims\n",
    "        my_mcts   = MCTS(model, simulations=my_sims, c_puct=1.5, win_len=win_len)\n",
    "        opp_mcts  = MCTS(model, simulations=opp_sims, c_puct=1.5, win_len=win_len)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.current_player == +1:   # “our” side\n",
    "                root = my_mcts.run(env, add_root_noise=False)\n",
    "                a = my_mcts.select_action(root, temperature=0.0)\n",
    "            else:                           # opponent strength\n",
    "                root = opp_mcts.run(env, add_root_noise=False)\n",
    "                a = opp_mcts.select_action(root, temperature=0.0)\n",
    "            _, reward, done = env.step(a)\n",
    "\n",
    "        if reward == 0: draws += 1\n",
    "        else:\n",
    "            winner = -env.current_player\n",
    "            if winner == +1: wins += 1\n",
    "            else: losses += 1\n",
    "    return wins, draws, losses\n",
    "\n",
    "def strength_ladder(model, N=9, win_len=5):\n",
    "    print(\"[Ladder] vs random\")\n",
    "    evaluate_vs_random(model, sims=400, games=50, N=N, win_len=win_len)\n",
    "\n",
    "    print(\"\\n[Ladder] symmetric sims\")\n",
    "    for sims in [100, 200, 400, 800, 1200]:\n",
    "        w,d,l = eval_vs_mcts(model, opp_sims=sims, games=50, N=N, win_len=win_len)  # set my_sims=opp_sims inside if you want symmetric\n",
    "        print(f\"MCTS({sims})  W {w}  D {d}  L {l}\")\n",
    "\n",
    "    print(\"\\n[Ladder] advantage (ours ≥800 sims)\")\n",
    "    for sims in [100, 200, 400, 800, 1200]:\n",
    "        w,d,l = eval_vs_mcts(model, opp_sims=sims, games=50, N=N, win_len=win_len)  # keep your current my_sims=max(opp_sims,800)\n",
    "        print(f\"opp MCTS({sims})  W {w}  D {d}  L {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f08d778f-89e9-4d8c-84f7-c39442958715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ladder] vs random\n",
      "[Eval vs random] W 28  D 0  L 22  (games=50)\n",
      "\n",
      "[Ladder] symmetric sims\n",
      "MCTS(100)  W 50  D 0  L 0\n",
      "MCTS(200)  W 50  D 0  L 0\n",
      "MCTS(400)  W 50  D 0  L 0\n",
      "MCTS(800)  W 50  D 0  L 0\n",
      "MCTS(1200)  W 50  D 0  L 0\n",
      "\n",
      "[Ladder] advantage (ours ≥800 sims)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model.eval()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mstrength_ladder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mstrength_ladder\u001b[39m\u001b[34m(model, N, win_len)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[Ladder] advantage (ours ≥800 sims)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sims \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m100\u001b[39m, \u001b[32m200\u001b[39m, \u001b[32m400\u001b[39m, \u001b[32m800\u001b[39m, \u001b[32m1200\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     w,d,l = \u001b[43meval_vs_mcts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopp_sims\u001b[49m\u001b[43m=\u001b[49m\u001b[43msims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgames\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwin_len\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# keep your current my_sims=max(opp_sims,800)\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mopp MCTS(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msims\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)  W \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  D \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  L \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36meval_vs_mcts\u001b[39m\u001b[34m(model, opp_sims, games, N, win_len)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m env.current_player == +\u001b[32m1\u001b[39m:   \u001b[38;5;66;03m# “our” side\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         root = \u001b[43mmy_mcts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_root_noise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m         a = my_mcts.select_action(root, temperature=\u001b[32m0.0\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:                           \u001b[38;5;66;03m# opponent strength\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mMCTS.run\u001b[39m\u001b[34m(self, env, add_root_noise, dir_alpha, dir_eps, opening_bias, opening_eps)\u001b[39m\n\u001b[32m     35\u001b[39m     mix_opening_bias(root, N, opening_bias, eps=opening_eps)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.simulations):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m root\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mMCTS._simulate\u001b[39m\u001b[34m(self, node, N)\u001b[39m\n\u001b[32m     64\u001b[39m         best_score, best_a = ucb, a\n\u001b[32m     66\u001b[39m child = node.children[best_a]\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m v_child = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m v = -v_child                              \u001b[38;5;66;03m# perspective switch\u001b[39;00m\n\u001b[32m     69\u001b[39m node.visit_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mMCTS._simulate\u001b[39m\u001b[34m(self, node, N)\u001b[39m\n\u001b[32m     64\u001b[39m         best_score, best_a = ucb, a\n\u001b[32m     66\u001b[39m child = node.children[best_a]\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m v_child = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m v = -v_child                              \u001b[38;5;66;03m# perspective switch\u001b[39;00m\n\u001b[32m     69\u001b[39m node.visit_count += \u001b[32m1\u001b[39m\n",
      "    \u001b[31m[... skipping similar frames: MCTS._simulate at line 67 (8 times)]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mMCTS._simulate\u001b[39m\u001b[34m(self, node, N)\u001b[39m\n\u001b[32m     64\u001b[39m         best_score, best_a = ucb, a\n\u001b[32m     66\u001b[39m child = node.children[best_a]\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m v_child = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m v = -v_child                              \u001b[38;5;66;03m# perspective switch\u001b[39;00m\n\u001b[32m     69\u001b[39m node.visit_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mMCTS._simulate\u001b[39m\u001b[34m(self, node, N)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Leaf: not visited yet and no children → expand\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node.visit_count == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node.children:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_expand_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     node.visit_count += \u001b[32m1\u001b[39m\n\u001b[32m     55\u001b[39m     node.total_value += v\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mMCTS._expand_and_evaluate\u001b[39m\u001b[34m(self, node, N)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Create children\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m np.where(\u001b[38;5;28mself\u001b[39m.ttable[key][\u001b[32m0\u001b[39m] > \u001b[32m0\u001b[39m)[\u001b[32m0\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     child_board = apply_action_raw(node.state, \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m, node.player, N)\n\u001b[32m    104\u001b[39m     child = Node(child_board, -node.player)\n\u001b[32m    105\u001b[39m     child.prior = \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m.ttable[key][\u001b[32m0\u001b[39m][a])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "strength_ladder(model, N=9, win_len=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fa02a-ec59-40e4-8fcc-ee8b52508612",
   "metadata": {},
   "source": [
    "# 6. Interative Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff38c77-d0f3-4738-9ad0-68cc54035844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipywidgets\n",
    "import ipywidgets as W\n",
    "from IPython.display import display\n",
    "\n",
    "class GomokuUI:\n",
    "    def __init__(self, model, N=9, win_len=5, sims=800, human_player=-1):\n",
    "        self.model = model\n",
    "        self.N = N\n",
    "        self.win_len = win_len\n",
    "        self.human_player = int(human_player)   # +1 black (X), -1 white (O)\n",
    "        self.sims = sims\n",
    "\n",
    "        self.env = Gomoku(N=N, win_len=win_len)\n",
    "        self.buttons = [[W.Button(description=\" \", layout=W.Layout(width=\"40px\", height=\"40px\")) \n",
    "                         for _ in range(N)] for _ in range(N)]\n",
    "        for r in range(N):\n",
    "            for c in range(N):\n",
    "                self.buttons[r][c].on_click(self._make_cb(r, c))\n",
    "\n",
    "        self.msg = W.HTML(value=\"\")\n",
    "        self.reset_btn = W.Button(description=\"Reset\")\n",
    "        self.reset_btn.on_click(self.reset)\n",
    "\n",
    "        grid = W.GridBox(\n",
    "            children=[self.buttons[r][c] for r in range(N) for c in range(N)],\n",
    "            layout=W.Layout(grid_template_columns=f\"repeat({N}, 42px)\")\n",
    "        )\n",
    "        display(W.VBox([grid, W.HBox([self.reset_btn]), self.msg]))\n",
    "\n",
    "        # AI may start\n",
    "        if self.human_player == -1 and self.env.current_player == +1:\n",
    "            self.ai_move()\n",
    "        self.refresh()\n",
    "\n",
    "    def _make_cb(self, r, c):\n",
    "        def _cb(btn):\n",
    "            if self.env.current_player != self.human_player:\n",
    "                return\n",
    "            a = r * self.N + c\n",
    "            if a not in self.env.get_legal_actions():\n",
    "                return\n",
    "            _, _, done = self.env.step(a)\n",
    "            if not done:\n",
    "                self.ai_move()\n",
    "            self.refresh()\n",
    "        return _cb\n",
    "\n",
    "    def ai_move(self):\n",
    "        mcts = MCTS(self.model, simulations=self.sims, c_puct=1.5, win_len=self.win_len)\n",
    "        root = mcts.run(self.env, add_root_noise=False)  # no noise for a strong play\n",
    "        a = mcts.select_action(root, temperature=0.0)\n",
    "        self.env.step(a)\n",
    "\n",
    "    def refresh(self):\n",
    "        sym = {1:\"X\", -1:\"O\", 0:\" \"}\n",
    "        for r in range(self.N):\n",
    "            for c in range(self.N):\n",
    "                v = int(self.env.board[r, c])\n",
    "                b = self.buttons[r][c]\n",
    "                b.description = sym[v]\n",
    "                b.disabled = bool(v != 0)\n",
    "\n",
    "        reward, done = self.env.check_winner_fast()\n",
    "        if done:\n",
    "            if reward == 0:\n",
    "                self.msg.value = \"<b>Draw.</b> Click Reset.\"\n",
    "            else:\n",
    "                winner = 'X' if -self.env.current_player == +1 else 'O'\n",
    "                self.msg.value = f\"<b>{winner} wins!</b> Click Reset.\"\n",
    "            for r in range(self.N):\n",
    "                for c in range(self.N):\n",
    "                    self.buttons[r][c].disabled = True\n",
    "        else:\n",
    "            turn = 'X' if self.env.current_player==+1 else 'O'\n",
    "            self.msg.value = f\"Player {turn} to move.\"\n",
    "\n",
    "    def reset(self, _=None):\n",
    "        self.env.reset()\n",
    "        if self.human_player == -1 and self.env.current_player == +1:\n",
    "            self.ai_move()\n",
    "        self.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055083b-7742-42c6-affc-441cf34cbaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "model.eval()\n",
    "ui = GomokuUI(model, N=9, win_len=5, sims=600, human_player=-1)  # human plays O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf874784-901b-471c-924a-97636861c0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
