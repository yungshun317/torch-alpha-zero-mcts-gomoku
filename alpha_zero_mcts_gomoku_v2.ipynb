{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9286a65-7939-4ae2-83dc-dfe41e5ecb33",
   "metadata": {},
   "source": [
    "# 1. Gomoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "207a7abc-73dc-445d-a062-c579be416971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Gomoku:\n",
    "    \"\"\"\n",
    "    N×N board, players in {+1 (black), -1 (white)}, empty=0.\n",
    "    Win condition: 5 in a row (any direction).\n",
    "    \"\"\"\n",
    "    def __init__(self, N=9, win_len=5):\n",
    "        self.N = int(N)\n",
    "        self.win_len = int(win_len)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.N, self.N), dtype=np.int8)\n",
    "        self.current_player = 1\n",
    "        self.done = False\n",
    "        self.last_move = None  # (r,c) or None\n",
    "        return self.get_state()  # canonical 1×(N*N) if you want; see below\n",
    "\n",
    "    # --- Canonical features for NN (3 planes): me, opp, to-move ---\n",
    "    def to_planes(self):\n",
    "        me  = (self.board == self.current_player).astype(np.float32)\n",
    "        opp = (self.board == -self.current_player).astype(np.float32)\n",
    "        turn = np.full_like(me, 1.0, dtype=np.float32)  # “to move” plane\n",
    "        # shape: [3, N, N]\n",
    "        return np.stack([me, opp, turn], axis=0)\n",
    "\n",
    "    # For drop-in compatibility with your older code:\n",
    "    def get_state(self):\n",
    "        # vectorized canonical state (board * current_player) like tic-tac-toe\n",
    "        return (self.board.flatten() * self.current_player).astype(np.float32)\n",
    "\n",
    "    def get_legal_actions(self):\n",
    "        # actions are 0..N*N-1\n",
    "        flat = self.board.reshape(-1)\n",
    "        return [i for i in range(self.N * self.N) if flat[i] == 0]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply action index (0..N^2-1). Returns (state, reward, done).\"\"\"\n",
    "        if self.done:\n",
    "            return self.get_state(), 0.0, True\n",
    "        r, c = divmod(int(action), self.N)\n",
    "        if self.board[r, c] != 0:\n",
    "            # illegal move penalty (AlphaZero typically forbids via masking;\n",
    "            # but keep this as a safeguard)\n",
    "            self.done = True\n",
    "            return self.get_state(), -1.0, True\n",
    "\n",
    "        self.board[r, c] = self.current_player\n",
    "        self.last_move = (r, c)\n",
    "\n",
    "        reward, done = self.check_winner_fast()\n",
    "        self.done = done\n",
    "        state = self.get_state()\n",
    "        self.current_player *= -1\n",
    "        return state, float(reward), bool(done)\n",
    "\n",
    "    # --------- winner checks ----------\n",
    "    def check_winner_fast(self):\n",
    "        \"\"\"\n",
    "        Check only the lines passing through self.last_move for speed.\n",
    "        Returns (reward_for_player_who_just_moved, done)\n",
    "        reward ∈ {+1 win, 0 draw/ongoing, -1 loss} relative to the mover.\n",
    "        \"\"\"\n",
    "        if self.last_move is None:\n",
    "            return 0.0, False\n",
    "        r, c = self.last_move\n",
    "        p = self.board[r, c]\n",
    "        if p == 0:\n",
    "            return 0.0, False\n",
    "\n",
    "        if ( self._count_dir(r, c, 1, 0, p) + self._count_dir(r, c, -1, 0, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 0, 1, p) + self._count_dir(r, c, 0, -1, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 1, 1, p) + self._count_dir(r, c, -1, -1, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 1, -1, p) + self._count_dir(r, c, -1, 1, p) - 1 >= self.win_len ):\n",
    "            # player p (who just moved) wins: reward +1 for mover\n",
    "            return 1.0, True\n",
    "\n",
    "        if (self.board != 0).all():\n",
    "            return 0.0, True  # draw\n",
    "\n",
    "        return 0.0, False\n",
    "\n",
    "    def _count_dir(self, r, c, dr, dc, p):\n",
    "        \"\"\"Count contiguous stones with color p starting at (r,c) inclusive along (dr,dc).\"\"\"\n",
    "        N = self.N\n",
    "        cnt = 0\n",
    "        rr, cc = r, c\n",
    "        while 0 <= rr < N and 0 <= cc < N and self.board[rr, cc] == p:\n",
    "            cnt += 1\n",
    "            rr += dr\n",
    "            cc += dc\n",
    "        return cnt\n",
    "\n",
    "    def num_moves(self):\n",
    "        \"\"\"Return the number of moves played so far.\"\"\"\n",
    "        # If you track move history: return len(self.move_history)\n",
    "        # If not, just count non-zero cells:\n",
    "        return int((self.board != 0).sum())\n",
    "    \n",
    "    # --------- convenience ----------\n",
    "    def render_ascii(self):\n",
    "        sym = {1:'X', -1:'O', 0:'.'}\n",
    "        print(\"\\n\".join(\" \".join(sym[v] for v in row) for row in self.board))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7745f-094c-42a4-ac6a-404e421c0b71",
   "metadata": {},
   "source": [
    "# 2. ResNet Policy-Value Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d937944f-353a-4802-a13e-2fe97c41c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(x + y)\n",
    "\n",
    "class PVResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy–Value net for Gomoku.\n",
    "    Input: [B, 3, N, N]\n",
    "    Outputs:\n",
    "      - policy_logits: [B, N*N]\n",
    "      - value:         [B, 1]  (tanh)\n",
    "    \"\"\"\n",
    "    def __init__(self, board_size=9, channels=64, n_blocks=6):\n",
    "        super().__init__()\n",
    "        self.N = board_size\n",
    "        C = channels\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, C, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(C),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.trunk = nn.Sequential(*[ResidualBlock(C) for _ in range(n_blocks)])\n",
    "\n",
    "        # Policy head\n",
    "        self.p_head = nn.Sequential(\n",
    "            nn.Conv2d(C, 2, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.p_fc = nn.Linear(2 * self.N * self.N, self.N * self.N)\n",
    "\n",
    "        # Value head\n",
    "        self.v_head = nn.Sequential(\n",
    "            nn.Conv2d(C, 1, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.v_fc1 = nn.Linear(1 * self.N * self.N, C)\n",
    "        self.v_fc2 = nn.Linear(C, 1)\n",
    "\n",
    "        # (optional) init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):  # x: [B, 3, N, N]\n",
    "        z = self.stem(x)\n",
    "        z = self.trunk(z)\n",
    "\n",
    "        # policy\n",
    "        p = self.p_head(z)\n",
    "        p = p.view(p.size(0), -1)\n",
    "        policy_logits = self.p_fc(p)  # [B, N*N]\n",
    "\n",
    "        # value\n",
    "        v = self.v_head(z)\n",
    "        v = v.view(v.size(0), -1)\n",
    "        v = F.relu(self.v_fc1(v))\n",
    "        value = torch.tanh(self.v_fc2(v))  # [-1, 1]\n",
    "\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074746e0-9ece-44e3-bc79-40358fdb2873",
   "metadata": {},
   "source": [
    "# 3. Monte Carlo Tree Search (MCTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71e6c3cd-3923-4635-9351-cee810961de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gomoku_planes_from_raw(raw_board_tuple, player, N):\n",
    "    \"\"\"Build [3, N, N] planes from (raw_board, player_to_move).\"\"\"\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N)\n",
    "    me  = (b == player).astype(np.float32)\n",
    "    opp = (b == -player).astype(np.float32)\n",
    "    turn = np.ones_like(me, dtype=np.float32)\n",
    "    return np.stack([me, opp, turn], axis=0)\n",
    "\n",
    "def legal_actions_from_raw(raw_board_tuple, N):\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N)\n",
    "    return [i for i in range(N*N) if b.flat[i] == 0]\n",
    "\n",
    "def apply_action_raw(raw_board_tuple, action, player, N):\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N).copy()\n",
    "    r, c = divmod(int(action), N)\n",
    "    b[r, c] = player\n",
    "    return tuple(b.reshape(-1))\n",
    "\n",
    "def check_terminal_gomoku(raw_board_tuple, N, win_len=5):\n",
    "    \"\"\"Return (value_for_current_node_player, done). Full scan (O(N^2)), fine for 9x9.\"\"\"\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N)\n",
    "\n",
    "    def has_k(p):\n",
    "        # rows\n",
    "        for r in range(N):\n",
    "            run = 0\n",
    "            for c in range(N):\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "        # cols\n",
    "        for c in range(N):\n",
    "            run = 0\n",
    "            for r in range(N):\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "        # diag \\\n",
    "        for r0 in range(N):\n",
    "            run = 0\n",
    "            r, c = r0, 0\n",
    "            while r < N and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r += 1; c += 1\n",
    "        for c0 in range(1, N):\n",
    "            run = 0\n",
    "            r, c = 0, c0\n",
    "            while r < N and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r += 1; c += 1\n",
    "        # diag /\n",
    "        for r0 in range(N):\n",
    "            run = 0\n",
    "            r, c = r0, 0\n",
    "            while r >= 0 and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r -= 1; c += 1\n",
    "        for c0 in range(1, N):\n",
    "            run = 0\n",
    "            r, c = N-1, c0\n",
    "            while r >= 0 and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r -= 1; c += 1\n",
    "        return False\n",
    "\n",
    "    if has_k(+1): return (+1.0, True)\n",
    "    if has_k(-1): return (-1.0, True)\n",
    "    if (b != 0).all(): return (0.0, True)\n",
    "    return (0.0, False)\n",
    "\n",
    "# Opening bias\n",
    "def center_bias(N, sigma=0.35):\n",
    "    \"\"\"Return normalized N*N center-biased prior (Gaussian on [0,1]^2 grid).\"\"\"\n",
    "    xs = np.linspace(0, 1, N)\n",
    "    X, Y = np.meshgrid(xs, xs, indexing='ij')\n",
    "    cx = cy = 0.5\n",
    "    g = np.exp(-((X-cx)**2 + (Y-cy)**2) / (2*(sigma**2)))\n",
    "    v = g.reshape(-1).astype(np.float32)\n",
    "    v /= v.sum() + 1e-8\n",
    "    return v\n",
    "\n",
    "def mix_opening_bias(node, N, bias_vec, eps=0.25):\n",
    "    \"\"\"Blend bias_vec into child priors at the root (legal only).\"\"\"\n",
    "    if eps is None or eps <= 0.0:\n",
    "        return\n",
    "    legal = list(node.children.keys())\n",
    "    if not legal: return\n",
    "    b = bias_vec.copy()\n",
    "    # renorm on legal only\n",
    "    s = b[legal].sum()\n",
    "    b = b / (s + 1e-8)\n",
    "    for a in legal:\n",
    "        p = node.children[a].prior\n",
    "        node.children[a].prior = (1 - eps) * p + eps * float(b[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efa8eaeb-17ca-4b4e-8727-7a773b80035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Node (add priors + legal cache, but no prebuilt children) ----\n",
    "class Node:\n",
    "    def __init__(self, raw_board_tuple, player_to_move):\n",
    "        self.state = raw_board_tuple          # tuple of length N*N (0/±1)\n",
    "        self.player = int(player_to_move)     # +1 or -1\n",
    "        self.children = {}                    # action -> Node (created lazily)\n",
    "        self.visit_count = 0\n",
    "        self.total_value = 0.0\n",
    "        self.prior = 0.0                      # used by parent\n",
    "        self.priors = None                    # np.float32[N*N], filled at expand\n",
    "        self.legal = None                     # list[int], filled at expand\n",
    "\n",
    "    def value(self):\n",
    "        return 0.0 if self.visit_count == 0 else self.total_value / self.visit_count\n",
    "\n",
    "# ---- MCTS ----\n",
    "class MCTS:\n",
    "    def __init__(self, model, simulations=800, c_puct=2.0, win_len=5, device=None):\n",
    "        self.model = model\n",
    "        self.simulations = simulations\n",
    "        self.c_puct = c_puct\n",
    "        self.win_len = win_len\n",
    "        self.ttable = {}     # (state_tuple, player, N) -> (priors[N*N], value_float)\n",
    "        self.root = None\n",
    "        self.device = device or next(model.parameters()).device\n",
    "\n",
    "    # ---------- public API ----------\n",
    "    def run(self, env, add_root_noise=True, dir_alpha=0.3, dir_eps=0.25,\n",
    "            opening_bias=None, opening_eps=0.25):\n",
    "        N = env.N\n",
    "        self.sync_to_env(env)  # ensures self.root exists and expanded\n",
    "\n",
    "        if add_root_noise:\n",
    "            self._add_dirichlet_noise(self.root, N, alpha=dir_alpha, eps=dir_eps)\n",
    "        if opening_bias is not None and opening_eps and opening_eps > 0.0:\n",
    "            mix_opening_bias(self.root, N, opening_bias, eps=opening_eps)\n",
    "\n",
    "        for _ in range(self.simulations):\n",
    "            self._simulate(self.root, N)\n",
    "        return self.root\n",
    "\n",
    "    def advance(self, action, N):\n",
    "        if self.root and action in self.root.children:\n",
    "            self.root = self.root.children[action]\n",
    "        else:\n",
    "            # rebuild minimal root if we don't have that child\n",
    "            child_board = apply_action_raw(self.root.state if self.root else tuple([0]*(N*N)),\n",
    "                                           int(action),\n",
    "                                           self.root.player if self.root else +1,\n",
    "                                           N)\n",
    "            self.root = Node(child_board, -(self.root.player if self.root else +1))\n",
    "            if self.root.visit_count == 0:\n",
    "                _ = self._expand_and_evaluate(self.root, N)\n",
    "\n",
    "    def sync_to_env(self, env):\n",
    "        N = env.N\n",
    "        state = tuple(env.board.reshape(-1))\n",
    "        player = env.current_player\n",
    "        if self.root and self.root.state == state and self.root.player == player:\n",
    "            return\n",
    "        if self.root:\n",
    "            for ch in self.root.children.values():\n",
    "                if ch.state == state and ch.player == player:\n",
    "                    self.root = ch\n",
    "                    if self.root.visit_count == 0:\n",
    "                        _ = self._expand_and_evaluate(self.root, N)\n",
    "                    return\n",
    "        self.root = Node(state, player)\n",
    "        _ = self._expand_and_evaluate(self.root, N)\n",
    "\n",
    "    # ---------- core search (lazy children) ----------\n",
    "    def _simulate(self, node, N):\n",
    "        # Terminal?\n",
    "        winner, done = check_terminal_gomoku(node.state, N, self.win_len)\n",
    "        if done:\n",
    "            v = 0.0 if winner == 0 else (+1.0 if winner == node.player else -1.0)\n",
    "            node.visit_count += 1\n",
    "            node.total_value += v\n",
    "            return v\n",
    "\n",
    "        # Immediate tactical win for node.player?\n",
    "        can_win, win_a = self.has_immediate_win(node.state, node.player, N, self.win_len)\n",
    "        if can_win:\n",
    "            v = +1.0\n",
    "            node.visit_count += 1\n",
    "            node.total_value += v\n",
    "            # ensure selectable child\n",
    "            if win_a not in node.children:\n",
    "                nb = apply_action_raw(node.state, win_a, node.player, N)\n",
    "                ch = Node(nb, -node.player); ch.prior = 1.0\n",
    "                node.children[win_a] = ch\n",
    "            return v\n",
    "\n",
    "        # First visit leaf -> expand (fills node.priors/node.legal but no children yet)\n",
    "        if node.visit_count == 0 and not node.children:\n",
    "            v = self._expand_and_evaluate(node, N)\n",
    "            node.visit_count += 1\n",
    "            node.total_value += v\n",
    "            return v\n",
    "\n",
    "        # Select via PUCT over legal moves; create child only for chosen action\n",
    "        if node.priors is None or node.legal is None:\n",
    "            # safety: if something went wrong, expand now\n",
    "            _ = self._expand_and_evaluate(node, N)\n",
    "\n",
    "        sqrt_N = np.sqrt(node.visit_count + 1e-8)\n",
    "        best_a, best_score = None, -1e9\n",
    "        for a in node.legal:\n",
    "            ch = node.children.get(a, None)\n",
    "            q = (0.0 if ch is None or ch.visit_count == 0 else ch.total_value / ch.visit_count)\n",
    "            n = (0 if ch is None else ch.visit_count)\n",
    "            ucb = q + self.c_puct * float(node.priors[a]) * (sqrt_N / (1 + n))\n",
    "            if ucb > best_score:\n",
    "                best_score, best_a = ucb, a\n",
    "\n",
    "        # Lazily create the chosen child if needed\n",
    "        child = node.children.get(best_a, None)\n",
    "        if child is None:\n",
    "            nb = apply_action_raw(node.state, best_a, node.player, N)\n",
    "            child = Node(nb, -node.player)\n",
    "            child.prior = float(node.priors[best_a])\n",
    "            node.children[best_a] = child\n",
    "\n",
    "        v_child = self._simulate(child, N)\n",
    "        v = -v_child\n",
    "        node.visit_count += 1\n",
    "        node.total_value += v\n",
    "        return v\n",
    "\n",
    "    def _expand_and_evaluate(self, node, N):\n",
    "        key = (node.state, node.player, N)\n",
    "        if key in self.ttable:\n",
    "            priors, v = self.ttable[key]\n",
    "            node.priors = priors  # full length N*N\n",
    "            node.legal  = legal_actions_from_raw(node.state, N)\n",
    "            return v\n",
    "\n",
    "        # canonical planes on device if provided\n",
    "        planes = gomoku_planes_from_raw(node.state, node.player, N).astype(np.float32)\n",
    "        st = torch.from_numpy(planes).unsqueeze(0)\n",
    "        if self.device is not None:\n",
    "            st = st.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits, value = self.model(st)\n",
    "            v = float(value.item())\n",
    "\n",
    "        logits_np = logits.squeeze(0).detach().cpu().numpy()\n",
    "        legal = legal_actions_from_raw(node.state, N)\n",
    "        priors = np.zeros(N*N, dtype=np.float32)\n",
    "        if legal:\n",
    "            ll = logits_np[legal]\n",
    "            ll -= ll.max()\n",
    "            p_legal = np.exp(ll)\n",
    "            p_legal /= (p_legal.sum() + 1e-8)\n",
    "            priors[legal] = p_legal\n",
    "\n",
    "        # prune suicidal legals (if possible) then renorm on survivors\n",
    "        suicidal = []\n",
    "        for a in legal:\n",
    "            if self.opponent_has_immediate_win_after(node.state, a, node.player, N, self.win_len):\n",
    "                suicidal.append(a)\n",
    "        if 0 < len(suicidal) < len(legal):\n",
    "            for a in suicidal: priors[a] = 0.0\n",
    "            s = priors[legal].sum()\n",
    "            if s > 0: priors[legal] = priors[legal] / s\n",
    "            else:\n",
    "                survivors = [a for a in legal if a not in suicidal]\n",
    "                for a in survivors: priors[a] = 1.0 / len(survivors)\n",
    "\n",
    "        node.priors = priors\n",
    "        node.legal  = legal\n",
    "        self.ttable[key] = (priors.astype(np.float32), v)\n",
    "        return v\n",
    "\n",
    "    # ---------- root noise ----------\n",
    "    def _add_dirichlet_noise(self, node, N, alpha=0.3, eps=0.25):\n",
    "        if node.priors is None or node.legal is None:\n",
    "            return\n",
    "        if not node.legal:\n",
    "            return\n",
    "        noise = np.random.dirichlet([alpha] * len(node.legal)).astype(np.float32)\n",
    "        for a, n in zip(node.legal, noise):\n",
    "            node.priors[a] = (1 - eps) * node.priors[a] + eps * float(n)\n",
    "\n",
    "    # ---------- action picker ----------\n",
    "    def select_action(self, root, temperature=0.0, tactical_guard=False):\n",
    "        N2 = int(np.sqrt(len(root.state)))\n",
    "        visits = np.zeros(N2*N2, dtype=np.float64)\n",
    "        legal_idxs = np.array(root.legal if root.legal is not None else legal_actions_from_raw(root.state, N2), dtype=int)\n",
    "\n",
    "        for a in legal_idxs:\n",
    "            ch = root.children.get(a, None)\n",
    "            visits[a] = 0 if ch is None else ch.visit_count\n",
    "\n",
    "        if tactical_guard:\n",
    "            survivors = [a for a in legal_idxs\n",
    "                         if not self.opponent_has_immediate_win_after(root.state, int(a),\n",
    "                                                                      root.player, N2, self.win_len)]\n",
    "            if len(survivors) > 0:\n",
    "                legal_idxs = np.array(survivors, dtype=int)\n",
    "\n",
    "        if temperature == 0.0:\n",
    "            vmax = visits[legal_idxs].max()\n",
    "            ties = legal_idxs[visits[legal_idxs] == vmax]\n",
    "            return int(np.random.choice(ties))\n",
    "\n",
    "        x = visits[legal_idxs] ** (1.0 / max(1e-8, temperature))\n",
    "        p = x / (x.sum() + 1e-8)\n",
    "        return int(np.random.choice(legal_idxs, p=p))\n",
    "\n",
    "    # ----- tiny tactics helpers (as staticmethods for convenience) -----\n",
    "    @staticmethod\n",
    "    def has_immediate_win(raw_board_tuple, player, N, win_len):\n",
    "        legal = legal_actions_from_raw(raw_board_tuple, N)\n",
    "        if not legal: return False, None\n",
    "        for a in legal:\n",
    "            nb = apply_action_raw(raw_board_tuple, a, player, N)\n",
    "            winner, done = check_terminal_gomoku(nb, N, win_len)\n",
    "            if done and winner == player:\n",
    "                return True, a\n",
    "        return False, None\n",
    "\n",
    "    @staticmethod\n",
    "    def opponent_has_immediate_win_after(raw_board_tuple, my_action, my_player, N, win_len):\n",
    "        child = apply_action_raw(raw_board_tuple, my_action, my_player, N)\n",
    "        opp = -my_player\n",
    "        legal_opp = legal_actions_from_raw(child, N)\n",
    "        for oa in legal_opp:\n",
    "            nb = apply_action_raw(child, oa, opp, N)\n",
    "            winner, done = check_terminal_gomoku(nb, N, win_len)\n",
    "            if done and winner == opp:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c540161-6273-4238-8de1-7bbb0633847d",
   "metadata": {},
   "source": [
    "# 4. Tactical Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a8b0a0e-d470-4034-866b-c2157607d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np, torch\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(0); np.random.seed(0); torch.manual_seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "# Build env/model first\n",
    "env    = Gomoku(N=9, win_len=5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model  = PVResNet(board_size=env.N, channels=64, n_blocks=6).to(device).eval()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Now create MCTS (on the SAME device)\n",
    "mcts   = MCTS(model, simulations=800, c_puct=2.0, win_len=env.win_len, device=device)\n",
    "\n",
    "# One move example (opening uses noise + T>0)\n",
    "root = mcts.run(env, add_root_noise=True)\n",
    "a = mcts.select_action(root, temperature=1.0)\n",
    "state, reward, done = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd860e28-d1cc-4f50-86ce-7a70f3835b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picked: 40  expected block: 40\n"
     ]
    }
   ],
   "source": [
    "def must_block_test(model, N=9, win_len=5, sims=800, c_puct=2.0, device=None):\n",
    "    b = np.zeros((N, N), dtype=np.int8)\n",
    "    r = 4\n",
    "    for c in [0,1,2,3]:   # single-ended four at edge\n",
    "        b[r, c] = +1\n",
    "\n",
    "    env = Gomoku(N=N, win_len=win_len)\n",
    "    env.board[:] = b\n",
    "    env.current_player = -1\n",
    "\n",
    "    expected_block = r * N + 4\n",
    "\n",
    "    mcts = MCTS(model, simulations=sims, c_puct=c_puct, win_len=win_len, device=device)\n",
    "    root = mcts.run(env, add_root_noise=False)\n",
    "    picked = mcts.select_action(root, temperature=0.0)\n",
    "    return picked, expected_block\n",
    "\n",
    "def must_win_test(model, N=9, win_len=5, sims=200, c_puct=2.0, device=None):\n",
    "    b = np.zeros((N, N), dtype=np.int8)\n",
    "    r = 4\n",
    "    for c in [0,1,2,3]:\n",
    "        b[r, c] = +1\n",
    "\n",
    "    env = Gomoku(N=N, win_len=win_len)\n",
    "    env.board[:] = b\n",
    "    env.current_player = +1\n",
    "\n",
    "    expected_win = r * N + 4\n",
    "    mcts = MCTS(model, simulations=sims, c_puct=c_puct, win_len=win_len, device=device)\n",
    "    root = mcts.run(env, add_root_noise=False)\n",
    "    picked = mcts.select_action(root, temperature=0.0)\n",
    "    return picked, expected_win\n",
    "\n",
    "def avoid_suicide_test(model, N=9, win_len=5, sims=400, c_puct=2.0, device=None):\n",
    "    # +1 has xxxx at row r=4, cols 0..3; only (4,4) completes five\n",
    "    b = np.zeros((N, N), dtype=np.int8)\n",
    "    r = 4\n",
    "    for c in [0,1,2,3]:\n",
    "        b[r, c] = +1\n",
    "\n",
    "    env = Gomoku(N=N, win_len=win_len)\n",
    "    env.board[:] = b\n",
    "    env.current_player = -1\n",
    "\n",
    "    block = r*N + 4\n",
    "    assert block in env.get_legal_actions()\n",
    "\n",
    "    mcts = MCTS(model, simulations=sims, c_puct=c_puct, win_len=win_len, device=device)\n",
    "    root = mcts.run(env, add_root_noise=False)\n",
    "    picked = mcts.select_action(root, temperature=0.0, tactical_guard=True)\n",
    "\n",
    "    raw = tuple(env.board.reshape(-1))\n",
    "    is_suicide = MCTS.opponent_has_immediate_win_after(raw, picked, -1, N, win_len)\n",
    "    is_block   = (picked == block)\n",
    "\n",
    "    return {\"picked\": picked, \"block\": block, \"is_block\": is_block,\n",
    "            \"is_suicide\": is_suicide, \"pass_test\": (is_block and not is_suicide)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a74c14-c261-4f3d-89b6-64efd979ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "picked, exp_block = must_block_test(model, sims=100, device=device)\n",
    "print(\"picked:\", picked, \" expected:\", exp_block)\n",
    "\n",
    "picked, exp_win = must_win_test(model, sims=100, device=device)\n",
    "print(\"picked:\", picked, \" expected:\", exp_win)\n",
    "\n",
    "print(avoid_suicide_test(model, sims=100, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da4567-9942-4349-8600-0f7306eab903",
   "metadata": {},
   "source": [
    "# 5. Self-Play with Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a09fef56-6773-40c9-806e-2cbe04fb2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, math, collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# --- helpers to rotate/reflect both planes and a flat policy over N*N cells ---\n",
    "def rotate90_board(planes):    # planes: [C,N,N]\n",
    "    return np.rot90(planes, k=1, axes=(1,2)).copy()\n",
    "\n",
    "def rotate180_board(planes):\n",
    "    return np.rot90(planes, k=2, axes=(1,2)).copy()\n",
    "\n",
    "def rotate270_board(planes):\n",
    "    return np.rot90(planes, k=3, axes=(1,2)).copy()\n",
    "\n",
    "def flip_h_board(planes):\n",
    "    return np.flip(planes, axis=2).copy()   # horizontal flip (left-right)\n",
    "\n",
    "def idx_map_rotate90(i, N):\n",
    "    r, c = divmod(i, N)\n",
    "    rr, cc = c, N-1-r\n",
    "    return rr*N + cc\n",
    "\n",
    "def idx_map_rotate180(i, N):\n",
    "    r, c = divmod(i, N)\n",
    "    rr, cc = N-1-r, N-1-c\n",
    "    return rr*N + cc\n",
    "\n",
    "def idx_map_rotate270(i, N):\n",
    "    r, c = divmod(i, N)\n",
    "    rr, cc = N-1-c, r\n",
    "    return rr*N + cc\n",
    "\n",
    "def idx_map_flip_h(i, N):\n",
    "    r, c = divmod(i, N)\n",
    "    rr, cc = r, N-1-c\n",
    "    return rr*N + cc\n",
    "\n",
    "def transform_pi(pi, idx_map_fn, N):\n",
    "    out = np.zeros_like(pi)\n",
    "    for i, p in enumerate(pi):\n",
    "        out[idx_map_fn(i, N)] = p\n",
    "    return out\n",
    "\n",
    "def compose_maps(f, g):\n",
    "    # (f ∘ g)(i, N) = f(g(i, N), N)\n",
    "    return lambda i, N: f(g(i, N), N)\n",
    "\n",
    "def symmetries_board_and_pi(planes, pi, N):\n",
    "    out = []\n",
    "\n",
    "    # identity\n",
    "    out.append((planes.copy(), pi.copy()))\n",
    "\n",
    "    # rotations\n",
    "    p1 = rotate90_board(planes)\n",
    "    out.append((p1,  transform_pi(pi, idx_map_rotate90,  N)))\n",
    "\n",
    "    p2 = rotate180_board(planes)\n",
    "    out.append((p2,  transform_pi(pi, idx_map_rotate180, N)))\n",
    "\n",
    "    p3 = rotate270_board(planes)\n",
    "    out.append((p3,  transform_pi(pi, idx_map_rotate270, N)))\n",
    "\n",
    "    # flip H + rotations\n",
    "    pf = flip_h_board(planes)\n",
    "    out.append((pf,  transform_pi(pi, idx_map_flip_h, N)))\n",
    "\n",
    "    pf1 = rotate90_board(pf)\n",
    "    out.append((pf1, transform_pi(pi, compose_maps(idx_map_rotate90,  idx_map_flip_h), N)))\n",
    "\n",
    "    pf2 = rotate180_board(pf)\n",
    "    out.append((pf2, transform_pi(pi, compose_maps(idx_map_rotate180, idx_map_flip_h), N)))\n",
    "\n",
    "    pf3 = rotate270_board(pf)\n",
    "    out.append((pf3, transform_pi(pi, compose_maps(idx_map_rotate270, idx_map_flip_h), N)))\n",
    "\n",
    "    return out\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buf = collections.deque(maxlen=capacity)\n",
    "    def add(self, x): self.buf.append(x)\n",
    "    def sample(self, bs): return random.sample(self.buf, bs)\n",
    "    def __len__(self): return len(self.buf)\n",
    "\n",
    "\"\"\"\n",
    "# fixed ceiling for clarity\n",
    "MAX_SIMS = 800\n",
    "\n",
    "def sims_schedule(iter_idx, move_no, base=200, max_sims=MAX_SIMS, warmup_iters=20, ramp_moves=12):\n",
    "    # ramp with training iteration (more search later)\n",
    "    # ramp with move number (opening cheaper, mid/late richer)\n",
    "    \n",
    "    # ramp by iteration\n",
    "    f_iter = min(1.0, iter_idx / float(warmup_iters))\n",
    "    sims_iter = int(base + (max_sims - base) * f_iter)\n",
    "    # ramp by move number\n",
    "    f_move = min(1.0, move_no / float(ramp_moves))\n",
    "    sims = int(base + (sims_iter - base) * f_move)\n",
    "    return max(base, min(max_sims, sims))\n",
    "\n",
    "def opening_eps_schedule(iter_idx, move_no, max_eps=0.40, stop_iter=30, stop_move=10):\n",
    "    f_iter = max(0.0, 1.0 - iter_idx/float(stop_iter))\n",
    "    f_move = max(0.0, 1.0 - move_no/float(stop_move))\n",
    "    return max_eps * f_iter * f_move\n",
    "\n",
    "def self_play_episode(env, model, mcts, temp_moves=8, iter_idx=1):\n",
    "    env.reset()\n",
    "    # list of (planes, pi, player)\n",
    "    traj = []\n",
    "    done = False\n",
    "    move_no = 0\n",
    "    while not done:\n",
    "        # adjust sims on the fly\n",
    "        # mcts.simulations = sims_schedule(iter_idx, move_no, base=200, max_sims=mcts.simulations)\n",
    "        mcts.simulations = sims_schedule(iter_idx, move_no, base=200, max_sims=MAX_SIMS)\n",
    "        eps_open = opening_eps_schedule(iter_idx, move_no)\n",
    "        root = mcts.run(\n",
    "            env, \n",
    "            add_root_noise=True, \n",
    "            dir_alpha=0.15, \n",
    "            dir_eps=0.25,\n",
    "            opening_bias=OPENING_BIAS,\n",
    "            opening_eps=eps_open\n",
    "        )\n",
    "        T = 1.0 if move_no < temp_moves else 0.0\n",
    "        a = mcts.select_action(root, temperature=T)\n",
    "\n",
    "        # build visit-count policy target π\n",
    "        pi = np.zeros(env.N * env.N, dtype=np.float32)\n",
    "        for aa, child in root.children.items():\n",
    "            pi[aa] = child.visit_count\n",
    "        pi /= (pi.sum() + 1e-8)\n",
    "\n",
    "        planes = env.to_planes()\n",
    "        traj.append((planes, pi, env.current_player))\n",
    "\n",
    "        _, reward, done = env.step(a)\n",
    "        move_no += 1\n",
    "\n",
    "    # assign final outcome z to each step from that step's player perspective\n",
    "    data = []\n",
    "    for planes, pi, player in traj:\n",
    "        # +1 win for player's perspective, 0 draw, -1 loss\n",
    "        z = reward * player  \n",
    "        # 8 sym augmentations\n",
    "        for sp in symmetries(planes):\n",
    "            data.append((sp.astype(np.float32), pi.copy(), float(z)))\n",
    "        # list of (planes[3,N,N], pi[N*N], z)\n",
    "    return data\n",
    "\"\"\"\n",
    "\n",
    "# ----- curriculum knobs -----\n",
    "CURRICULUM = dict(\n",
    "    max_sims=1600,          # stronger ceiling later if you like (e.g., 1600)\n",
    "    base_sims=200,         # very cheap opening search\n",
    "    sims_warmup_iters=40,  # how many iterations to reach near max_sims\n",
    "    sims_ramp_moves=15,    # ramp sims across the opening moves\n",
    "    opening_max_eps=0.40,  # strength of center prior mix at start\n",
    "    opening_stop_iter=30,  # how fast to fade opening bias across iters\n",
    "    opening_stop_move=10,  # ... and across moves\n",
    "    temp_opening_moves=10   # use T=1 for first K moves, then T=0\n",
    ")\n",
    "\n",
    "def sims_schedule(iter_idx, move_no, cfg=CURRICULUM):\n",
    "    base, maxs = cfg['base_sims'], cfg['max_sims']\n",
    "    f_iter = min(1.0, iter_idx / float(cfg['sims_warmup_iters']))\n",
    "    sims_iter = int(base + (maxs - base) * f_iter)\n",
    "    f_move = min(1.0, move_no / float(cfg['sims_ramp_moves']))\n",
    "    sims = int(base + (sims_iter - base) * f_move)\n",
    "    return max(base, min(maxs, sims))\n",
    "\n",
    "def opening_eps_schedule(iter_idx, move_no, cfg=CURRICULUM):\n",
    "    max_eps = cfg['opening_max_eps']\n",
    "    f_iter = max(0.0, 1.0 - iter_idx/float(cfg['opening_stop_iter']))\n",
    "    f_move = max(0.0, 1.0 - move_no/float(cfg['opening_stop_move']))\n",
    "    return max_eps * f_iter * f_move\n",
    "\n",
    "def temperature_for_move(move_no, cfg=CURRICULUM):\n",
    "    return 1.0 if move_no < cfg['temp_opening_moves'] else 0.0\n",
    "\n",
    "def self_play_episode(env, model, mcts, iter_idx=1, cfg=CURRICULUM):\n",
    "    env.reset()\n",
    "    traj, done, move_no = [], False, 0\n",
    "    while not done:\n",
    "        mcts.simulations = sims_schedule(iter_idx, move_no, cfg)\n",
    "        eps_open = opening_eps_schedule(iter_idx, move_no, cfg)\n",
    "        root = mcts.run(\n",
    "            env,\n",
    "            add_root_noise=True,\n",
    "            dir_alpha=0.15, dir_eps=0.25,\n",
    "            opening_bias=center_bias(env.N, sigma=0.35),\n",
    "            opening_eps=eps_open\n",
    "        )\n",
    "        T = temperature_for_move(move_no, cfg)\n",
    "        a = mcts.select_action(root, temperature=T)\n",
    "\n",
    "        pi = np.zeros(env.N * env.N, dtype=np.float32)\n",
    "        for aa, child in root.children.items():\n",
    "            pi[aa] = child.visit_count\n",
    "        s = pi.sum()\n",
    "        if s > 0: pi /= s\n",
    "\n",
    "        traj.append((env.to_planes(), pi, env.current_player))\n",
    "        _, reward, done = env.step(a)\n",
    "        move_no += 1\n",
    "\n",
    "    # --- FIXED outcome mapping ---\n",
    "    if reward == 0:\n",
    "        outcome = 0\n",
    "    else:\n",
    "        winner = -env.current_player     # player who just moved\n",
    "        outcome = +1 if winner == +1 else -1\n",
    "\n",
    "    data = []\n",
    "    for planes, pi, player in traj:\n",
    "        z = outcome * player\n",
    "        for planes_aug, pi_aug in symmetries_board_and_pi(planes, pi, env.N):\n",
    "            data.append((planes_aug.astype(np.float32), pi_aug.astype(np.float32), float(z)))\n",
    "    return data\n",
    "\n",
    "def train_step(\n",
    "    model,\n",
    "    optimizer,\n",
    "    batch,\n",
    "    N,\n",
    "    policy_coef=1.0,\n",
    "    value_coef=0.5,\n",
    "    entropy_coef=0.0,   # try 1e-3 for the first ~20 iterations\n",
    "    grad_clip=None,     # e.g., 1.0 to clip by norm\n",
    "):\n",
    "    # batch: list of (planes[3,N,N], pi[N*N], z in {-1,0,+1})\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    planes = torch.from_numpy(np.stack([b[0] for b in batch])).to(device=device, dtype=torch.float32)\n",
    "    pi     = torch.from_numpy(np.stack([b[1] for b in batch])).to(device=device, dtype=torch.float32)\n",
    "    z      = torch.tensor([[b[2]] for b in batch], device=device, dtype=torch.float32)\n",
    "\n",
    "    logits, value = model(planes)                 # logits: [B, N*N], value: [B,1]\n",
    "    logp = F.log_softmax(logits, dim=1)\n",
    "    p    = F.softmax(logits, dim=1)\n",
    "\n",
    "    # policy cross-entropy with the MCTS targets\n",
    "    policy_loss = -(pi * logp).sum(dim=1).mean()\n",
    "    # value MSE to {-1,0,1} (after your z fix in self-play)\n",
    "    value_loss  = F.mse_loss(value, z)\n",
    "    # optional exploration bonus early in training\n",
    "    entropy     = -(p * logp).sum(dim=1).mean()\n",
    "\n",
    "    loss = policy_coef * policy_loss + value_coef * value_loss - entropy_coef * entropy\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    if grad_clip is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss.item()), float(policy_loss.item()), float(value_loss.item())\n",
    "\n",
    "def random_action(env):\n",
    "    return random.choice(env.get_legal_actions())\n",
    "\n",
    "def evaluate_vs_random(model, sims=800, games=50, N=9, win_len=5, device=None):\n",
    "    wins = draws = losses = 0\n",
    "    for g in range(games):\n",
    "        env = Gomoku(N=N, win_len=win_len)\n",
    "        my_color = +1 if (g % 2 == 0) else -1\n",
    "        env.current_player = my_color\n",
    "        my_mcts = MCTS(model, simulations=sims, c_puct=2.0, win_len=win_len, device=device)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.current_player == my_color:\n",
    "                my_mcts.sync_to_env(env)                     # <-- reuse\n",
    "                root = my_mcts.run(env, add_root_noise=False)\n",
    "                a = my_mcts.select_action(root, temperature=0.0)\n",
    "            else:\n",
    "                a = random_action(env)\n",
    "            _, reward, done = env.step(a)\n",
    "\n",
    "        if reward == 0: draws += 1\n",
    "        else:\n",
    "            winner = -env.current_player\n",
    "            if winner == my_color: wins += 1\n",
    "            else: losses += 1\n",
    "    print(f\"[Eval vs random] W {wins}  D {draws}  L {losses}  (games={games})\")\n",
    "    return wins, draws, losses\n",
    "\n",
    "# ------------- main training loop (sketch) -------------\n",
    "def train_gomoku(num_iters=200, episodes_per_iter=20, sims=800, batch_size=256):\n",
    "    env = Gomoku(N=9, win_len=5)\n",
    "    \n",
    "    # ---- GPU setup (put this right after you create the model) ----\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = PVResNet(board_size=env.N, channels=64, n_blocks=6)\n",
    "    model.to(device)\n",
    "    torch.backends.cudnn.benchmark = True  # ok for fixed-size convs\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_iters)\n",
    "\n",
    "    # Pass device into MCTS so _expand_and_evaluate runs the net on GPU\n",
    "    mcts = MCTS(model, simulations=sims, c_puct=2.0, win_len=env.win_len, device=device)\n",
    "    replay = Replay(200_000)\n",
    "\n",
    "    ENTROPY_WARM_ITERS = 20     # turn on small entropy bonus for first ~20 iterations\n",
    "    ENTROPY_COEF_START = 1e-3   # or 5e-4\n",
    "\n",
    "    for it in range(1, num_iters+1):\n",
    "        # ------- self-play -------\n",
    "        for _ in range(episodes_per_iter):\n",
    "            data = self_play_episode(env, model, mcts, iter_idx=it, cfg=CURRICULUM)\n",
    "            for item in data:\n",
    "                replay.add(item)\n",
    "\n",
    "        # ------- train steps -------\n",
    "        # schedule entropy: on for first ENTROPY_WARM_ITERS, then 0.0\n",
    "        # entropy_coef = ENTROPY_COEF_START if it <= ENTROPY_WARM_ITERS else 0.0\n",
    "        # cosine decay from ENTROPY_COEF_START → 0 over ENTROPY_WARM_ITERS\n",
    "        if it <= ENTROPY_WARM_ITERS:\n",
    "            t = (it-1) / max(1, ENTROPY_WARM_ITERS-1)\n",
    "            entropy_coef = 0.5 * (1 + math.cos(math.pi * t)) * ENTROPY_COEF_START\n",
    "        else:\n",
    "            entropy_coef = 0.0\n",
    "\n",
    "        losses = []\n",
    "        num_batches = min(400, len(replay)//batch_size)\n",
    "        for _ in range(num_batches):\n",
    "            batch = replay.sample(batch_size)\n",
    "            l, pl, vl = train_step(\n",
    "                model, optimizer, batch, env.N,\n",
    "                policy_coef=1.0, value_coef=0.5,\n",
    "                entropy_coef=entropy_coef,\n",
    "                grad_clip=1.0,   # optional but often helpful\n",
    "            )\n",
    "            losses.append(l)\n",
    "\n",
    "        sched.step()\n",
    "        print(f\"Iter {it:03d} | replay {len(replay)} | sims {sims} | \"\n",
    "              f\"lr {sched.get_last_lr()[0]:.4e} | loss {np.mean(losses) if losses else 0:.3f}\")\n",
    "\n",
    "        # cheap/occasional evals only if you want\n",
    "        # if it % 25 == 0:\n",
    "        #     evaluate_vs_random(model, sims=200, games=8, N=env.N, win_len=env.win_len)\n",
    "\n",
    "        if it % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"models/gomoku_alpha_zero_iter{it:03d}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"models/gomoku_alpha_zero.pth\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60033c42-23d3-48be-af4a-30de9d67be59",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_gomoku\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 304\u001b[39m, in \u001b[36mtrain_gomoku\u001b[39m\u001b[34m(num_iters, episodes_per_iter, sims, batch_size)\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_iters+\u001b[32m1\u001b[39m):\n\u001b[32m    302\u001b[39m     \u001b[38;5;66;03m# ------- self-play -------\u001b[39;00m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes_per_iter):\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         data = \u001b[43mself_play_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCURRICULUM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[32m    306\u001b[39m             replay.add(item)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 196\u001b[39m, in \u001b[36mself_play_episode\u001b[39m\u001b[34m(env, model, mcts, iter_idx, cfg)\u001b[39m\n\u001b[32m    188\u001b[39m root = mcts.run(\n\u001b[32m    189\u001b[39m     env,\n\u001b[32m    190\u001b[39m     add_root_noise=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     opening_eps=eps_open\n\u001b[32m    194\u001b[39m )\n\u001b[32m    195\u001b[39m T = temperature_for_move(move_no, cfg)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m a = \u001b[43mmcts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m pi = np.zeros(env.N * env.N, dtype=np.float32)\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m aa, child \u001b[38;5;129;01min\u001b[39;00m root.children.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 195\u001b[39m, in \u001b[36mMCTS.select_action\u001b[39m\u001b[34m(self, root, temperature, tactical_guard)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Drop any move that lets opponent win in 1 (if any safe survivors exist)\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tactical_guard:\n\u001b[32m    194\u001b[39m     survivors = [a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m legal_idxs\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m                  \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopponent_has_immediate_win_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m                                                         \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwin_len\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(survivors) > \u001b[32m0\u001b[39m:\n\u001b[32m    199\u001b[39m         legal_idxs = np.array(survivors, dtype=\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 230\u001b[39m, in \u001b[36mMCTS.opponent_has_immediate_win_after\u001b[39m\u001b[34m(raw_board_tuple, my_action, my_player, N, win_len)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m oa \u001b[38;5;129;01min\u001b[39;00m legal_opp:\n\u001b[32m    229\u001b[39m     nb = apply_action_raw(child, oa, opp, N)\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     winner, done = \u001b[43mcheck_terminal_gomoku\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m winner == opp:\n\u001b[32m    232\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mcheck_terminal_gomoku\u001b[39m\u001b[34m(raw_board_tuple, N, win_len)\u001b[39m\n\u001b[32m     69\u001b[39m             r -= \u001b[32m1\u001b[39m; c += \u001b[32m1\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m: \u001b[38;5;28;01mreturn\u001b[39;00m (+\u001b[32m1.0\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_k(-\u001b[32m1\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m (-\u001b[32m1.0\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (b != \u001b[32m0\u001b[39m).all(): \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[32m0.0\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mcheck_terminal_gomoku.<locals>.has_k\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[32m     32\u001b[39m         run = run + \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m b[r, c] == p \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run >= win_len: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# cols\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_gomoku()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500607d0-5ccc-4750-8364-a61f22098fde",
   "metadata": {},
   "source": [
    "# 6. Strength Ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ad424-5288-4808-8f83-bd3a2df30cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_vs_mcts(model, my_sims, opp_sims, games=40, N=9, win_len=5, device=None):\n",
    "    wins = draws = losses = 0\n",
    "    for g in range(games):\n",
    "        env = Gomoku(N=N, win_len=win_len)\n",
    "        my_color = +1 if (g % 2 == 0) else -1\n",
    "        env.current_player = my_color\n",
    "\n",
    "        my_mcts  = MCTS(model, simulations=my_sims,  c_puct=2.0, win_len=win_len, device=device)\n",
    "        opp_mcts = MCTS(model, simulations=opp_sims, c_puct=2.0, win_len=win_len, device=device)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.current_player == my_color:\n",
    "                my_mcts.sync_to_env(env)\n",
    "                root = my_mcts.run(env, add_root_noise=False)\n",
    "                a = my_mcts.select_action(root, temperature=0.0)\n",
    "            else:\n",
    "                opp_mcts.sync_to_env(env)\n",
    "                root = opp_mcts.run(env, add_root_noise=False)\n",
    "                a = opp_mcts.select_action(root, temperature=0.0)\n",
    "            _, reward, done = env.step(a)\n",
    "\n",
    "        if reward == 0: draws += 1\n",
    "        else:\n",
    "            winner = -env.current_player\n",
    "            if winner == my_color: wins += 1\n",
    "            else: losses += 1\n",
    "    return wins, draws, losses\n",
    "\n",
    "def strength_ladder(model, N=9, win_len=5):\n",
    "    print(\"[Ladder] vs random (color-aware)\")\n",
    "    evaluate_vs_random(model, sims=400, games=50, N=N, win_len=win_len)\n",
    "\n",
    "    print(\"\\n[Ladder] symmetric sims (ours == theirs)\")\n",
    "    for sims in [100, 200, 400, 800, 1200]:\n",
    "        w,d,l = eval_vs_mcts(model, my_sims=sims, opp_sims=sims, games=50, N=N, win_len=win_len)\n",
    "        print(f\"sims={sims:4}  W {w:2}  D {d:2}  L {l:2}\")\n",
    "\n",
    "    print(\"\\n[Ladder] advantage (ours ≥ 800 sims)\")\n",
    "    for opp in [100, 200, 400, 800, 1200]:\n",
    "        w,d,l = eval_vs_mcts(model, my_sims=max(opp, 800), opp_sims=opp, games=50, N=N, win_len=win_len)\n",
    "        print(f\"opp={opp:4}  ours={max(opp,800):4}  W {w:2}  D {d:2}  L {l:2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d778f-89e9-4d8c-84f7-c39442958715",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "strength_ladder(model, N=9, win_len=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fa02a-ec59-40e4-8fcc-ee8b52508612",
   "metadata": {},
   "source": [
    "# 7. Interative Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eff38c77-d0f3-4738-9ad0-68cc54035844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipywidgets\n",
    "import ipywidgets as W\n",
    "from IPython.display import display\n",
    "\n",
    "class GomokuUI:\n",
    "    def __init__(self, model, N=9, win_len=5, sims=800, human_player=-1):\n",
    "        self.model = model\n",
    "        self.N = N\n",
    "        self.win_len = win_len\n",
    "        self.human_player = int(human_player)   # +1 black (X), -1 white (O)\n",
    "        self.sims = sims\n",
    "\n",
    "        self.env = Gomoku(N=N, win_len=win_len)\n",
    "        self.buttons = [[W.Button(description=\" \", layout=W.Layout(width=\"40px\", height=\"40px\")) \n",
    "                         for _ in range(N)] for _ in range(N)]\n",
    "        for r in range(N):\n",
    "            for c in range(N):\n",
    "                self.buttons[r][c].on_click(self._make_cb(r, c))\n",
    "\n",
    "        self.msg = W.HTML(value=\"\")\n",
    "        self.reset_btn = W.Button(description=\"Reset\")\n",
    "        self.reset_btn.on_click(self.reset)\n",
    "\n",
    "        grid = W.GridBox(\n",
    "            children=[self.buttons[r][c] for r in range(N) for c in range(N)],\n",
    "            layout=W.Layout(grid_template_columns=f\"repeat({N}, 42px)\")\n",
    "        )\n",
    "        display(W.VBox([grid, W.HBox([self.reset_btn]), self.msg]))\n",
    "\n",
    "        # AI may start\n",
    "        if self.human_player == -1 and self.env.current_player == +1:\n",
    "            self.ai_move()\n",
    "        self.refresh()\n",
    "\n",
    "    def _make_cb(self, r, c):\n",
    "        def _cb(btn):\n",
    "            if self.env.current_player != self.human_player:\n",
    "                return\n",
    "            a = r * self.N + c\n",
    "            if a not in self.env.get_legal_actions():\n",
    "                return\n",
    "            _, _, done = self.env.step(a)\n",
    "            if not done:\n",
    "                self.ai_move()\n",
    "            self.refresh()\n",
    "        return _cb\n",
    "\n",
    "    \"\"\"\n",
    "    # Deterministic\n",
    "    def ai_move(self):\n",
    "        mcts = MCTS(self.model, simulations=self.sims, c_puct=1.5, win_len=self.win_len)\n",
    "        root = mcts.run(self.env, add_root_noise=False)  # no noise for a strong play\n",
    "        a = mcts.select_action(root, temperature=0.0)\n",
    "        self.env.step(a)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Less deterministic, more human-like variety\n",
    "    def ai_move(self):\n",
    "        mcts = MCTS(self.model, simulations=self.sims, c_puct=2.0, win_len=self.win_len)\n",
    "        # Optional: tiny root noise for variety in opening\n",
    "        add_noise = (self.env.num_moves() < 8)  # implement num_moves() if you like\n",
    "        root = mcts.run(self.env, add_root_noise=add_noise, dir_alpha=0.15, dir_eps=0.10)\n",
    "        # Temperature schedule\n",
    "        T = 1.0 if self.env.num_moves() < 8 else 0.0\n",
    "        a = mcts.select_action(root, temperature=T)\n",
    "        self.env.step(a)\n",
    "    \n",
    "    def refresh(self):\n",
    "        sym = {1:\"X\", -1:\"O\", 0:\" \"}\n",
    "        for r in range(self.N):\n",
    "            for c in range(self.N):\n",
    "                v = int(self.env.board[r, c])\n",
    "                b = self.buttons[r][c]\n",
    "                b.description = sym[v]\n",
    "                b.disabled = bool(v != 0)\n",
    "\n",
    "        reward, done = self.env.check_winner_fast()\n",
    "        if done:\n",
    "            if reward == 0:\n",
    "                self.msg.value = \"<b>Draw.</b> Click Reset.\"\n",
    "            else:\n",
    "                winner = 'X' if -self.env.current_player == +1 else 'O'\n",
    "                self.msg.value = f\"<b>{winner} wins!</b> Click Reset.\"\n",
    "            for r in range(self.N):\n",
    "                for c in range(self.N):\n",
    "                    self.buttons[r][c].disabled = True\n",
    "        else:\n",
    "            turn = 'X' if self.env.current_player==+1 else 'O'\n",
    "            self.msg.value = f\"Player {turn} to move.\"\n",
    "\n",
    "    def reset(self, _=None):\n",
    "        self.env.reset()\n",
    "        if self.human_player == -1 and self.env.current_player == +1:\n",
    "            self.ai_move()\n",
    "        self.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d055083b-7742-42c6-affc-441cf34cbaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f528294456462e8e9a70028eb8785f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(GridBox(children=(Button(description=' ', layout=Layout(height='40px', width='40px'), style=But…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "model.eval()\n",
    "ui = GomokuUI(model, N=9, win_len=5, sims=600, human_player=-1)  # human plays O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf874784-901b-471c-924a-97636861c0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e008aaed57a43fba0da330acebde1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(GridBox(children=(Button(description=' ', layout=Layout(height='40px', width='40px'), style=But…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "model.eval()\n",
    "ui = GomokuUI(model, N=9, win_len=5, sims=1600, human_player=-1)  # human plays O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22aad97-8a14-4311-9c0a-aff9a9e23641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
