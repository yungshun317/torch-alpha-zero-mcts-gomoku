{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9286a65-7939-4ae2-83dc-dfe41e5ecb33",
   "metadata": {},
   "source": [
    "# 1. Gomoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "207a7abc-73dc-445d-a062-c579be416971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Gomoku:\n",
    "    \"\"\"\n",
    "    N×N board, players in {+1 (black), -1 (white)}, empty=0.\n",
    "    Win condition: 5 in a row (any direction).\n",
    "    \"\"\"\n",
    "    def __init__(self, N=9, win_len=5):\n",
    "        self.N = int(N)\n",
    "        self.win_len = int(win_len)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.N, self.N), dtype=np.int8)\n",
    "        self.current_player = 1\n",
    "        self.done = False\n",
    "        self.last_move = None  # (r,c) or None\n",
    "        return self.get_state()  # canonical 1×(N*N) if you want; see below\n",
    "\n",
    "    # --- Canonical features for NN (3 planes): me, opp, to-move ---\n",
    "    def to_planes(self):\n",
    "        me  = (self.board == self.current_player).astype(np.float32)\n",
    "        opp = (self.board == -self.current_player).astype(np.float32)\n",
    "        turn = np.full_like(me, 1.0, dtype=np.float32)  # “to move” plane\n",
    "        # shape: [3, N, N]\n",
    "        return np.stack([me, opp, turn], axis=0)\n",
    "\n",
    "    # For drop-in compatibility with your older code:\n",
    "    def get_state(self):\n",
    "        # vectorized canonical state (board * current_player) like tic-tac-toe\n",
    "        return (self.board.flatten() * self.current_player).astype(np.float32)\n",
    "\n",
    "    def get_legal_actions(self):\n",
    "        # actions are 0..N*N-1\n",
    "        flat = self.board.reshape(-1)\n",
    "        return [i for i in range(self.N * self.N) if flat[i] == 0]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply action index (0..N^2-1). Returns (state, reward, done).\"\"\"\n",
    "        if self.done:\n",
    "            return self.get_state(), 0.0, True\n",
    "        r, c = divmod(int(action), self.N)\n",
    "        if self.board[r, c] != 0:\n",
    "            # illegal move penalty (AlphaZero typically forbids via masking;\n",
    "            # but keep this as a safeguard)\n",
    "            self.done = True\n",
    "            return self.get_state(), -1.0, True\n",
    "\n",
    "        self.board[r, c] = self.current_player\n",
    "        self.last_move = (r, c)\n",
    "\n",
    "        reward, done = self.check_winner_fast()\n",
    "        self.done = done\n",
    "        state = self.get_state()\n",
    "        self.current_player *= -1\n",
    "        return state, float(reward), bool(done)\n",
    "\n",
    "    # --------- winner checks ----------\n",
    "    def check_winner_fast(self):\n",
    "        \"\"\"\n",
    "        Check only the lines passing through self.last_move for speed.\n",
    "        Returns (reward_for_player_who_just_moved, done)\n",
    "        reward ∈ {+1 win, 0 draw/ongoing, -1 loss} relative to the mover.\n",
    "        \"\"\"\n",
    "        if self.last_move is None:\n",
    "            return 0.0, False\n",
    "        r, c = self.last_move\n",
    "        p = self.board[r, c]\n",
    "        if p == 0:\n",
    "            return 0.0, False\n",
    "\n",
    "        if ( self._count_dir(r, c, 1, 0, p) + self._count_dir(r, c, -1, 0, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 0, 1, p) + self._count_dir(r, c, 0, -1, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 1, 1, p) + self._count_dir(r, c, -1, -1, p) - 1 >= self.win_len or\n",
    "             self._count_dir(r, c, 1, -1, p) + self._count_dir(r, c, -1, 1, p) - 1 >= self.win_len ):\n",
    "            # player p (who just moved) wins: reward +1 for mover\n",
    "            return 1.0, True\n",
    "\n",
    "        if (self.board != 0).all():\n",
    "            return 0.0, True  # draw\n",
    "\n",
    "        return 0.0, False\n",
    "\n",
    "    def _count_dir(self, r, c, dr, dc, p):\n",
    "        \"\"\"Count contiguous stones with color p starting at (r,c) inclusive along (dr,dc).\"\"\"\n",
    "        N = self.N\n",
    "        cnt = 0\n",
    "        rr, cc = r, c\n",
    "        while 0 <= rr < N and 0 <= cc < N and self.board[rr, cc] == p:\n",
    "            cnt += 1\n",
    "            rr += dr\n",
    "            cc += dc\n",
    "        return cnt\n",
    "\n",
    "    def num_moves(self):\n",
    "        \"\"\"Return the number of moves played so far.\"\"\"\n",
    "        # If you track move history: return len(self.move_history)\n",
    "        # If not, just count non-zero cells:\n",
    "        return int((self.board != 0).sum())\n",
    "    \n",
    "    # --------- convenience ----------\n",
    "    def render_ascii(self):\n",
    "        sym = {1:'X', -1:'O', 0:'.'}\n",
    "        print(\"\\n\".join(\" \".join(sym[v] for v in row) for row in self.board))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7745f-094c-42a4-ac6a-404e421c0b71",
   "metadata": {},
   "source": [
    "# 2. ResNet Policy-Value Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d937944f-353a-4802-a13e-2fe97c41c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(x + y)\n",
    "\n",
    "class PVResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy–Value net for Gomoku.\n",
    "    Input: [B, 3, N, N]\n",
    "    Outputs:\n",
    "      - policy_logits: [B, N*N]\n",
    "      - value:         [B, 1]  (tanh)\n",
    "    \"\"\"\n",
    "    def __init__(self, board_size=9, channels=64, n_blocks=6):\n",
    "        super().__init__()\n",
    "        self.N = board_size\n",
    "        C = channels\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, C, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(C),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.trunk = nn.Sequential(*[ResidualBlock(C) for _ in range(n_blocks)])\n",
    "\n",
    "        # Policy head\n",
    "        self.p_head = nn.Sequential(\n",
    "            nn.Conv2d(C, 2, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.p_fc = nn.Linear(2 * self.N * self.N, self.N * self.N)\n",
    "\n",
    "        # Value head\n",
    "        self.v_head = nn.Sequential(\n",
    "            nn.Conv2d(C, 1, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.v_fc1 = nn.Linear(1 * self.N * self.N, C)\n",
    "        self.v_fc2 = nn.Linear(C, 1)\n",
    "\n",
    "        # (optional) init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):  # x: [B, 3, N, N]\n",
    "        z = self.stem(x)\n",
    "        z = self.trunk(z)\n",
    "\n",
    "        # policy\n",
    "        p = self.p_head(z)\n",
    "        p = p.view(p.size(0), -1)\n",
    "        policy_logits = self.p_fc(p)  # [B, N*N]\n",
    "\n",
    "        # value\n",
    "        v = self.v_head(z)\n",
    "        v = v.view(v.size(0), -1)\n",
    "        v = F.relu(self.v_fc1(v))\n",
    "        value = torch.tanh(self.v_fc2(v))  # [-1, 1]\n",
    "\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074746e0-9ece-44e3-bc79-40358fdb2873",
   "metadata": {},
   "source": [
    "# 3. Monte Carlo Tree Search (MCTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71e6c3cd-3923-4635-9351-cee810961de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gomoku_planes_from_raw(raw_board_tuple, player, N):\n",
    "    \"\"\"Build [3, N, N] planes from (raw_board, player_to_move).\"\"\"\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N)\n",
    "    me  = (b == player).astype(np.float32)\n",
    "    opp = (b == -player).astype(np.float32)\n",
    "    turn = np.ones_like(me, dtype=np.float32)\n",
    "    return np.stack([me, opp, turn], axis=0)\n",
    "\n",
    "def legal_actions_from_raw(raw_board_tuple, N):\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N)\n",
    "    return [i for i in range(N*N) if b.flat[i] == 0]\n",
    "\n",
    "def apply_action_raw(raw_board_tuple, action, player, N):\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N).copy()\n",
    "    r, c = divmod(int(action), N)\n",
    "    b[r, c] = player\n",
    "    return tuple(b.reshape(-1))\n",
    "\n",
    "def check_terminal_gomoku(raw_board_tuple, N, win_len=5):\n",
    "    \"\"\"Return (value_for_current_node_player, done). Full scan (O(N^2)), fine for 9x9.\"\"\"\n",
    "    b = np.array(raw_board_tuple, dtype=np.int8).reshape(N, N)\n",
    "\n",
    "    def has_k(p):\n",
    "        # rows\n",
    "        for r in range(N):\n",
    "            run = 0\n",
    "            for c in range(N):\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "        # cols\n",
    "        for c in range(N):\n",
    "            run = 0\n",
    "            for r in range(N):\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "        # diag \\\n",
    "        for r0 in range(N):\n",
    "            run = 0\n",
    "            r, c = r0, 0\n",
    "            while r < N and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r += 1; c += 1\n",
    "        for c0 in range(1, N):\n",
    "            run = 0\n",
    "            r, c = 0, c0\n",
    "            while r < N and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r += 1; c += 1\n",
    "        # diag /\n",
    "        for r0 in range(N):\n",
    "            run = 0\n",
    "            r, c = r0, 0\n",
    "            while r >= 0 and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r -= 1; c += 1\n",
    "        for c0 in range(1, N):\n",
    "            run = 0\n",
    "            r, c = N-1, c0\n",
    "            while r >= 0 and c < N:\n",
    "                run = run + 1 if b[r, c] == p else 0\n",
    "                if run >= win_len: return True\n",
    "                r -= 1; c += 1\n",
    "        return False\n",
    "\n",
    "    if has_k(+1): return (+1.0, True)\n",
    "    if has_k(-1): return (-1.0, True)\n",
    "    if (b != 0).all(): return (0.0, True)\n",
    "    return (0.0, False)\n",
    "\n",
    "# Opening bias\n",
    "def center_bias(N, sigma=0.35):\n",
    "    \"\"\"Return normalized N*N center-biased prior (Gaussian on [0,1]^2 grid).\"\"\"\n",
    "    xs = np.linspace(0, 1, N)\n",
    "    X, Y = np.meshgrid(xs, xs, indexing='ij')\n",
    "    cx = cy = 0.5\n",
    "    g = np.exp(-((X-cx)**2 + (Y-cy)**2) / (2*(sigma**2)))\n",
    "    v = g.reshape(-1).astype(np.float32)\n",
    "    v /= v.sum() + 1e-8\n",
    "    return v\n",
    "\n",
    "def mix_opening_bias(node, N, bias_vec, eps=0.25):\n",
    "    \"\"\"Blend bias_vec into child priors at the root (legal only).\"\"\"\n",
    "    if eps is None or eps <= 0.0:\n",
    "        return\n",
    "    legal = list(node.children.keys())\n",
    "    if not legal: return\n",
    "    b = bias_vec.copy()\n",
    "    # renorm on legal only\n",
    "    s = b[legal].sum()\n",
    "    b = b / (s + 1e-8)\n",
    "    for a in legal:\n",
    "        p = node.children[a].prior\n",
    "        node.children[a].prior = (1 - eps) * p + eps * float(b[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efa8eaeb-17ca-4b4e-8727-7a773b80035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Node ----\n",
    "class Node:\n",
    "    def __init__(self, raw_board_tuple, player_to_move):\n",
    "        self.state = raw_board_tuple          # tuple of length N*N (0/±1)\n",
    "        self.player = int(player_to_move)     # +1 or -1\n",
    "        self.children = {}                    # action -> Node\n",
    "        self.visit_count = 0\n",
    "        self.total_value = 0.0                # mean value from this node's player perspective\n",
    "        self.prior = 0.0\n",
    "\n",
    "    def value(self):\n",
    "        return 0.0 if self.visit_count == 0 else self.total_value / self.visit_count\n",
    "\n",
    "# ---- MCTS ----\n",
    "class MCTS:\n",
    "    def __init__(self, model, simulations=800, c_puct=2.0, win_len=5):\n",
    "        self.model = model\n",
    "        self.simulations = simulations\n",
    "        self.c_puct = c_puct\n",
    "        self.win_len = win_len\n",
    "        self.ttable = {}   # (state_tuple, player, N) -> (priors[N*N], value_float)\n",
    "        self.root = None\n",
    "\n",
    "    # ---------- public API ----------\n",
    "    def run(self, env, add_root_noise=True, dir_alpha=0.3, dir_eps=0.25,\n",
    "            opening_bias=None, opening_eps=0.25):\n",
    "        N = env.N\n",
    "        self.sync_to_env(env)\n",
    "    \n",
    "        if add_root_noise:\n",
    "            self._add_dirichlet_noise(self.root, N, alpha=dir_alpha, eps=dir_eps)\n",
    "    \n",
    "        if opening_bias is not None:\n",
    "            # was: mix_opening_bias(self.root, N, opening_eps)\n",
    "            mix_opening_bias(self.root, N, opening_bias, eps=opening_eps)\n",
    "    \n",
    "        for _ in range(self.simulations):\n",
    "            self._simulate(self.root, N)\n",
    "        return self.root\n",
    "\n",
    "    def advance(self, action, N):\n",
    "        if self.root and action in self.root.children:\n",
    "            self.root = self.root.children[action]\n",
    "        else:\n",
    "            base_state = self.root.state if self.root else tuple([0]*(N*N))\n",
    "            base_player = self.root.player if self.root else +1\n",
    "            child_board = apply_action_raw(base_state, int(action), base_player, N)\n",
    "            self.root = Node(child_board, -base_player)\n",
    "            if not self.root.children:\n",
    "                _ = self._expand_and_evaluate(self.root, N)\n",
    "\n",
    "    def sync_to_env(self, env):\n",
    "        N = env.N\n",
    "        state = tuple(env.board.reshape(-1))\n",
    "        player = env.current_player\n",
    "\n",
    "        if self.root and self.root.state == state and self.root.player == player:\n",
    "            return\n",
    "        if self.root:\n",
    "            for ch in self.root.children.values():\n",
    "                if ch.state == state and ch.player == player:\n",
    "                    self.root = ch\n",
    "                    return\n",
    "        self.root = Node(state, player)\n",
    "        if not self.root.children:\n",
    "            _ = self._expand_and_evaluate(self.root, N)\n",
    "\n",
    "    # ---------- core search ----------\n",
    "    def _simulate(self, node, N):\n",
    "        winner, done = check_terminal_gomoku(node.state, N, self.win_len)\n",
    "        if done:\n",
    "            if winner == 0:\n",
    "                v = 0.0\n",
    "            elif winner == node.player:\n",
    "                v = +1.0\n",
    "            else:\n",
    "                v = -1.0\n",
    "            node.visit_count += 1\n",
    "            node.total_value += v\n",
    "            return v\n",
    "\n",
    "        # short-circuit immediate win\n",
    "        can_win, win_a = self.has_immediate_win(node.state, node.player, N, self.win_len)\n",
    "        if can_win:\n",
    "            v = +1.0\n",
    "            node.visit_count += 1\n",
    "            node.total_value += v\n",
    "            if win_a not in node.children:\n",
    "                nb = apply_action_raw(node.state, win_a, node.player, N)\n",
    "                ch = Node(nb, -node.player)\n",
    "                ch.prior = 1.0\n",
    "                node.children[win_a] = ch\n",
    "            child = node.children[win_a]\n",
    "            child.visit_count += 1\n",
    "            child.total_value  += -v   # from child’s perspective, value flips sign    \n",
    "            return v\n",
    "\n",
    "        if node.visit_count == 0 and not node.children:\n",
    "            v = self._expand_and_evaluate(node, N)\n",
    "            node.visit_count += 1\n",
    "            node.total_value += v\n",
    "            return v\n",
    "\n",
    "        best_a, best_score = None, -1e9\n",
    "        sqrt_N = np.sqrt(node.visit_count + 1e-8)\n",
    "        for a, child in node.children.items():\n",
    "            ucb = child.value() + self.c_puct * child.prior * (sqrt_N / (1 + child.visit_count))\n",
    "            if ucb > best_score:\n",
    "                best_score, best_a = ucb, a\n",
    "\n",
    "        child = node.children[best_a]\n",
    "        v_child = self._simulate(child, N)\n",
    "        v = -v_child\n",
    "        node.visit_count += 1\n",
    "        node.total_value += v\n",
    "        return v\n",
    "\n",
    "    def _expand_and_evaluate(self, node, N):\n",
    "        key = (node.state, node.player, N)\n",
    "        if key in self.ttable:\n",
    "            priors, v = self.ttable[key]\n",
    "        else:\n",
    "            planes = gomoku_planes_from_raw(node.state, node.player, N).astype(np.float32)\n",
    "            st = torch.from_numpy(planes).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                logits, value = self.model(st)\n",
    "                v = float(value.item())\n",
    "\n",
    "            logits_np = logits.squeeze(0).detach().cpu().numpy()\n",
    "            legal = legal_actions_from_raw(node.state, N)\n",
    "\n",
    "            priors = np.zeros(N*N, dtype=np.float32)\n",
    "            if legal:\n",
    "                ll = logits_np[legal]\n",
    "                ll -= ll.max()\n",
    "                p_legal = np.exp(ll)\n",
    "                p_legal /= (p_legal.sum() + 1e-8)\n",
    "                priors[legal] = p_legal\n",
    "\n",
    "            # zero-out moves that allow opponent immediate win\n",
    "            suicidal = []\n",
    "            for a in legal:\n",
    "                if self.opponent_has_immediate_win_after(node.state, a, node.player, N, self.win_len):\n",
    "                    suicidal.append(a)\n",
    "            if 0 < len(suicidal) < len(legal):\n",
    "                for a in suicidal:\n",
    "                    priors[a] = 0.0\n",
    "                s = priors[legal].sum()\n",
    "                if s > 0:\n",
    "                    priors[legal] /= s\n",
    "                else:\n",
    "                    survivors = [a for a in legal if a not in suicidal]\n",
    "                    for a in survivors:\n",
    "                        priors[a] = 1.0 / len(survivors)\n",
    "\n",
    "            self.ttable[key] = (priors.astype(np.float32), v)\n",
    "\n",
    "        # create all legal children\n",
    "        legal = legal_actions_from_raw(node.state, N)\n",
    "        for a in legal:\n",
    "            child_board = apply_action_raw(node.state, int(a), node.player, N)\n",
    "            ch = Node(child_board, -node.player)\n",
    "            ch.prior = float(self.ttable[key][0][a])\n",
    "            node.children[int(a)] = ch\n",
    "\n",
    "        return self.ttable[key][1]\n",
    "\n",
    "    # ---------- root noise ----------\n",
    "    def _add_dirichlet_noise(self, node, N, alpha=0.3, eps=0.25):\n",
    "        if not node.children: return\n",
    "        legal_actions = list(node.children.keys())\n",
    "        if not legal_actions: return\n",
    "        noise = np.random.dirichlet([alpha]*len(legal_actions)).astype(np.float32)\n",
    "        for a, n in zip(legal_actions, noise):\n",
    "            p = node.children[a].prior\n",
    "            node.children[a].prior = (1 - eps)*p + eps*float(n)\n",
    "\n",
    "    # ---------- action picker ----------\n",
    "    def select_action(self, root, temperature=1.0, tactical_guard=True):\n",
    "        N2 = int(np.sqrt(len(root.state)))\n",
    "        visits = np.zeros(N2*N2, dtype=np.float64)\n",
    "        legal  = np.zeros(N2*N2, dtype=bool)\n",
    "        for a, child in root.children.items():\n",
    "            visits[a] = child.visit_count\n",
    "            legal[a]  = True\n",
    "    \n",
    "        legal_idxs = np.where(legal)[0]\n",
    "        if legal_idxs.size == 0:\n",
    "            legal_idxs = np.array(legal_actions_from_raw(root.state, N2), dtype=int)\n",
    "            return int(np.random.choice(legal_idxs)) if legal_idxs.size > 0 else 0\n",
    "    \n",
    "        # Drop any move that lets opponent win in 1 (if any safe survivors exist)\n",
    "        if tactical_guard:\n",
    "            survivors = [a for a in legal_idxs\n",
    "                         if not self.opponent_has_immediate_win_after(root.state, int(a),\n",
    "                                                                 root.player, N2, self.win_len)]\n",
    "            \n",
    "            if len(survivors) > 0:\n",
    "                legal_idxs = np.array(survivors, dtype=int)\n",
    "                visits = visits  # visits are still aligned by absolute indices; OK\n",
    "    \n",
    "        if temperature == 0.0:\n",
    "            vmax = visits[legal_idxs].max()\n",
    "            ties = legal_idxs[visits[legal_idxs] == vmax]\n",
    "            return int(np.random.choice(ties))\n",
    "    \n",
    "        x = visits[legal_idxs] ** (1.0 / max(1e-8, temperature))\n",
    "        p = x / (x.sum() + 1e-8)\n",
    "        return int(np.random.choice(legal_idxs, p=p))\n",
    "\n",
    "    # ---------- tiny tactical helpers ----------\n",
    "    @staticmethod\n",
    "    def has_immediate_win(raw_board_tuple, player, N, win_len):\n",
    "        legal = legal_actions_from_raw(raw_board_tuple, N)\n",
    "        if not legal: return False, None\n",
    "        for a in legal:\n",
    "            nb = apply_action_raw(raw_board_tuple, a, player, N)\n",
    "            winner, done = check_terminal_gomoku(nb, N, win_len)\n",
    "            if done and winner == player:\n",
    "                return True, a\n",
    "        return False, None\n",
    "\n",
    "    @staticmethod\n",
    "    def opponent_has_immediate_win_after(raw_board_tuple, my_action, my_player, N, win_len):\n",
    "        child = apply_action_raw(raw_board_tuple, my_action, my_player, N)\n",
    "        opp = -my_player\n",
    "        legal_opp = legal_actions_from_raw(child, N)\n",
    "        for oa in legal_opp:\n",
    "            nb = apply_action_raw(child, oa, opp, N)\n",
    "            winner, done = check_terminal_gomoku(nb, N, win_len)\n",
    "            if done and winner == opp:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c540161-6273-4238-8de1-7bbb0633847d",
   "metadata": {},
   "source": [
    "# 4. Tactical Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a8b0a0e-d470-4034-866b-c2157607d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np, torch\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(0); np.random.seed(0); torch.manual_seed(0)\n",
    "\n",
    "# Build env/model\n",
    "env = Gomoku(N=9, win_len=5)\n",
    "model = PVResNet(board_size=env.N, channels=64, n_blocks=6)\n",
    "# model.load_state_dict(torch.load(\"models/gomoku_alpha_zero.pth\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(model, simulations=800, c_puct=2.0, win_len=env.win_len)\n",
    "\n",
    "# One move:\n",
    "root = mcts.run(env, add_root_noise=True)\n",
    "a = mcts.select_action(root, temperature=1.0)   # temperature=1 in opening, 0 later\n",
    "state, reward, done = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd860e28-d1cc-4f50-86ce-7a70f3835b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picked: 40  expected block: 40\n"
     ]
    }
   ],
   "source": [
    "def must_block_test(model, N=9, win_len=5, sims=800, c_puct=2.0):\n",
    "    b = np.zeros((N, N), dtype=np.int8)\n",
    "    r = 4\n",
    "    for c in [0,1,2,3]:   # single-ended four at edge\n",
    "        b[r, c] = +1\n",
    "\n",
    "    env = Gomoku(N=N, win_len=win_len)\n",
    "    env.board[:] = b\n",
    "    env.current_player = -1   # defender to move\n",
    "\n",
    "    expected_block = r * N + 4  # only one completion for attacker\n",
    "\n",
    "    mcts = MCTS(model, simulations=sims, c_puct=c_puct, win_len=win_len)\n",
    "    root = mcts.run(env, add_root_noise=False)\n",
    "    picked = mcts.select_action(root, temperature=0.0)\n",
    "    return picked, expected_block\n",
    "\n",
    "picked, exp_block = must_block_test(model, sims=100)  # bump sims if needed\n",
    "print(\"picked:\", picked, \" expected block:\", exp_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d405ba22-5311-4d07-b9d0-e0a4cc508a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picked: 40  expected win: 40\n"
     ]
    }
   ],
   "source": [
    "def must_win_test(model, N=9, win_len=5, sims=200):\n",
    "    b = np.zeros((N, N), dtype=np.int8)\n",
    "    r = 4\n",
    "    for c in [0,1,2,3]:\n",
    "        b[r, c] = +1\n",
    "    env = Gomoku(N=N, win_len=win_len)\n",
    "    env.board[:] = b\n",
    "    env.current_player = +1   # attacker to move\n",
    "\n",
    "    expected_win = r * N + 4\n",
    "    mcts = MCTS(model, simulations=sims, c_puct=2.0, win_len=win_len)\n",
    "    root = mcts.run(env, add_root_noise=False)\n",
    "    picked = mcts.select_action(root, temperature=0.0)\n",
    "    return picked, expected_win\n",
    "\n",
    "picked, expected_win = must_win_test(model, sims=100)  # bump sims if needed\n",
    "print(\"picked:\", picked, \" expected win:\", expected_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10dc29df-2cb5-4807-be69-23119b20ea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'picked': 40, 'block': 40, 'is_block': True, 'is_suicide': False, 'pass_test': True}\n"
     ]
    }
   ],
   "source": [
    "def avoid_suicide_test(model, N=9, win_len=5, sims=400, c_puct=2.0):\n",
    "    \"\"\"\n",
    "    Position where attacker has a single immediate winning square.\n",
    "    Defender (-1) must block that square; any other move is suicidal.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    # +1 stones at row r=4, columns 0..3 (edge). Only (4,4) completes five.\n",
    "    b = np.zeros((N, N), dtype=np.int8)\n",
    "    r = 4\n",
    "    for c in [0,1,2,3]:\n",
    "        b[r, c] = +1\n",
    "\n",
    "    env = Gomoku(N=N, win_len=win_len)\n",
    "    env.board[:] = b\n",
    "    env.current_player = -1\n",
    "\n",
    "    block = r*N + 4      # the only square that prevents an immediate win\n",
    "    legal = set(env.get_legal_actions())\n",
    "    assert block in legal\n",
    "\n",
    "    mcts = MCTS(model, simulations=sims, c_puct=c_puct, win_len=win_len)\n",
    "    root = mcts.run(env, add_root_noise=False)\n",
    "    picked = mcts.select_action(root, temperature=0.0, tactical_guard=True)\n",
    "\n",
    "    # Suicide if opponent can then win in one move\n",
    "    raw = tuple(env.board.reshape(-1))\n",
    "    is_suicide = MCTS.opponent_has_immediate_win_after(raw, picked, -1, N, win_len)\n",
    "    is_block   = (picked == block)\n",
    "\n",
    "    return {\n",
    "        \"picked\": picked,\n",
    "        \"block\": block,\n",
    "        \"is_block\": is_block,\n",
    "        \"is_suicide\": is_suicide,\n",
    "        \"pass_test\": (is_block and not is_suicide)\n",
    "    }\n",
    "\n",
    "# Example:\n",
    "res = avoid_suicide_test(model, sims=100)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da4567-9942-4349-8600-0f7306eab903",
   "metadata": {},
   "source": [
    "# 5. Self-Play with Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a09fef56-6773-40c9-806e-2cbe04fb2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, math, collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# --- helpers to rotate/reflect both planes and a flat policy over N*N cells ---\n",
    "def rotate90_board(planes):    # planes: [C,N,N]\n",
    "    return np.rot90(planes, k=1, axes=(1,2)).copy()\n",
    "\n",
    "def rotate180_board(planes):\n",
    "    return np.rot90(planes, k=2, axes=(1,2)).copy()\n",
    "\n",
    "def rotate270_board(planes):\n",
    "    return np.rot90(planes, k=3, axes=(1,2)).copy()\n",
    "\n",
    "def flip_h_board(planes):\n",
    "    return np.flip(planes, axis=2).copy()   # horizontal flip (left-right)\n",
    "\n",
    "def idx_map_rotate90(i, N):\n",
    "    r, c = divmod(i, N)\n",
    "    rr, cc = c, N-1-r\n",
    "    return rr*N + cc\n",
    "\n",
    "def idx_map_rotate180(i, N):\n",
    "    r, c = divmod(i, N)\n",
    "    rr, cc = N-1-r, N-1-c\n",
    "    return rr*N + cc\n",
    "\n",
    "def idx_map_rotate270(i, N):\n",
    "    r, c = divmod(i, N)\n",
    "    rr, cc = N-1-c, r\n",
    "    return rr*N + cc\n",
    "\n",
    "def idx_map_flip_h(i, N):\n",
    "    r, c = divmod(i, N)\n",
    "    rr, cc = r, N-1-c\n",
    "    return rr*N + cc\n",
    "\n",
    "def transform_pi(pi, idx_map, N):\n",
    "    out = np.zeros_like(pi)\n",
    "    for i, p in enumerate(pi):\n",
    "        out[idx_map(i, N)] = p\n",
    "    return out\n",
    "\n",
    "def symmetries_board_and_pi(planes, pi, N):\n",
    "    # 8 sym group: I, R90, R180, R270, FH, FH+R90, FH+R180, FH+R270\n",
    "    out = []\n",
    "\n",
    "    # Identity\n",
    "    out.append((planes.copy(), pi.copy()))\n",
    "\n",
    "    # Rotations\n",
    "    p1, m1 = rotate90_board(planes),  (lambda i: idx_map_rotate90(i, N))\n",
    "    out.append((p1,  transform_pi(pi, m1, N)))\n",
    "\n",
    "    p2, m2 = rotate180_board(planes), (lambda i: idx_map_rotate180(i, N))\n",
    "    out.append((p2,  transform_pi(pi, m2, N)))\n",
    "\n",
    "    p3, m3 = rotate270_board(planes), (lambda i: idx_map_rotate270(i, N))\n",
    "    out.append((p3,  transform_pi(pi, m3, N)))\n",
    "\n",
    "    # Flip + rotations\n",
    "    pf, mf = flip_h_board(planes),    (lambda i: idx_map_flip_h(i, N))\n",
    "    out.append((pf,  transform_pi(pi, mf, N)))\n",
    "\n",
    "    pf1 = rotate90_board(pf)\n",
    "    out.append((pf1, transform_pi(pi, lambda i: idx_map_rotate90(mf(i, N), N), N)))\n",
    "\n",
    "    pf2 = rotate180_board(pf)\n",
    "    out.append((pf2, transform_pi(pi, lambda i: idx_map_rotate180(mf(i, N), N), N)))\n",
    "\n",
    "    pf3 = rotate270_board(pf)\n",
    "    out.append((pf3, transform_pi(pi, lambda i: idx_map_rotate270(mf(i, N), N), N)))\n",
    "\n",
    "    return out\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buf = collections.deque(maxlen=capacity)\n",
    "    def add(self, x): self.buf.append(x)\n",
    "    def sample(self, bs): return random.sample(self.buf, bs)\n",
    "    def __len__(self): return len(self.buf)\n",
    "\n",
    "\"\"\"\n",
    "# fixed ceiling for clarity\n",
    "MAX_SIMS = 800\n",
    "\n",
    "def sims_schedule(iter_idx, move_no, base=200, max_sims=MAX_SIMS, warmup_iters=20, ramp_moves=12):\n",
    "    # ramp with training iteration (more search later)\n",
    "    # ramp with move number (opening cheaper, mid/late richer)\n",
    "    \n",
    "    # ramp by iteration\n",
    "    f_iter = min(1.0, iter_idx / float(warmup_iters))\n",
    "    sims_iter = int(base + (max_sims - base) * f_iter)\n",
    "    # ramp by move number\n",
    "    f_move = min(1.0, move_no / float(ramp_moves))\n",
    "    sims = int(base + (sims_iter - base) * f_move)\n",
    "    return max(base, min(max_sims, sims))\n",
    "\n",
    "def opening_eps_schedule(iter_idx, move_no, max_eps=0.40, stop_iter=30, stop_move=10):\n",
    "    f_iter = max(0.0, 1.0 - iter_idx/float(stop_iter))\n",
    "    f_move = max(0.0, 1.0 - move_no/float(stop_move))\n",
    "    return max_eps * f_iter * f_move\n",
    "\n",
    "def self_play_episode(env, model, mcts, temp_moves=8, iter_idx=1):\n",
    "    env.reset()\n",
    "    # list of (planes, pi, player)\n",
    "    traj = []\n",
    "    done = False\n",
    "    move_no = 0\n",
    "    while not done:\n",
    "        # adjust sims on the fly\n",
    "        # mcts.simulations = sims_schedule(iter_idx, move_no, base=200, max_sims=mcts.simulations)\n",
    "        mcts.simulations = sims_schedule(iter_idx, move_no, base=200, max_sims=MAX_SIMS)\n",
    "        eps_open = opening_eps_schedule(iter_idx, move_no)\n",
    "        root = mcts.run(\n",
    "            env, \n",
    "            add_root_noise=True, \n",
    "            dir_alpha=0.15, \n",
    "            dir_eps=0.25,\n",
    "            opening_bias=OPENING_BIAS,\n",
    "            opening_eps=eps_open\n",
    "        )\n",
    "        T = 1.0 if move_no < temp_moves else 0.0\n",
    "        a = mcts.select_action(root, temperature=T)\n",
    "\n",
    "        # build visit-count policy target π\n",
    "        pi = np.zeros(env.N * env.N, dtype=np.float32)\n",
    "        for aa, child in root.children.items():\n",
    "            pi[aa] = child.visit_count\n",
    "        pi /= (pi.sum() + 1e-8)\n",
    "\n",
    "        planes = env.to_planes()\n",
    "        traj.append((planes, pi, env.current_player))\n",
    "\n",
    "        _, reward, done = env.step(a)\n",
    "        move_no += 1\n",
    "\n",
    "    # assign final outcome z to each step from that step's player perspective\n",
    "    data = []\n",
    "    for planes, pi, player in traj:\n",
    "        # +1 win for player's perspective, 0 draw, -1 loss\n",
    "        z = reward * player  \n",
    "        # 8 sym augmentations\n",
    "        for sp in symmetries(planes):\n",
    "            data.append((sp.astype(np.float32), pi.copy(), float(z)))\n",
    "        # list of (planes[3,N,N], pi[N*N], z)\n",
    "    return data\n",
    "\"\"\"\n",
    "\n",
    "# ----- curriculum knobs -----\n",
    "CURRICULUM = dict(\n",
    "    max_sims=1600,          # stronger ceiling later if you like (e.g., 1600)\n",
    "    base_sims=200,         # very cheap opening search\n",
    "    sims_warmup_iters=40,  # how many iterations to reach near max_sims\n",
    "    sims_ramp_moves=15,    # ramp sims across the opening moves\n",
    "    opening_max_eps=0.40,  # strength of center prior mix at start\n",
    "    opening_stop_iter=30,  # how fast to fade opening bias across iters\n",
    "    opening_stop_move=10,  # ... and across moves\n",
    "    temp_opening_moves=10   # use T=1 for first K moves, then T=0\n",
    ")\n",
    "\n",
    "def sims_schedule(iter_idx, move_no, cfg=CURRICULUM):\n",
    "    base, maxs = cfg['base_sims'], cfg['max_sims']\n",
    "    f_iter = min(1.0, iter_idx / float(cfg['sims_warmup_iters']))\n",
    "    sims_iter = int(base + (maxs - base) * f_iter)\n",
    "    f_move = min(1.0, move_no / float(cfg['sims_ramp_moves']))\n",
    "    sims = int(base + (sims_iter - base) * f_move)\n",
    "    return max(base, min(maxs, sims))\n",
    "\n",
    "def opening_eps_schedule(iter_idx, move_no, cfg=CURRICULUM):\n",
    "    max_eps = cfg['opening_max_eps']\n",
    "    f_iter = max(0.0, 1.0 - iter_idx/float(cfg['opening_stop_iter']))\n",
    "    f_move = max(0.0, 1.0 - move_no/float(cfg['opening_stop_move']))\n",
    "    return max_eps * f_iter * f_move\n",
    "\n",
    "def temperature_for_move(move_no, cfg=CURRICULUM):\n",
    "    return 1.0 if move_no < cfg['temp_opening_moves'] else 0.0\n",
    "\n",
    "def self_play_episode(env, model, mcts, iter_idx=1, cfg=CURRICULUM):\n",
    "    env.reset()\n",
    "    traj, done, move_no = [], False, 0\n",
    "    while not done:\n",
    "        mcts.simulations = sims_schedule(iter_idx, move_no, cfg)\n",
    "        eps_open = opening_eps_schedule(iter_idx, move_no, cfg)\n",
    "        root = mcts.run(\n",
    "            env,\n",
    "            add_root_noise=True,\n",
    "            dir_alpha=0.15, dir_eps=0.25,\n",
    "            opening_bias=center_bias(env.N, sigma=0.35),\n",
    "            opening_eps=eps_open\n",
    "        )\n",
    "        T = temperature_for_move(move_no, cfg)\n",
    "        a = mcts.select_action(root, temperature=T)\n",
    "\n",
    "        pi = np.zeros(env.N * env.N, dtype=np.float32)\n",
    "        for aa, child in root.children.items():\n",
    "            pi[aa] = child.visit_count\n",
    "        s = pi.sum()\n",
    "        if s > 0: pi /= s\n",
    "\n",
    "        traj.append((env.to_planes(), pi, env.current_player))\n",
    "        _, reward, done = env.step(a)\n",
    "        move_no += 1\n",
    "\n",
    "    # --- FIXED outcome mapping ---\n",
    "    if reward == 0:\n",
    "        outcome = 0\n",
    "    else:\n",
    "        winner = -env.current_player     # player who just moved\n",
    "        outcome = +1 if winner == +1 else -1\n",
    "\n",
    "    data = []\n",
    "    for planes, pi, player in traj:\n",
    "        z = outcome * player\n",
    "        for planes_aug, pi_aug in symmetries_board_and_pi(planes, pi, env.N):\n",
    "            data.append((planes_aug.astype(np.float32), pi_aug.astype(np.float32), float(z)))\n",
    "    return data\n",
    "\n",
    "def train_step(\n",
    "    model,\n",
    "    optimizer,\n",
    "    batch,\n",
    "    N,\n",
    "    policy_coef=1.0,\n",
    "    value_coef=0.5,\n",
    "    entropy_coef=0.0,   # try 1e-3 for the first ~20 iterations\n",
    "    grad_clip=None,     # e.g., 1.0 to clip by norm\n",
    "):\n",
    "    # batch: list of (planes[3,N,N], pi[N*N], z in {-1,0,+1})\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    planes = torch.from_numpy(np.stack([b[0] for b in batch])).to(device=device, dtype=torch.float32)\n",
    "    pi     = torch.from_numpy(np.stack([b[1] for b in batch])).to(device=device, dtype=torch.float32)\n",
    "    z      = torch.tensor([[b[2]] for b in batch], device=device, dtype=torch.float32)\n",
    "\n",
    "    logits, value = model(planes)                 # logits: [B, N*N], value: [B,1]\n",
    "    logp = F.log_softmax(logits, dim=1)\n",
    "    p    = F.softmax(logits, dim=1)\n",
    "\n",
    "    # policy cross-entropy with the MCTS targets\n",
    "    policy_loss = -(pi * logp).sum(dim=1).mean()\n",
    "    # value MSE to {-1,0,1} (after your z fix in self-play)\n",
    "    value_loss  = F.mse_loss(value, z)\n",
    "    # optional exploration bonus early in training\n",
    "    entropy     = -(p * logp).sum(dim=1).mean()\n",
    "\n",
    "    loss = policy_coef * policy_loss + value_coef * value_loss - entropy_coef * entropy\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    if grad_clip is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss.item()), float(policy_loss.item()), float(value_loss.item())\n",
    "\n",
    "def random_action(env):\n",
    "    return random.choice(env.get_legal_actions())\n",
    "\n",
    "def evaluate_vs_random(model, sims=800, games=50, N=9, win_len=5):\n",
    "    wins = draws = losses = 0\n",
    "    for g in range(games):\n",
    "        env = Gomoku(N=N, win_len=win_len)\n",
    "        my_color = +1 if (g % 2 == 0) else -1  # alternate our color\n",
    "        env.current_player = my_color          # start from our color that game\n",
    "        my_mcts = MCTS(model, simulations=sims, c_puct=2.0, win_len=win_len)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.current_player == my_color:\n",
    "                root = my_mcts.run(env, add_root_noise=False)\n",
    "                a = my_mcts.select_action(root, temperature=0.0)\n",
    "            else:\n",
    "                a = random_action(env)\n",
    "            _, reward, done = env.step(a)\n",
    "\n",
    "        if reward == 0:\n",
    "            draws += 1\n",
    "        else:\n",
    "            winner = -env.current_player  # player who just moved\n",
    "            if winner == my_color: wins += 1\n",
    "            else: losses += 1\n",
    "    print(f\"[Eval vs random] W {wins}  D {draws}  L {losses}  (games={games})\")\n",
    "    return wins, draws, losses\n",
    "\n",
    "# ------------- main training loop (sketch) -------------\n",
    "def train_gomoku(num_iters=200, episodes_per_iter=20, sims=800, batch_size=256):\n",
    "    env = Gomoku(N=9, win_len=5)\n",
    "    model = PVResNet(board_size=env.N, channels=64, n_blocks=6)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_iters)\n",
    "    mcts = MCTS(model, simulations=sims, c_puct=2.0, win_len=env.win_len)\n",
    "    replay = Replay(200_000)\n",
    "\n",
    "    ENTROPY_WARM_ITERS = 20     # turn on small entropy bonus for first ~20 iterations\n",
    "    ENTROPY_COEF_START = 1e-3   # or 5e-4\n",
    "\n",
    "    for it in range(1, num_iters+1):\n",
    "        # ------- self-play -------\n",
    "        for _ in range(episodes_per_iter):\n",
    "            data = self_play_episode(env, model, mcts, iter_idx=it, cfg=CURRICULUM)\n",
    "            for item in data:\n",
    "                replay.add(item)\n",
    "\n",
    "        # ------- train steps -------\n",
    "        # schedule entropy: on for first ENTROPY_WARM_ITERS, then 0.0\n",
    "        # entropy_coef = ENTROPY_COEF_START if it <= ENTROPY_WARM_ITERS else 0.0\n",
    "        # cosine decay from ENTROPY_COEF_START → 0 over ENTROPY_WARM_ITERS\n",
    "        if it <= ENTROPY_WARM_ITERS:\n",
    "            t = (it-1) / max(1, ENTROPY_WARM_ITERS-1)\n",
    "            entropy_coef = 0.5 * (1 + math.cos(math.pi * t)) * ENTROPY_COEF_START\n",
    "        else:\n",
    "            entropy_coef = 0.0\n",
    "\n",
    "        losses = []\n",
    "        num_batches = min(400, len(replay)//batch_size)\n",
    "        for _ in range(num_batches):\n",
    "            batch = replay.sample(batch_size)\n",
    "            l, pl, vl = train_step(\n",
    "                model, optimizer, batch, env.N,\n",
    "                policy_coef=1.0, value_coef=0.5,\n",
    "                entropy_coef=entropy_coef,\n",
    "                grad_clip=1.0,   # optional but often helpful\n",
    "            )\n",
    "            losses.append(l)\n",
    "\n",
    "        sched.step()\n",
    "        print(f\"Iter {it:03d} | replay {len(replay)} | sims {sims} | \"\n",
    "              f\"lr {sched.get_last_lr()[0]:.4e} | loss {np.mean(losses) if losses else 0:.3f}\")\n",
    "\n",
    "        # cheap/occasional evals only if you want\n",
    "        # if it % 25 == 0:\n",
    "        #     evaluate_vs_random(model, sims=200, games=8, N=env.N, win_len=env.win_len)\n",
    "\n",
    "        if it % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"models/gomoku_alpha_zero_iter{it:03d}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"models/gomoku_alpha_zero.pth\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60033c42-23d3-48be-af4a-30de9d67be59",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_gomoku\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 301\u001b[39m, in \u001b[36mtrain_gomoku\u001b[39m\u001b[34m(num_iters, episodes_per_iter, sims, batch_size)\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_iters+\u001b[32m1\u001b[39m):\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# ------- self-play -------\u001b[39;00m\n\u001b[32m    300\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes_per_iter):\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m         data = \u001b[43mself_play_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCURRICULUM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[32m    303\u001b[39m             replay.add(item)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 185\u001b[39m, in \u001b[36mself_play_episode\u001b[39m\u001b[34m(env, model, mcts, iter_idx, cfg)\u001b[39m\n\u001b[32m    183\u001b[39m mcts.simulations = sims_schedule(iter_idx, move_no, cfg)\n\u001b[32m    184\u001b[39m eps_open = opening_eps_schedule(iter_idx, move_no, cfg)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m root = \u001b[43mmcts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_root_noise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdir_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_eps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopening_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenter_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.35\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopening_eps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps_open\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m T = temperature_for_move(move_no, cfg)\n\u001b[32m    193\u001b[39m a = mcts.select_action(root, temperature=T)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mMCTS.run\u001b[39m\u001b[34m(self, env, add_root_noise, dir_alpha, dir_eps, opening_bias, opening_eps)\u001b[39m\n\u001b[32m     35\u001b[39m     mix_opening_bias(\u001b[38;5;28mself\u001b[39m.root, N, opening_bias, eps=opening_eps)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.simulations):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.root\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mMCTS._simulate\u001b[39m\u001b[34m(self, node, N)\u001b[39m\n\u001b[32m    109\u001b[39m         best_score, best_a = ucb, a\n\u001b[32m    111\u001b[39m child = node.children[best_a]\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m v_child = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m v = -v_child\n\u001b[32m    114\u001b[39m node.visit_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mMCTS._simulate\u001b[39m\u001b[34m(self, node, N)\u001b[39m\n\u001b[32m    109\u001b[39m         best_score, best_a = ucb, a\n\u001b[32m    111\u001b[39m child = node.children[best_a]\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m v_child = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m v = -v_child\n\u001b[32m    114\u001b[39m node.visit_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mMCTS._simulate\u001b[39m\u001b[34m(self, node, N)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m v\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node.visit_count == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node.children:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_expand_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m     node.visit_count += \u001b[32m1\u001b[39m\n\u001b[32m    101\u001b[39m     node.total_value += v\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mMCTS._expand_and_evaluate\u001b[39m\u001b[34m(self, node, N)\u001b[39m\n\u001b[32m    141\u001b[39m suicidal = []\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m legal:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopponent_has_immediate_win_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwin_len\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    144\u001b[39m         suicidal.append(a)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m0\u001b[39m < \u001b[38;5;28mlen\u001b[39m(suicidal) < \u001b[38;5;28mlen\u001b[39m(legal):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 230\u001b[39m, in \u001b[36mMCTS.opponent_has_immediate_win_after\u001b[39m\u001b[34m(raw_board_tuple, my_action, my_player, N, win_len)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m oa \u001b[38;5;129;01min\u001b[39;00m legal_opp:\n\u001b[32m    229\u001b[39m     nb = apply_action_raw(child, oa, opp, N)\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     winner, done = \u001b[43mcheck_terminal_gomoku\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m winner == opp:\n\u001b[32m    232\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mcheck_terminal_gomoku\u001b[39m\u001b[34m(raw_board_tuple, N, win_len)\u001b[39m\n\u001b[32m     69\u001b[39m             r -= \u001b[32m1\u001b[39m; c += \u001b[32m1\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m: \u001b[38;5;28;01mreturn\u001b[39;00m (+\u001b[32m1.0\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_k(-\u001b[32m1\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m (-\u001b[32m1.0\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (b != \u001b[32m0\u001b[39m).all(): \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[32m0.0\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mcheck_terminal_gomoku.<locals>.has_k\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[32m     38\u001b[39m         run = run + \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m b[r, c] == p \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run >= win_len: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# diag \\\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r0 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_gomoku()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500607d0-5ccc-4750-8364-a61f22098fde",
   "metadata": {},
   "source": [
    "# 6. Strength Ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ad424-5288-4808-8f83-bd3a2df30cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_vs_mcts(model, my_sims, opp_sims, games=40, N=9, win_len=5):\n",
    "    wins = draws = losses = 0\n",
    "    for g in range(games):\n",
    "        env = Gomoku(N=N, win_len=win_len)\n",
    "        my_color = +1 if (g % 2 == 0) else -1\n",
    "        env.current_player = my_color\n",
    "\n",
    "        my_mcts  = MCTS(model, simulations=my_sims,  c_puct=2.0, win_len=win_len)\n",
    "        opp_mcts = MCTS(model, simulations=opp_sims, c_puct=2.0, win_len=win_len)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.current_player == my_color:\n",
    "                root = my_mcts.run(env, add_root_noise=False)\n",
    "                a = my_mcts.select_action(root, temperature=0.0)\n",
    "            else:\n",
    "                root = opp_mcts.run(env, add_root_noise=False)\n",
    "                a = opp_mcts.select_action(root, temperature=0.0)\n",
    "            _, reward, done = env.step(a)\n",
    "\n",
    "        if reward == 0: draws += 1\n",
    "        else:\n",
    "            winner = -env.current_player\n",
    "            if winner == my_color: wins += 1\n",
    "            else: losses += 1\n",
    "    return wins, draws, losses\n",
    "\n",
    "def strength_ladder(model, N=9, win_len=5):\n",
    "    print(\"[Ladder] vs random (color-aware)\")\n",
    "    evaluate_vs_random(model, sims=400, games=50, N=N, win_len=win_len)\n",
    "\n",
    "    print(\"\\n[Ladder] symmetric sims (ours == theirs)\")\n",
    "    for sims in [100, 200, 400, 800, 1200]:\n",
    "        w,d,l = eval_vs_mcts(model, my_sims=sims, opp_sims=sims, games=50, N=N, win_len=win_len)\n",
    "        print(f\"sims={sims:4}  W {w:2}  D {d:2}  L {l:2}\")\n",
    "\n",
    "    print(\"\\n[Ladder] advantage (ours ≥ 800 sims)\")\n",
    "    for opp in [100, 200, 400, 800, 1200]:\n",
    "        w,d,l = eval_vs_mcts(model, my_sims=max(opp, 800), opp_sims=opp, games=50, N=N, win_len=win_len)\n",
    "        print(f\"opp={opp:4}  ours={max(opp,800):4}  W {w:2}  D {d:2}  L {l:2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d778f-89e9-4d8c-84f7-c39442958715",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "strength_ladder(model, N=9, win_len=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fa02a-ec59-40e4-8fcc-ee8b52508612",
   "metadata": {},
   "source": [
    "# 7. Interative Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eff38c77-d0f3-4738-9ad0-68cc54035844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipywidgets\n",
    "import ipywidgets as W\n",
    "from IPython.display import display\n",
    "\n",
    "class GomokuUI:\n",
    "    def __init__(self, model, N=9, win_len=5, sims=800, human_player=-1):\n",
    "        self.model = model\n",
    "        self.N = N\n",
    "        self.win_len = win_len\n",
    "        self.human_player = int(human_player)   # +1 black (X), -1 white (O)\n",
    "        self.sims = sims\n",
    "\n",
    "        self.env = Gomoku(N=N, win_len=win_len)\n",
    "        self.buttons = [[W.Button(description=\" \", layout=W.Layout(width=\"40px\", height=\"40px\")) \n",
    "                         for _ in range(N)] for _ in range(N)]\n",
    "        for r in range(N):\n",
    "            for c in range(N):\n",
    "                self.buttons[r][c].on_click(self._make_cb(r, c))\n",
    "\n",
    "        self.msg = W.HTML(value=\"\")\n",
    "        self.reset_btn = W.Button(description=\"Reset\")\n",
    "        self.reset_btn.on_click(self.reset)\n",
    "\n",
    "        grid = W.GridBox(\n",
    "            children=[self.buttons[r][c] for r in range(N) for c in range(N)],\n",
    "            layout=W.Layout(grid_template_columns=f\"repeat({N}, 42px)\")\n",
    "        )\n",
    "        display(W.VBox([grid, W.HBox([self.reset_btn]), self.msg]))\n",
    "\n",
    "        # AI may start\n",
    "        if self.human_player == -1 and self.env.current_player == +1:\n",
    "            self.ai_move()\n",
    "        self.refresh()\n",
    "\n",
    "    def _make_cb(self, r, c):\n",
    "        def _cb(btn):\n",
    "            if self.env.current_player != self.human_player:\n",
    "                return\n",
    "            a = r * self.N + c\n",
    "            if a not in self.env.get_legal_actions():\n",
    "                return\n",
    "            _, _, done = self.env.step(a)\n",
    "            if not done:\n",
    "                self.ai_move()\n",
    "            self.refresh()\n",
    "        return _cb\n",
    "\n",
    "    \"\"\"\n",
    "    # Deterministic\n",
    "    def ai_move(self):\n",
    "        mcts = MCTS(self.model, simulations=self.sims, c_puct=1.5, win_len=self.win_len)\n",
    "        root = mcts.run(self.env, add_root_noise=False)  # no noise for a strong play\n",
    "        a = mcts.select_action(root, temperature=0.0)\n",
    "        self.env.step(a)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Less deterministic, more human-like variety\n",
    "    def ai_move(self):\n",
    "        mcts = MCTS(self.model, simulations=self.sims, c_puct=2.0, win_len=self.win_len)\n",
    "        # Optional: tiny root noise for variety in opening\n",
    "        add_noise = (self.env.num_moves() < 8)  # implement num_moves() if you like\n",
    "        root = mcts.run(self.env, add_root_noise=add_noise, dir_alpha=0.15, dir_eps=0.10)\n",
    "        # Temperature schedule\n",
    "        T = 1.0 if self.env.num_moves() < 8 else 0.0\n",
    "        a = mcts.select_action(root, temperature=T)\n",
    "        self.env.step(a)\n",
    "    \n",
    "    def refresh(self):\n",
    "        sym = {1:\"X\", -1:\"O\", 0:\" \"}\n",
    "        for r in range(self.N):\n",
    "            for c in range(self.N):\n",
    "                v = int(self.env.board[r, c])\n",
    "                b = self.buttons[r][c]\n",
    "                b.description = sym[v]\n",
    "                b.disabled = bool(v != 0)\n",
    "\n",
    "        reward, done = self.env.check_winner_fast()\n",
    "        if done:\n",
    "            if reward == 0:\n",
    "                self.msg.value = \"<b>Draw.</b> Click Reset.\"\n",
    "            else:\n",
    "                winner = 'X' if -self.env.current_player == +1 else 'O'\n",
    "                self.msg.value = f\"<b>{winner} wins!</b> Click Reset.\"\n",
    "            for r in range(self.N):\n",
    "                for c in range(self.N):\n",
    "                    self.buttons[r][c].disabled = True\n",
    "        else:\n",
    "            turn = 'X' if self.env.current_player==+1 else 'O'\n",
    "            self.msg.value = f\"Player {turn} to move.\"\n",
    "\n",
    "    def reset(self, _=None):\n",
    "        self.env.reset()\n",
    "        if self.human_player == -1 and self.env.current_player == +1:\n",
    "            self.ai_move()\n",
    "        self.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d055083b-7742-42c6-affc-441cf34cbaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f528294456462e8e9a70028eb8785f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(GridBox(children=(Button(description=' ', layout=Layout(height='40px', width='40px'), style=But…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "model.eval()\n",
    "ui = GomokuUI(model, N=9, win_len=5, sims=600, human_player=-1)  # human plays O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf874784-901b-471c-924a-97636861c0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e008aaed57a43fba0da330acebde1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(GridBox(children=(Button(description=' ', layout=Layout(height='40px', width='40px'), style=But…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "model.eval()\n",
    "ui = GomokuUI(model, N=9, win_len=5, sims=1600, human_player=-1)  # human plays O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22aad97-8a14-4311-9c0a-aff9a9e23641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
